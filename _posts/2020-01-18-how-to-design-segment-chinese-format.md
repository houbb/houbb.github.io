---
layout: post
title: 如何解决分词的繁简体问题？
date:  2020-1-9 10:09:32 +0800
categories: [Search]
tags: [nlp, index, math, sh]
published: true
---

# 分词的繁简体问题

分词是基本所有 nlp 的基础，那么分词中的繁简体如何解决？


## 方案1-HMM预测

不依赖字典，直接根据繁题的字频预测。

优点：降低内存消耗

缺点：不知道 jieba 有没有对应的繁体字频统计。

而且我想在 HMM 服务降级的时候依然可以支持。

## 方案2-繁体字典预处理

基于 jieba-fenci + opencc4j 首先做一次预处理。

将所有的繁体词组处理一遍，频率和简体的保持一致。

为什么要这样呢？

### 互转的场景

我们肯定会有【简体=》繁体=》简体】的这种测试验证，我觉得应该保证可逆性。


* any list
{:toc}