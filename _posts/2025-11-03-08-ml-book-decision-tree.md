---
layout: post
title: 第8章　决策树与集成学习
date: 2025-11-03 20:40:12 +0800
categories: [AI]
tags: [ai, learn-note]
published: true
---


## **第8章　决策树与集成学习**

---

### **8.1 决策树构建原理（ID3、C4.5、CART）**

#### **（1）核心思想**

决策树（Decision Tree）是一种 **基于规则的监督学习方法**。

它通过在特征空间中 **逐步划分样本**，最终形成一个 **树状结构** 来进行分类或回归。

每个内部节点表示一个“特征判定条件”，每个叶子节点对应一个“类别”或“预测值”。

**直观理解**：
就像人类判断事物一样，“如果温度高→再看湿度→如果湿度低→晴天”，形成一棵“判断树”。

#### **（2）构建流程**

1. 从数据集中选择一个最优特征；
2. 根据该特征的不同取值划分数据；
3. 对每个子集递归构建子树；
4. 直到满足停止条件（如样本纯度高、深度限制等）。

---

#### **（3）ID3 算法**

* **核心指标**：信息增益（Information Gain）
  用信息论的角度衡量“某个特征对分类的不确定性减少程度”。

  [
  IG(D, A) = H(D) - H(D|A)
  ]
  其中 (H(D)) 为数据集的熵，(H(D|A)) 为按特征 A 划分后的条件熵。
* **问题**：ID3 只能处理离散特征，对连续特征和缺失值支持差。

---

#### **（4）C4.5 算法**

* **改进点**：

  * 支持连续特征；
  * 使用“信息增益率”（Information Gain Ratio）；
  * 支持缺失值；
  * 使用剪枝（Pruning）防止过拟合。
* **信息增益率定义**：
  [
  GainRatio(D, A) = \frac{IG(D, A)}{H_A(D)}
  ]
  其中 (H_A(D)) 为特征 A 的“分裂信息熵”。

---

#### **（5）CART 算法**

* **Classification And Regression Tree**：既可用于分类，也可用于回归。
* **核心指标**：

  * 分类任务：基尼指数（Gini Index）；
  * 回归任务：均方误差（MSE）。
* **特点**：

  * 使用二叉树结构；
  * 连续特征按阈值二分；
  * 剪枝方法采用 **代价复杂度剪枝（Cost-Complexity Pruning）**。

---

#### **（6）优缺点**

| 优点          | 缺点                  |
| ----------- | ------------------- |
| 直观易解释，可视化友好 | 容易过拟合               |
| 可处理非线性关系    | 对数据噪声敏感             |
| 可处理离散与连续特征  | 树不稳定（小数据扰动可能导致结构大变） |

---

### **8.2 随机森林与特征重要性**

#### **（1）随机森林（Random Forest）简介**

随机森林是 **Bagging（自助采样法）+ 决策树** 的集成模型。
它通过构建多棵互相独立的树，再让它们进行投票或平均预测，从而提高泛化性能。

#### **（2）核心机制**

* **样本随机性**：每棵树使用不同的 Bootstrap 样本；
* **特征随机性**：每个节点分裂时随机选择部分特征；
* **集成预测**：分类取多数投票，回归取平均值。

#### **（3）优点**

* 大幅降低过拟合；
* 对高维数据、缺失数据、离群点鲁棒；
* 可计算“特征重要性”。

#### **（4）特征重要性计算方法**

* **基于分裂增益**：统计特征在所有树中带来的信息增益之和；
* **基于置换（Permutation Importance）**：打乱某个特征的取值，看模型性能下降幅度。

#### **（5）典型应用**

随机森林是工业界最实用、最稳健的机器学习模型之一，常用于：

* 信贷风险评估；
* 医学诊断；
* 客户流失预测；
* 特征筛选。

---

### **8.3 Boosting、Bagging、XGBoost、LightGBM**

#### **（1）Bagging 与 Boosting 的区别**

| 对比维度 | Bagging | Boosting                  |
| ---- | ------- | ------------------------- |
| 训练方式 | 并行      | 串行（逐步改进）                  |
| 目标   | 降低方差    | 降低偏差                      |
| 代表算法 | 随机森林    | AdaBoost、XGBoost、LightGBM |

---

#### **（2）AdaBoost（Adaptive Boosting）**

* 每一轮训练一个弱分类器（如浅层决策树）；
* 赋予被错分样本更高的权重；
* 最终用加权投票融合所有分类器。

**思想核心**：让模型聚焦于“前一轮没学好的样本”。

---

#### **（3）XGBoost（Extreme Gradient Boosting）**

* 基于梯度提升（Gradient Boosting）的高效实现；
* 引入 **二阶导信息**，更快收敛；
* 具有 **剪枝、正则化、列抽样、并行化** 等优化。

**目标函数：**
[
Obj = \sum_i l(y_i, \hat{y}_i) + \sum_k \Omega(f_k)
]
其中 (\Omega(f_k)) 是树的复杂度正则项。

---

#### **（4）LightGBM**

* 微软开源，进一步优化 XGBoost 的速度和内存占用；
* 采用 **Histogram-based 分裂法**（将连续特征分桶）；
* 使用 **Leaf-wise 策略**（更深的生长方向）；
* 对大规模、高维数据表现优异。

---

#### **（5）CatBoost**

* Yandex 提出的 Boosting 框架；
* 针对分类特征进行了特别优化；
* 使用对称树结构（balanced tree），训练更稳定；
* 避免了“目标泄漏”（target leakage）。

---

### **8.4 集成方法的偏差-方差机制分析**

#### **（1）偏差-方差分解回顾**

模型误差通常可分解为三部分：
[
E_{total} = Bias^2 + Variance + Noise
]

* **偏差（Bias）**：模型拟合能力不足；
* **方差（Variance）**：模型对样本扰动敏感；
* **噪声（Noise）**：数据中不可消除的随机误差。

#### **（2）Bagging 的作用**

* 降低方差；
* 提高稳定性；
* 对高方差模型（如决策树）尤其有效。

#### **（3）Boosting 的作用**

* 降低偏差；
* 提升拟合能力；
* 对弱学习器组合后可形成强模型。

#### **（4）集成学习的本质**

> “弱者联合可以成为强者”——通过组合多个不完美模型，形成一个强大且泛化良好的整体。

这正是现代机器学习中“模型融合（Model Ensemble）”与“多专家协作（Mixture of Experts）”思想的理论根基。

---

### ✅ **总结**

| 方法       | 特点         | 优点      | 缺点       | 应用场景        |
| -------- | ---------- | ------- | -------- | ----------- |
| 决策树      | 规则可解释      | 可视化、易解释 | 容易过拟合    | 特征分析、教学     |
| 随机森林     | Bagging 集成 | 稳定、高精度  | 模型大、解释性弱 | 工业生产模型      |
| AdaBoost | 串行提升       | 精度高     | 噪声敏感     | 小样本分类       |
| XGBoost  | 二阶提升+正则化   | 高性能、稳定  | 参数多      | Kaggle 常胜算法 |
| LightGBM | 基于直方图      | 速度快、资源低 | 可解释性弱    | 大数据任务       |

* any list
{:toc}