---
layout: post
title: 第17章　卷积与序列模型 卷积神经网络（Convolutional Neural Network, CNN）
date: 2025-11-03 20:40:12 +0800
categories: [AI]
tags: [ai, learn-note]
published: true
---


这一章标志着从传统机器学习走向深度学习时代的关键转折点，它聚焦于两大深度结构 —— **CNN（卷积神经网络）** 与 **RNN（循环神经网络）/Transformer（注意力模型）**，它们分别代表了机器在“看图”和“理解序列”上的两条主线。

---

# 第17章　卷积与序列模型

## **17.1 CNN 的思想：局部感受野与权重共享**

### 🔹 一、背景

传统的全连接神经网络在图像处理上效率极低，因为每个像素点都与所有神经元相连，导致参数量巨大、过拟合严重、计算成本极高。
为了解决这一问题，卷积神经网络（Convolutional Neural Network, CNN）应运而生。

### 🔹 二、核心思想

CNN 模型灵感来自生物视觉皮层的感知机制，核心有两大关键思想：

1. **局部感受野（Local Receptive Field）**

   * 每个神经元只“看”输入图像中的局部区域，而不是整个图像。
   * 这使模型能捕捉局部特征（如边缘、角点、纹理等）。

2. **权重共享（Weight Sharing）**

   * 同一卷积核（Filter）在整张图像上滑动，用相同的参数检测不同位置的同类特征。
   * 大幅减少参数量，提高泛化能力。

### 🔹 三、典型结构

CNN 一般包含以下层级：

* **卷积层（Convolution Layer）**：提取局部特征。
* **激活层（ReLU）**：引入非线性。
* **池化层（Pooling Layer）**：降维、增强平移不变性。
* **全连接层（Fully Connected Layer）**：进行分类或回归。

### 🔹 四、代表模型

* **LeNet-5（1998）**：最早的手写数字识别网络。
* **AlexNet（2012）**：ImageNet 大赛冠军，标志深度学习复兴。
* **VGG、ResNet、Inception**：多层结构与残差连接的典型代表。

### 🔹 五、意义

CNN 的诞生使得“特征工程”从人工设计转向**自动学习**，成为计算机视觉（CV）领域的核心基础。

---

## **17.2 RNN、LSTM、GRU 与序列依赖**

### 🔹 一、为什么需要 RNN？

在 NLP、语音识别、时间序列预测等任务中，输入数据是**有顺序的**。

传统神经网络无法捕捉“时间上的依赖关系”，因此引入了循环神经网络（RNN）。

### 🔹 二、RNN 的核心机制

RNN 的关键思想是：

> 当前时刻的输出不仅依赖当前输入，还依赖上一个时刻的隐藏状态。

即：
[
h_t = f(Wx_t + Uh_{t-1})
]
这样，模型可以“记住”先前的信息，实现序列建模。

### 🔹 三、RNN 的问题：梯度消失/爆炸

由于反向传播要在时间维度上展开（BPTT），RNN 很容易出现**长期依赖问题**，导致训练不稳定。

### 🔹 四、改进模型

1. **LSTM（Long Short-Term Memory）**

   * 通过“门机制”（输入门、遗忘门、输出门）控制信息流动。
   * 能更好地捕捉长期依赖。

2. **GRU（Gated Recurrent Unit）**

   * LSTM 的简化版本，减少参数，提高训练速度。

### 🔹 五、典型应用

* 语言模型（预测下一个词）
* 机器翻译
* 语音识别
* 股票预测、传感器序列分析等

### 🔹 六、意义

RNN 系列模型让神经网络从“静态感知”迈向“动态理解”，为后续 Transformer 奠定了思想基础。

---

## **17.3 注意力机制与 Transformer**

### 🔹 一、RNN 的局限性

* 序列依赖导致计算**难以并行**；
* 远距离依赖仍然难以捕捉；
* 训练时间长。

### 🔹 二、注意力机制（Attention Mechanism）

灵感来源于人类的注意力：

> 在处理信息时，人类不会平均关注所有输入，而是会“聚焦”在关键部分。

**核心公式：**
[
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
]

* Q：查询向量（query）
* K：键向量（key）
* V：值向量（value）

这个机制允许模型在每个时间步动态选择“该关注的输入部分”。

### 🔹 三、Transformer 的崛起

2017年，Google 提出了 Transformer 结构（论文《Attention is All You Need》），完全抛弃循环结构，**仅依靠注意力机制**建模序列关系。

#### Transformer 的优势：

* 完全并行化（适合 GPU 加速）；
* 捕捉长距离依赖；
* 可扩展性极强。

#### 核心组件：

* **多头注意力（Multi-Head Attention）**：让模型从多个角度关注不同的关系。
* **位置编码（Positional Encoding）**：保留序列顺序信息。
* **前馈网络（Feed Forward Layer）**：在每个位置独立变换特征。

### 🔹 四、代表模型与应用

* **BERT（2018）**：预训练语言模型，引发 NLP 革命。
* **GPT 系列**：从生成式预训练到大语言模型（LLM）。
* **Vision Transformer（ViT）**：将 Transformer 引入计算机视觉。
* **Time Series Transformer**：用于序列预测与异常检测。

### 🔹 五、意义

Transformer 的出现标志着“统一的深度学习架构”时代。
它打通了文本、图像、语音等不同模态的壁垒，成为当今 AI 的底层支撑结构。

---

## 📘 小结

| 模型类型        | 核心思想         | 代表模型           | 典型应用          | 意义   |
| ----------- | ------------ | -------------- | ------------- | ---- |
| CNN         | 局部感受野 + 权重共享 | LeNet, ResNet  | 图像分类、检测、识别    | 视觉革命 |
| RNN         | 序列依赖 + 时间状态  | LSTM, GRU      | NLP、语音、时间序列   | 序列建模 |
| Transformer | 注意力 + 并行化    | BERT, GPT, ViT | NLP、CV、语音、跨模态 | 统一架构 |

* any list
{:toc}