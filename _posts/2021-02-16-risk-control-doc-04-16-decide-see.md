---
layout: post
title:  风控资料汇总-04-16-精细化运营
date:  2021-2-14 16:52:15 +0800
categories: [BIZ]
tags: [biz, risk, risk-control, sh]
published: true
---



# 精细化运营

精细化运营是个老词了，特别是在电商、社交这类大的、成熟的互联网领域。其实在业务安全领域，精细化运营的思想也是适用的。

我个人总结的精细化运营的核心问题就是要解决2个问题：

- 看得见

- 看得清

解决“看得见”这个问题，也就是解决数据的获取问题，分为3个子问题：

- 数据是否被存储了？

- 数据能否可以被快速、灵活的访问？

- 数据能否可以被关联分析？


解决“看得清”这个问题，也就是解决数据的理解问题

- 能否量化业务，从而尽量还原客观事实？

- 能否制定合理的核心指标，从而化繁为简、提高运营效率？

- 能否建立合理、灵活、快速的可视化体系，从而提高人的理解？


# 决策的第一步-看得见

开篇提到解决“看得见”这个问题，也就是解决数据的获取问题，分为3个子问题：

数据是否被存储了？
数据能否可以被快速、灵活的访问？
数据能否可以被关联分析？

## 数据的存储

请以自己的网站或应用为例，试着回答一下这几个问题：

1. 你的网站或应用的登录失败率是多少？

2. 登录量上升 + 登录成功率下降，让你很紧张，那么你是受到攻击了吗？

要回答上面的问题，至少你要有登录日志，我相信大部分公司都会存。

但如果你只存了登录日志，而没有存验证日志，那你就很难回答第2个问题了。

下面这2种可能都可能在特定时期导致登录量上升 + 登录成功率下降

1. 黑产真的来撞库了

2. 验证码服务升级，提高了验证难度

所以，如果你的数据一开始就没有被存下来，那么你就真的只能靠猜了。

## 数据的访问

除了存日志，你的日志还加了充足的字段，例如：IP、时间、验证码类型、验证码服务版本、客户端版本、设备号等等。

这很好，给未来的分析开了个好头。

可是这也带来了问题：我增加了日志，增加了字段，我的业务很好，可是数据量太大了，我用excel打不开

这里的excel是一个比喻，但也不是个比喻。

随着数据量增大，要想访问数据就需要大数据工具，例如Hive、Spark、Hbase、Cassandra、Elasticsearch。我相信稍微大一点的公司都有使用。

但是 使用大数据技术不代表数据可以被快速、灵活的访问。

我见过拥有完备大数据体系的公司里，不少运营同事却无法方便的获取数据，只好请工程师导出数据样本后，本地excel人工分析。

### 常见问题一：数据没有被整理

很多重要的业务数据混杂在一个大的日志文件中，例如这样产生的日志

```java
System.out.println("user " + userId + " logged in at " + time)
//or
Logger.info("user {} uploaded a photo at {}", userId, time)
```

用户行为虽然记录到了日志中的，但是不同行为日志混在一起，甚至还包含代码的报错堆栈信息。

**如果日志格式没有规范，事后想要排查问题都很难，更不要提做分析**。

常见方案：有价值的日志应该按照便于后期处理的格式打印到单独文件中。然后使用Apache Flume + Kafka + Spark转存到Hive中。

### 常见问题二：分析师没有入口可以查询

我见过太多的系统工程师忘了一件事情：不是每个人都会编程。

数据应该可以让团队中每一个需要的人方便获取到。

通过ssh登录到服务器上，使用命令行读取很明显不是一个方便的方式。

特别是当命令行查询出的数据是JSON格式时，大多数人都会是一头雾水。

另外，使用Spark查询数据也不是一个好的方式，除了要写代码，spark代码发布到集群、启动任务都需要花不少时间，这会浪费分析师大量的时间。

常见方案：搭建或者开发GUI查询工具，不需要很漂亮，但要方便。这里推荐工具Hue。


# 数据的关联分析

数据量大了以后，数据之间的关联分析就变得困难。

## 常见问题一：存储不一样

基于业务的不同特性，数据可能会存在不同类型的数据库中，MySQL、MongoDB、Cassandra、Hbase等等。

虽然你依然可以使用Spark从不同存储中读出数据，在内存中关联分析。

但是如前文所述，不是一个易用的方案。

常见方案：采用统一数据仓库，例如Hive。每天将其他数据库中的数据同步到数据仓库中。

## 常见问题二：数据量大，关联耗时长

数据量大了以后，一条Join语句可能需要运行很久。

常见方案比较多，例如：

- 对数据合理分区

- 成本换时间。采用基于内存的分析引擎，如Impala、Presto

- 抽象数据聚合层，如果Join数据长耗时不可避免，那就避免重复进行。

说直白一点，热数据提前Join，冗余常用的分析字段，构建大宽表。

## 总结

我认为所有的决策都要先解决“看的见、看得清”这个问题，而“看得见”是基础。

首先需要我们合理设计数据内容，其次需要我们选取合适的数据存储分析技术。都不是容易的事。

# 决策的核心问题-看得清

前文提到解决“看得清”这个问题，也就是解决数据的理解问题，分为2个子问题：

1. 能否制定合理的核心指标，从而化繁为简、提高运营效率？

2. 能否建立合理、灵活、快速的可视化体系，从而提高人的理解？

## 制定合理的业务核心指标

上面列了几个常见的指标，我们应该选择看哪个指标呢？

都看可以吗？当然可以，但是没必要。

在爱奇艺我设计了一个系统叫“深度分析”，说起来也简单，就是输入一批用户，看这批用户在爱奇艺所有的行为指标。最后结果有好几页。要是每个统计认真看过去，可能要花3-5分钟才能看完。实际上如果真的是攻击，靠其中1-3个指标就已经可以大致定性了。

如果我们称这1-3个指标为核心指标，那是哪几个呢？

答案是要看是什么业务风险，不同业务不一样。例如密码尝试次数和错误率是一个不常用的指标，绝大多数问题不会考虑这个指标。但是在撞库问题中，就成了核心的指标，需要重点关照。

所以对于你自己的业务，要思考一下你最关心什么，哪些指标可以带来更多信息。然后把重心放在这些核心指标上。

其他指标就不需要了吗？

不是的，在给出最终判定前，最好再花一点时间看看其他指标。

主要是为了看看还有没有别的可能，避免确认偏误(Confirmation Bias)、后见之明偏误(Hindsight Bias)等心理偏误。

```
It is easier to construct a coherent story when you know little, when there are fewer pieces to fit into the puzzle. Our comforting conviction that the world makes sense rests on a secure foundation: our almost unlimited ability to ignore our ignorance.

Kahneman. Thinking, Fast and Slow
```

# 建立合理、灵活、快速的可视化体系

当你确定了业务风险，量化并制定出了核心指标，于是就可以利用机器对业务进行自动化分析和监控了。

但是除了让机器理解你的问题，人也得理解。这个时候可视化能力就很重要了。

可视化不仅可以加快对数据的接受速度，也可以加深对数据的理解。

拿当前世界各地的时间信息为例，普通的表格可以提供完整的信息。通过筛选也可以快速找到某个城市的时间。但是却很难宏观的理解数据，发现城市和城市之间的关系。

世界时间

将信息通过时区地图可视化出来之后，就变得更容易理解。例如你可以看到中国全境采用的是北京时间。但是美国本土区分了好几个时间。

时区地图

关于可视化，我建议使用一些BI工具提升效率，例如

- BDP

- Tableau

- Apache Superset

只有BI工具方便还不够，如果BI工具依赖的引擎不够快也是没用的，所以接下来着重说一下大数据工具，例如Hive、Spark、Kylin、Impala之类的。

对于重要的指标，如果想加快分析的速度，可以从3个方面入手：

- 内存换时间，例如使用Spark、Impala这里基于内存的分析引擎。内存比硬盘贵，但是速度快

- 空间换时间，例如Kylin

- 提升工具的使用性（人力换时间），例如Zeppelin


这里主要从工具角度来介绍，首先是因为方法论较为抽象，一下难以理解，工具是方法论的具象化延伸，易于理解和落地；其次是提升工具确实可以大幅提升效率。




# 参考资料

https://github.com/WalterInSH/risk-management-note/blob/master/%E7%94%A8%E6%88%B7%E8%AE%BE%E5%A4%87%E8%A1%8C%E4%B8%BA%E4%B9%A0%E6%83%AF.md

https://github.com/WalterInSH/risk-management-note/blob/master/%E5%86%B3%E7%AD%96%E7%9A%84%E7%AC%AC%E4%B8%80%E6%AD%A5-%E7%9C%8B%E5%BE%97%E8%A7%81.md

* any list
{:toc}
