---
layout: post
title: 第18章　大模型与预训练范式
date: 2025-11-03 20:40:12 +0800
categories: [AI]
tags: [ai, learn-note]
published: true
---


第18章是整本书的“现代 AI 篇”的核心部分——它讲述了从「词向量」到「BERT」，再到「GPT 与智能体（Agent）」的整条技术演化主线。

这一章不仅是机器学习与深度学习的融合点，更是当代人工智能（尤其是大语言模型）的技术根基。

---

# 第18章　大模型与预训练范式

---

## **18.1 从词向量到 BERT**

### 🔹 一、传统NLP的局限

在早期的自然语言处理中，机器学习模型（如SVM、朴素贝叶斯）使用的特征往往是：

* 词袋模型（Bag of Words）
* TF-IDF 向量
  它们**忽略了词序**，也无法捕捉语义。例如：

> “我爱学习” 与 “学习我爱” 的TF-IDF几乎一样。

这种稀疏、高维、无语义的表示严重限制了机器对语言的理解。

---

### 🔹 二、分布式表示与词向量（Word Embedding）

20世纪80年代后，学者提出了一个关键假设：

> **词义可以通过上下文来定义。**
> ——“你能看出一个词的意义，就看它周围的词。”

这催生了“分布式词表示”的思想：
每个词都可以表示为一个低维、稠密的向量，语义相似的词在空间中也相近。

#### 代表模型：

* **Word2Vec（2013, Google）**

  * **Skip-gram / CBOW** 模型：通过预测上下文或目标词学习词向量。
  * 能捕捉“语义关系”：

    > vector("国王") - vector("男人") + vector("女人") ≈ vector("王后")

* **GloVe（2014, Stanford）**

  * 利用全局词频共现信息（统计方法 + 神经网络）。

这阶段的模型**静态**，同一个词无论出现在什么语境中都只有一个向量。

---

### 🔹 三、从静态到动态：上下文词向量

自然语言的语义往往取决于上下文。
例如：“bank”在以下两句话中的含义不同：

> * I went to the **bank** to deposit money.
> * The boat is on the **bank** of the river.

于是研究者开始探索**上下文相关的词向量**：

#### 📘 代表进展：

* **ELMo（2018, AllenNLP）**：双向LSTM建模上下文；
* **ULMFiT（2018, fast.ai）**：提出“预训练 + 微调”的思想雏形；
* **BERT（2018, Google）**：基于 Transformer 的双向编码器，彻底引爆 NLP。

---

### 🔹 四、BERT 的关键创新

BERT（Bidirectional Encoder Representations from Transformers）将 NLP 带入了“预训练语言模型”时代。

核心思想：

1. **双向上下文建模**：通过 Transformer Encoder 同时看到左右语境；
2. **自监督预训练任务**：

   * **MLM（Masked Language Modeling）**：随机遮盖部分词，预测被遮盖的词；
   * **NSP（Next Sentence Prediction）**：预测句子是否连续；
3. **通用语义表示**：在大规模语料上预训练，再迁移到下游任务（如分类、问答、命名实体识别等）。

---

## **18.2 预训练—微调范式**

### 🔹 一、传统做法的缺陷

过去的 NLP 模型都是“任务特定”的：

* 每个任务（如情感分析、翻译、摘要）都需要独立训练；
* 需要大量标注数据；
* 模型之间无法共享知识。

---

### 🔹 二、预训练—微调（Pretrain-Finetune）范式

BERT 带来的最大变革，就是**统一训练流程**：

1. **预训练（Pretraining）**

   * 在大规模无标签语料上，学习语言通用规律；
   * 模型掌握“语言常识”和“语义知识”。

2. **微调（Finetuning）**

   * 在下游任务的小数据集上，调整模型参数；
   * 学会特定任务的模式。

这种方法极大地提升了模型的**泛化能力**与**数据效率**。

---

### 🔹 三、进一步的演化

* **GPT 系列（OpenAI）**：使用“自回归”结构，只预测下一个词；
* **BART、T5（Google）**：采用“Encoder-Decoder”架构，支持生成任务；
* **RoBERTa、ELECTRA**：改进预训练任务与数据规模。

---

## **18.3 大语言模型（LLM）的架构与原理**

### 🔹 一、从预训练到大规模预训练

当模型参数从百万级增长到**百亿、千亿**级时，出现了“涌现能力（Emergent Abilities）”：

* 能理解复杂语义；
* 会生成连贯、上下文一致的文本；
* 甚至具备逻辑推理与编程能力。

---

### 🔹 二、LLM 的架构核心：Transformer Decoder

大语言模型（如 GPT 系列）通常只使用 Transformer 的 **Decoder** 部分：

* **自回归机制**：预测下一个 token；
* **多层堆叠的注意力网络**；
* **大规模并行训练（分布式）**；
* **位置编码 + LayerNorm + 残差连接** 提高稳定性。

核心目标函数：
[
\text{maximize } P(w_t | w_1, w_2, ..., w_{t-1})
]

---

### 🔹 三、训练要素

* **数据规模**：数千亿 tokens；
* **参数规模**：从 1B → 175B（GPT-3）→ 万亿（GPT-4）；
* **硬件支持**：GPU/TPU 集群、张量并行、模型并行；
* **优化方法**：AdamW、混合精度训练、梯度检查点、ZeRO。

---

### 🔹 四、涌现能力与知识涌现

随着模型规模扩大，出现了人类未显式设计的“智能特性”：

* 上下文学习（In-context learning）；
* 零样本 / 少样本推理；
* 逻辑推断；
* 多模态融合（语言 + 图像 + 音频）。

这些特性让 LLM 从“语言模型”逐渐进化为“通用智能系统”的雏形。

---

## **18.4 从 GPT 到 Agent：智能体的出现**

### 🔹 一、LLM 不再只是“说话的模型”

当大模型具备理解、推理、规划、调用工具的能力时，它就开始具备“**智能体（Agent）**”的特征。

智能体不只是回答问题，而是能**主动行动**、**调用外部工具**、**与环境交互**。

---

### 🔹 二、从 GPT 到 ChatGPT

* **GPT-2 → GPT-3**：实现强大的语言生成；
* **InstructGPT（2022）**：引入人类反馈强化学习（RLHF），学会“听懂指令”；
* **ChatGPT（2022）**：对话式交互 + 工具调用；
* **GPT-4（2023）**：多模态输入 + 推理增强。

---

### 🔹 三、Agent 化的关键特征

| 能力    | 说明           | 示例                       |
| ----- | ------------ | ------------------------ |
| 目标驱动  | 能基于用户目标规划任务  | “帮我预订东京的行程”              |
| 工具使用  | 能调用外部API/数据库 | 浏览器、代码执行器、SQL查询          |
| 记忆机制  | 具备长期与短期记忆    | 回忆过去对话或上下文               |
| 推理与规划 | 多步思考与自我纠错    | Chain-of-Thought, ReAct  |
| 自主行动  | 能执行决策并返回结果   | AutoGPT, LangChain Agent |

---

### 🔹 四、智能体生态

* **LangChain / AutoGPT / BabyAGI**：构建多步骤任务代理；
* **OpenAI GPTs / Assistants API**：让 LLM 化身为个人助理；
* **Toolformer / ReAct**：融合推理与工具使用；
* **AI OS 概念**：未来将是以 Agent 为核心的操作系统时代。

---

### 🔹 五、从大模型到通用智能（AGI）的路径

> 机器学习 → 深度学习 → Transformer → 大模型 → 智能体 → **通用人工智能（AGI）**

大模型是“智能的核心引擎”，
智能体是“智能的外在形态”，
两者结合构成了未来 AI 的操作基座。

---

## 📘 小结

| 阶段         | 关键技术                | 代表模型                | 特征      | 意义         |
| ---------- | ------------------- | ------------------- | ------- | ---------- |
| 词向量        | 分布式语义               | Word2Vec / GloVe    | 静态语义表示  | 语义理解的起点    |
| 上下文预训练     | 双向建模                | ELMo / BERT         | 动态语义表示  | NLP 通用模型诞生 |
| 自回归语言模型    | Transformer Decoder | GPT 系列              | 生成式语言建模 | 通用生成能力     |
| 智能体（Agent） | LLM + 工具 + 记忆 + 推理  | AutoGPT / LangChain | 主动智能    | 向 AGI 迈进   |


* any list
{:toc}