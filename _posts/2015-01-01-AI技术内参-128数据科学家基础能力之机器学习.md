---
layout: post
title:  AI技术内参-128数据科学家基础能力之机器学习
date:   2015-01-01 23:20:27 +0800
categories: [AI技术内参]
tags: [AI技术内参, other]
published: true
---



128 数据科学家基础能力之机器学习
想要成为合格的，或者更进一步成为优秀的人工智能工程师或数据科学家，机器学习的各种基础知识是必不可少的。然而，机器学习领域浩如烟海，各类教材和入门课程层出不穷。特别是机器学习基础需要不少的数学知识，这对于想进入这一领域的工程师而言，无疑是一个比较高的门槛。

今天，我来和你聊一聊如何学习和掌握机器学习基础知识，又如何通过核心的知识脉络快速掌握更多的机器学习算法和模型。

## 监督学习和无监督学习

要问机器学习主要能解决什么问题，抛开各式各样的机器学习流派和层出不穷的算法模型不谈，**机器学习主要解决的是两类问题：监督学习和无监督学习。掌握机器学习，主要就是学习这两类问题，掌握解决这两类问题的基本思路**。

什么是解决这两类问题的基本思路呢？基本思路，简而言之就是“套路”。放在这里的语境，那就是指：

* 如何把现实场景中的问题抽象成相应的数学模型，并知道在这个抽象过程中，数学模型有怎样的假设。
* 如何利用数学工具，对相应的数学模型参数进行求解。
* 如何根据实际问题提出评估方案，对应用的数学模型进行评估，看是否解决了实际问题。

这三步就是我们学习监督学习和无监督学习，乃至所有的机器学习算法的核心思路。机器学习中不同模型、不同算法都是围绕这三步来展开的，我们不妨把这个思路叫作“三步套路”。

那什么是监督学习呢？监督学习是指这么一个过程，我们通过外部的响应变量（Response Variable）来指导模型学习我们关心的任务，并达到我们需要的目的。这也就是“监督学习”中“监督”两字的由来。

也就是说，**监督学习的最终目标，是使模型可以更准确地对我们所需要的响应变量建模**。比如，我们希望通过一系列特征来预测某个地区的房屋销售价格，希望预测电影的票房，或者希望预测用户可能购买的商品。这里的“销售价格”、“电影票房”以及“可能购买的商品”都是监督学习中的响应变量。

那什么是无监督学习呢？通常情况下，无监督学习并没有明显的响应变量。**无监督学习的核心，往往是希望发现数据内部的潜在结构和规律，为我们进行下一步决断提供参考**。典型的无监督学习就是希望能够利用数据特征来把数据分组，机器学习语境下叫作“聚类”。

根据不同的应用场景，聚类又有很多变种，比如认为某一个数据点属于一个类别，或者认为某一个数据点同时属于好几个类别，只是属于每个类别的概率不同等等。

无监督学习的另外一个作用是为监督学习提供更加有力的特征。通常情况下，无监督学习能够挖掘出数据内部的结构，而这些结构可能会比我们提供的数据特征更能抓住数据的本质联系，因此监督学习中往往也需要无监督学习来进行辅助。

我们简要回顾了机器学习中两大类问题的定义。在学习这两大类模型和算法的时候，有这么一个技巧，就是要不断地回归到上面提到的基本思路上去，就是这个“三步套路”，反复用这三个方面来审视当前的模型。另外，我们也可以慢慢地体会到，任何新的模型或者算法的诞生，往往都是基于旧有的模型算法，在以上三个方面中的某一个或几个方向有所创新。

## 监督学习的基础

监督学习的基础是三类模型：

* 线性模型
* 决策树模型
* 神经网络模型

**掌握这三类模型就掌握了监督学习的主干**。利用监督学习来解决的问题，占所有机器学习或者人工智能任务的绝大多数。这里面，有90%甚至更多的监督学习问题，都可以用这三类模型得到比较好的解决。

这三类监督学习模型又可以细分为处理两类问题：

* 分类问题
* 回归问题

**分类问题的核心是如何利用模型来判别一个数据点的类别**。这个类别一般是离散的，比如两类或者多类。**回归问题的核心则是利用模型来输出一个预测的数值**。这个数值一般是一个实数，是连续的。

有了这个基本的认识以后，我们利用前面的思路来看一下如何梳理监督学习的思路。这里用线性模型的回归问题来做例子。但整个思路可以推广到所有的监督学习模型。

**线性回归模型**（Linear Regression）是所有回归模型中最简单也是最核心的一个模型。我们依次来看上面所讲的“三步套路”。

首先第一步，我们需要回答的问题是，线性回归对现实场景是如何抽象的。顾名思义，线性回归认为现实场景中的响应变量（比如房价、比如票房）和数据特征之间存在线性关系。而线性回归的数学假设有两个部分：

* 响应变量的预测值是数据特征的线性变换。这里的参数是一组系数。而预测值是系数和数据特征的线性组合。
* 响应变量的预测值和真实值之间有一个误差。这个误差服从一个正态（高斯）分布，分布的期望值是0，方差是σ的平方。

有了这样的假设以后。第二步就要看线性回归模型的参数是如何求解的。这里从历史上就衍生出了很多方法。比如在教科书中一般会介绍线性回归的解析解（Closed-form Solution）。线性回归的解析解虽然简单优美，但是在现实计算中一般不直接采用，因为需要对矩阵进行逆运算，而矩阵求逆运算量很大。解析解主要用于各种理论分析中。

线性回归的参数还可以用数值计算的办法，比如梯度下降（Gradient Descent）的方法求得近似结果。然而梯度下降需要对所有的数据点进行扫描。当数据量很多的时候，梯度下降会变得很慢。于是随机梯度下降（Stochastic Gradient Descent）算法就应运而生。随机梯度下降并不需要对所有的数据点扫描后才对参数进行更新，而可以对一部分数据，有时甚至是一个数据点进行更新。

从这里我们也可以看到，**对于同一个模型而言，可以用不同的算法来求解模型的参数。这是机器学习的一个核心特点**。

最后第三步，我们来看如何评估线性回归模型。由于线性回归是对问题的响应变量进行一个实数预测。那么，最简单的评估方式就是看这个预测值和真实值之间的绝对误差。如果对于每一个数据点我们都可以计算这么一个误差，那么对于所有的数据点而言，我们就可以计算一个平均误差。

上述对于线性回归的讨论可以扩展到监督学习的三类基本模型。这样你就可以很快掌握这些模型的特点和这些模型算法之间的联系。

## 无监督学习的基础

现实中绝大多数的应用场景并不需要无监督学习。然而无监督学习中很多有价值的思想非常值得初学者掌握。另外，**无监督学习，特别是深度学习支持下的无监督学习，是目前机器学习乃至深度学习的前沿研究方向**。所以从长远来看，了解无监督学习是非常必要的。

我们前面说到，无监督学习的主要目的就是挖掘出数据内在的联系。这里的根本问题是，不同的无监督学习方法对数据内部的结构有不同的假设。因此，无监督学习不同模型之间常常有很大的差别。在众多无监督学习模型中，聚类模型无疑是重要的代表。了解和熟悉聚类模型有助于我们了解数据的一些基本信息。

聚类模型也有很多种类。这里我们就用最常见的、非常重要的**K均值算法**（K-means），来看看如何通过前面讲过的“三步套路”来掌握其核心思路。

首先，K均值算法认为数据由K个类别组成。每个类别内部的数据相距比较近，而距离所有其他类别中的数据都比较遥远。这里面的数学假设，需要定义数据到一个类别的距离以及距离函数本身。在K均值算法中，数据到一个类别的距离被定义为到这个类别的平均点的距离。这也是K均值名字的由来。而距离函数则采用了欧几里得距离，来衡量两个数据点之间的远近。

直接求解K均值的目标函数是一个NP难（NP-hard）的问题。于是大多数现有的方法都是用迭代的贪心算法来求解。

一直以来，对聚类问题、对无监督学习任务的评估都是机器学习的一个难点。无监督学习没有一个真正的目标，或者是我们之前提到的响应变量，因此无法真正客观地衡量模型或者算法的好坏。

对于K均值算法而言，比较简单的衡量指标就是，看所有类别内部的数据点的平均距离和类别两两之间的所有点的平均距离的大小。如果聚类成功，则类别内部的数据点会相距较近，而类别两两之间的所有点的平均距离则比较远。

以上我们通过“三步套路”的三个方面讨论了K均值算法的核心思路，这种讨论方法也适用所有的聚类模型和算法。

## 小结

当你可以熟练使用我今天介绍的“三步套路”，去分析更多监督学习和无监督学习的模型算法以后，对于基础的内容，也就是教科书上经常讲到的内容，你就可以去看这些内容究竟是在讲解这三个方面的哪个方面。

**对于绝大多数模型来说，第一部分往往是最重要的，也就是说，这个模型究竟和现实问题的联系是什么**。第二部分，也就是模型的求解，取决于模型本身的复杂度和成熟度，现在很多模型往往都有现成的软件包提供求解过程。而第三部分，模型的评估则在现实生产中至关重要。**牢牢把握这三个方面，来对机器学习模型算法进行讨论，是成长为成熟数据科学家必不可少的过程**。

今天我为你讲了掌握机器学习基础知识的一些核心思路。一起来回顾下要点：第一，机器学习主要的任务有监督学习和无监督学习。这两种机器学习任务的很多模型和算法都可以用一个“三步套路”的思路来进行分析。第二，我们用线性回归作为例子探讨了如何用这个“三步套路”来分析监督学习的模型和算法。第三，我们用K均值聚类算法作为例子探讨了如何用“三步套路”来分析无监督学习的模型和算法。

最后，给你留一个思考题，在现实场景中，当你发现一个模型并没有很好地解决你的问题时，从这个“三步套路”的角度来看，究竟哪个方面最容易出问题？




# 参考资料

https://learn.lianglianglee.com/%e4%b8%93%e6%a0%8f/AI%e6%8a%80%e6%9c%af%e5%86%85%e5%8f%82/128%20%e6%95%b0%e6%8d%ae%e7%a7%91%e5%ad%a6%e5%ae%b6%e5%9f%ba%e7%a1%80%e8%83%bd%e5%8a%9b%e4%b9%8b%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0.md

* any list
{:toc}
