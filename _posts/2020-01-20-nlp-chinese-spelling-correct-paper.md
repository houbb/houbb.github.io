---
layout: post
title: NLP 中文拼写检测纠正 Paper
date:  2020-1-20 10:09:32 +0800
categories: [Data-Struct]
tags: [data-struct, block-chain, sh]
published: true
---

# 摘要

本文介绍了SIGHAN 2015 拼写中文拼写检查，包括任务描述，数据准备， 绩效指标和评估结果。 

比赛揭示了当前处理中文拼写检查的最新NLP技术。 

所有此次测试中使用的带有黄金标准和评估工具的数据集可公开获取，以备将来研究之用。

# 介绍

中文拼写检查器相对较难开发，部分原因是中文单词之间不存在单词定界符，并且中文单词只能包含单个字符或多个字符。

此外，还有超过13,000个汉字，而不是英文的26个字母，每个字母都有自己的上下文构成一个有意义的中文单词。

所有这些使中文拼写检查成为一项艰巨的任务。

一项实证分析表明，中文拼写错误通常是由多个字符单词之间的混淆引起的，这些单词在语音和视觉上相似，但在语义上却截然不同（Liu等，2011）。

自动拼写检查器应具有以下两项功能：

**识别拼写错误，并建议错误用法的正确字符。**

SIGHAN 2013年中国拼写检查竞赛是第一个提供数据集作为中国拼写检查员绩效评估基准的活动（Wu等人，2013）。

SIGHAN 2013中的数据来自中国母语人士撰写的论文。

根据第一次评估的经验，在CIPS-SIGHAN CLP-2014联合会议上举行了第二次评估，会议重点是汉语作为外语（CFL）的学习者撰写的论文（Yu等，2014）。 。

由于在CFL leaners书面论文中检测和纠正拼写错误方面面临的更大挑战，SIGHAN 2015 Bake-off再次具有中文拼写检查任务，为自动中文拼写检查器的开发和实施提供评估平台。

给定由几句话组成的段落，检查人员应识别所有可能的拼写错误，突出显示其位置并提出可能的更正

本文的其余部分安排如下。

第2部分概述了SIGHAN 2015年中文拼写检查的流程。

第三部分介绍了开发的数据集。

第4节提出了评估指标。

第5节比较了各个参赛者的结果。

最后，我们在结论中总结了本文，并在第6节中提供了未来的研究方向。

# 任务描述

这项任务的目的是评估中文拼写检查器的功能。

输入的段落包含几个有/没有拼写错误的句子。

检查器应返回错误字符的位置并建议正确的字符。

每个字符或标点符号占用1个点进行计数
位置。

输入实例具有唯一的通道编号pid。

如果句子中没有拼写错误，则检查器应返回“ pid，0”。

如果输入段落中至少包含一个拼写错误，则输出格式为 `“pid [, location, correction]+”` ，其中符号“ +”表示存在一个或多个预测元素的实例
`“ [，位置，更正]”。` 

“位置”和“更正”分别表示不正确字符的位置及其正确版本。


## 例子如下。


- Example 1

```
Input: (pid=A2-0047-1) 我真的洗碗我可以去看你
Output: A2-0047-1, 4, 希, 5, 望
```

这里实际是音近字。

xiwan

xiwang

所以中文纠错真的更难了--

- Example 2

```
Input: (pid=B2-1670-2) 在日本，大學生打工的情況是相當普偏的。
Output: B2-1670-2, 17, 遍
```

偏
遍

这个是形近字

- Example 3

```
Input: (pid=B2-1903-7) 我也是你的朋友，我會永遠在你身邊。
Output: B2-1903-7, 0
```

Ex中有2个错误的字符。 

1，正确的字符“希”和“望”应分别在位置4和5中使用。 

在Ex。 2，第17个字符“偏”是错误的，应为“遍”。

位置“ 0”表示Ex。中没有拼写错误。 3


# 资料准备

用于我们任务的学习者语料库是从在台湾管理的基于计算机的汉语作为外语考试（TOCFL）的论文部分中收集的。

拼写错误是由受过训练的中文母语者手动注释的，他们还会提供与每个错误对应的更正。

然后将论文分为以下三组

（1）训练集：该训练集包括970篇精选论文，总共3,143个拼写错误。

每篇文章以SGML格式表示，如图1所示。

title属性用于描述文章主题。

每个段落由几个句子组成，每个段落至少包含一个拼写错误，并且数据既指示错误的位置，又指示相应的更正。

这套文章中的所有文章都用于训练已开发的拼写检查器

（2）Dryrun SET

共有39篇文章提供给参与者，使他们熟悉最终的测试过程。

每个参与者可以提交使用不同模型生成的多个运行，并使用其检查器的不同参数设置。

除了确保可以正确评估提交的结果之外，参与者还可以在试运行阶段微调其开发的模型。

空运行的目的是仅验证提交的输出格式，官方评估中未考虑空运行结果

（3）测试集

这套包含1100条测试段落。

这些段落中有一半没有拼写错误，而另一半则至少包含一个拼写错误。

评估以公开测试的形式进行。

除了提供的数据集外，还允许注册的参与者团队使用任何语言和计算资源来检测和纠正拼写错误。

此外，由CFL学习者撰写的文章可能会产生语法错误，单词遗漏或多余，单词选择不正确或单词顺序问题。

所讨论的任务仅专注于拼写错误纠正。

```xml
<ESSAY title="學中文的第一天">
<TEXT>
<PASSAGE id="A2-0521-1"> 這位小姐說：你應
該一直走到十只路口，再右磚一直走經過一家銀
行就到了。</PASSAGE>
<PASSAGE id="A2-0521-2">應為今天是第一天，
老師先請學生自己給介紹。</PASSAGE>
</TEXT>
<MISTAKE id="A2-0521-1" location="15">
<WRONG>十只路口</WRONG>
<CORRECTION>十字路口</CORRECTION>
</MISTAKE>
<MISTAKE id="A2-0521-1" location="21">
<WRONG>右磚</WRONG>
<CORRECTION>右轉</CORRECTION>
</MISTAKE>
<MISTAKE id="A2-0521-2" location="1">
<WRONG>應為</WRONG>
<CORRECTION>因為</CORRECTION>
</MISTAKE>
</ESSAY>
```

Figure 1. An essay represented in SGML format

# 性能指标

表1显示了用于性能评估的混淆矩阵。 

在矩阵中，TP（True Positive）是通过拼写检查器正确识别的带有拼写错误的段落数； 

FP（假阳性）是识别出不存在的错误的段落数； 

TN（True Negative）是没有正确拼写错误的段落数量

这样确定； FN（假阴性）是指拼写错误没有发现错误的段落数。

判断正确性的标准按以下两个级别确定。

（1）检测水平：给定段落中所有不正确字符的位置应与黄金标准完全相同。

（2）校正级别：所有位置和不正确字符的相应校正应与黄金标准完全相同。

除了获得令人满意的检测/校正性能外，降低假阳性率（即错误不存在时的错误识别）也很重要（Wu等人，2010）。

ps: 就是性能与正确率。

在混淆矩阵的帮助下，在两个级别上都测量了以下指标。

```
False Positive Rate (FPR) = FP / (FP+TN)

Accuracy = (TP+TN) / (TP+FP+TN+FN)

Precision = TP / (TP+FP)

Recall = TP / (TP+FN)

F1= 2 *Precision*Recall/(Precision+Recall)
```

![image](https://user-images.githubusercontent.com/18375710/72768665-174ad400-3c33-11ea-9d0d-2757ac5c57c7.png)

## 例子

例如，如果5个采用黄金标准的测试输入是

### FPR

- False Positive Rate (FPR) = 0.5 (=1/2)

Notes: {“A2-0092-2, 5”}/{“A2-0092-2, 0”, “B2-2731-1, 0”}

### Detection-level

- Accuracy =0.6 (=3/5)

Notes: {“A2-0243-1, 3, 4”, “B2-1923-2, 8, 41”, “B2-2731-1, 0”} / {“A2-0092-2, 5”, A2-0243-1, 3, 4”, “B2-1923-2, 8, 41”, “B2-2731-1, 0”, “B2-3754-3, 11”}

- Precision = 0.5 (=2/4)

Notes: {“A2-0243-1, 3, 4”, “B2-1923-2, 8, 41”} / {“A2-0092-2, 5”, A2-0243-1, 3, 4”, “B2-1923-2, 8, 41”, “B2-3754-3, 11”}

- Recall = 0.67 (=2/3).

Notes: {“A2-0243-1, 3, 4”, “B2-1923-2, 8, 41”} / {A2-0243-1, 3, 4”, “B2-1923-2, 8, 41”, “B2-3754-3, 10”}

评估工具将产生以下性能：

```
F1=0.57 (=2*0.5*0.67/(0.5+0.67))
```

### Correction-level

-  Accuracy =0.4 (=2/5)

Notes: {“B2-1923-2, 8, 誤, 41, 情”, “B2- 2731-1, 0”} / {“A2-0092-2, 5, 玩”, “A2- 0243-1, 3, 件, 4, 康”, “B2-1923-2, 8, 誤, 41, 情”, “B2-2731-1, 0”, “B2-3754-3, 11, 觀”}

- Precision = 0.25 (=1/4)

Notes: {“B2-1923-2, 8, 誤, 41, 情”} / {“A2-0092-2, 5, 玩”, “A2-0243-1, 3, 件, 4, 康”, “B2-1923-2, 8, 誤, 41, 情”, “B2-3754-3, 11, 觀”}

- Recall = 0.33 (=1/3)

Notes: {“B2-1923-2, 8, 誤, 41, 情”} / {, “A2-0243-1, 3, 健, 4, 康”, “B2-1923-2, 8, 誤, 41, 情”, “B2-3754-3, 10, 觀”}

- `F1=0.28 (=2*0.25*0.33/(0.25+0.33))`

# 评价结果

表2汇总了9个参与团队的提交统计数据，其中包括来自中国大学和研究机构的4个团队（CAS，ECNU，SCAU和WHU），来自台湾的4个团队（KUAS， NCTU＆NTUT，NCYU和NTOU）和一间私人公司（Lingage）。

在9个注册团队中，有6个团队提交了他们的测试结果。 

在正式测试阶段，每个参与者最多可以提交三个采用不同模型或参数设置的运行。

总共我们收到了15次运行。

![image](https://user-images.githubusercontent.com/18375710/72769821-0308d600-3c37-11ea-99a3-37d16fdad077.png)

表3显示了任务测试结果。 

NCTU＆NTUT研究团队的假阳性率最低，为0.0509。

对于检测级别评估，根据测试数据分布，基线系统可以通过始终将所有测试用例报告为正确无误的方式来达到0.5的准确度。

CAS提交的系统结果取得了超过0.7的良好性能。

我们使用F1分数来反映准确性和召回率之间的权衡。

如测试结果所示，CAS提供了最佳的错误检测结果，F1得分高达0.6404。

对于校正级别的评估，CAS系统（0.6918）提供的校正精度明显优于
其他团队。

此外，在改正精度和召回率方面，CAS开发的拼写检查器也优于其他的，

F1得分最高，为0.6254。

![image](https://user-images.githubusercontent.com/18375710/72769843-1b78f080-3c37-11ea-8e69-13c7c02f057d.png)

请注意，很难纠正输入段落中发现的所有拼写错误，因为某些句子包含多个错误，而在我们的评估中，仅纠正其中一些错误被视为错误情况。

表4总结了参与者开发的方法和语言资源的用法。 在提交正式测试结果的6个团队中，NCYU没有提交其开发方法的报告。

尽管考虑到不同的度量标准，CAS和NCTU＆NTUT提交的那些系统都提供了相对最佳的整体性能，但没有一个提交的系统在所有度量标准上都提供了卓越的性能。

CAS团队提出了统一的中文拼写纠正框架。

他们使用了**基于HMM的方法来分割句子并生成更正候选。**

然后，**采用两阶段过滤过程对候选进行重新排序，以选择最有前途的候选。**

NCTU＆NTUT团队提出了一种**基于词向量/条件随机场的拼写错误检测器。**

ps: 这里可以看到原理和分词很类似，都是基于概率或者熵去处理的。其实我们人在阅读的时候也是同理的，大部分都是基于概率推断出答案的。

他们利用错误检测结果来指导并加快耗时的语言模型记录过程。

通过这种方式，可以在具有最大语言模型得分的修饰句子中检测并纠正潜在的中文拼写错误。

![image](https://user-images.githubusercontent.com/18375710/72770025-9d691980-3c37-11ea-958c-957182ebfc40.png)

# 结论与未来工作

本文概述了SIGHAN 2015年中文拼写检查的概况，包括任务设计，数据准备，评估指标，性能评估结果以及参与团队使用的方法。

无论实际表现如何，所有提交的内容都有助于寻找有效的中文拼写检查程序的知识，并且Bake-off程序中的各个报告提供了对中文处理的有用见解。

我们希望为此Bakeoff收集的数据集可以促进并加快有效的中文拼写检查程序的未来开发。

因此，所有具有黄金标准和评估工具的数据集均可在 [http://ir.itc.ntnu.edu.tw/lre/sighan8csc.html](http://ir.itc.ntnu.edu.tw/lre/sighan8csc.html) 上公开获得。

未来的方向集中在中文语法错误纠正的发展上。

我们计划建立新的语言资源，以帮助改进现有的计算机辅助中文学习技术。 

此外，将从CFL学习者那里获得的新数据集将进行调查，以进一步丰富该研究主题。

# 个人说活

这篇 paper 虽然没有说具体的实现方式，但是整体的评估思路是提供了的。

而且也说了大概的实现方式，基于 HMM 或者 CRF 的方式。

# 参考资料

[didi-拼写检测](https://chinesenlp.xyz/#/zh/docs/spell_correction)

[Introduction to SIGHAN 2015 Bake-off for Chinese Spelling Check](https://www.aclweb.org/anthology/W15-3106.pdf)

[data 下载地址](http://nlp.ee.ncu.edu.tw/resource/csc_download.html)

* any list
{:toc}