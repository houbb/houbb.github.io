---
layout: post
title:  dl4j doc-03-minist 手写识别 Lenet-5 paper 论文学习 《Gradient-Based Learning Applied to Document Recognition》 III
date:  2017-04-16 12:03:32 +0800
categories: [Deep Learning]
tags: [AI, DL, dl4j, neural network]
published: true
---

# 前言

书接上回，我们继续学习 lenet5.

# IV. 多模块系统和图变换网络

经典的反向传播算法，如前文所述和使用的那样，是一种简单形式的梯度下降学习。

然而，很明显，由方程4描述的梯度反向传播算法描述了比简单的由交替的线性变换和Sigmoid函数组成的多层前馈网络更一般的情况。

原则上，导数可以通过任何功能模块的任何排列进行反向传播，只要我们可以计算这些模块的雅可比矩阵与任何向量的乘积。

为什么我们想要训练由多个异构模块组成的系统？

答案是，**大型和复杂的可训练系统需要由简单的、专门的模块构建**。

最简单的例子是LeNet-5，它混合了卷积层、子采样层、全连接层和RBF层。

另一个不那么平凡的例子，在接下来的两个部分中描述，是一个用于识别单词的系统，该系统可以被训练以同时分割和识别单词，而无需提供正确的分割。

图14显示了一个可训练的多模块系统的示例。多模块系统由每个模块实现的函数和模块之间的相互连接图定义。

- F14

![F14](https://img-blog.csdnimg.cn/direct/f1fb545694bc49eca477b7788b6d516e.png#pic_center)

图隐含地定义了一个部分顺序，根据这个顺序，模块必须在正向传递中更新。

例如，在图14中，首先更新模块0，然后更新模块1和2（可能并行），最后更新模块3。模块可以有或没有可训练的参数。测量系统性能的损失函数作为模块4实现。

在最简单的情况下，损失函数模块接收一个携带所需输出的外部输入。

在这个框架中，可训练参数（W1、W2在图中）、外部输入和输出（Z、D、E）以及中间状态变量（X1、X2、X3、X4、X5）之间没有定性上的区别。

# A. 面向对象的方法

面向对象编程提供了一种特别方便的实现多模块系统的方式。

每个模块都是一个类的实例。模块类具有一个称为 "forward propagation" 的方法（或成员函数），称为 fprop，其参数是模块的输入和输出。

例如，在图14中计算模块3的输出可以通过调用模块3的方法 fprop，并传入参数 X3、X4、X5 来完成。复杂的模块可以通过简单地定义一个新类来构建，该类的插槽将包含成员模块和这些模块之间的中间状态变量。该类的 fprop 方法只需调用成员模块的 fprop 方法，并将适当的中间状态变量或外部输入和输出作为参数传递。虽然算法很容易推广到任何这样的模块网络，包括具有循环的模块网络，我们将限制讨论到有向无环图（前馈网络）的情况。

在多模块系统中计算导数同样简单。可以为每个模块类定义一个称为 "backward propagation" 的方法，称为 bprop，为此目的。模块的 bprop 方法接受与 fprop 方法相同的参数。可以通过在反向传播阶段逆序调用所有模块的 bprop 方法来计算系统中的所有导数。

状态变量假定包含插槽，用于在反向传播期间存储计算的梯度，以及用于在前向传播期间计算的状态的存储。反向传播有效地计算损失 E 关于系统中所有状态变量和参数的偏导数。某些模块的前向和后向函数之间存在有趣的对偶性质。例如，在前向方向中的几个变量的求和在后向方向中被转换为简单的扇出（复制）。

相反，在前向方向中的扇出被转换为后向方向中的求和。用于获取本文描述的结果的软件环境称为 SN3.1，它使用了上述概念。它基于一种自制的面向对象的 Lisp 方言，具有一个到 C 的编译器。

导数可以通过在反向图中进行传播来计算的事实在直观上很容易理解。

理论上证明它的最佳方法是通过拉格朗日函数的使用。相同的形式主义可以用于将程序扩展到具有递归连接的网络。


# B. 特殊模块

神经网络和许多其他标准模式识别技术可以用梯度下降学习来制定多模块系统。

常用的模块包括矩阵乘法和Sigmoid模块，它们的组合可用于构建传统的神经网络。其他模块包括卷积层、子采样层、RBF层和“softmax”层。

损失函数也被表示为模块，其单个输出产生损失的值。常用模块具有简单的bprop方法。一般来说，函数F的bprop方法是F的Jacobian矩阵的乘积。

以下是几个常用示例。fanout（“Y”连接）的bprop方法是求和，反之亦然。乘以系数的bprop方法是乘以相同系数。乘以矩阵的bprop方法是乘以该矩阵的转置。与常数相加的bprop方法是恒等式。

有趣的是，某些非可微模块可以被插入到多模块系统中而不产生不良影响。其中一个有趣的例子是复用器模块。它有两个（或更多）常规输入、一个切换输入和一个输出。该模块根据切换输入的（离散）值选择其输入之一，并将其复制到其输出上。虽然该模块在切换输入方面是不可微的，但在常规输入方面是可微的。因此，包含此类模块的系统的整体函数将在不涉及参数的情况下与其参数可微。例如，切换输入可以是外部输入。另一个有趣的例子是最小模块。该模块有两个（或更多）输入和一个输出。该模块的输出是输入的最小值。该模块的函数在所有地方都是可微的，除了切换表面，切换表面是零测度集。有趣的是，该函数是连续且相当规则的，这足以确保梯度下降学习算法的收敛。

多模块思想的面向对象实现可以轻松扩展以包括一个传播高斯-牛顿近似二阶导数的bbprop方法。这导致了附录中给出的模块化系统的二阶导数反向传播方程22的直接泛化。

复用器模块是一个更一般情况的特例，在第八节中详细描述，在该情况下，系统的体系结构会根据输入数据动态变化。复用器模块可用于动态地为每个新的输入模式重新连接（或重新配置）系统的体系结构。


# C. 图变换网络

多模块系统是构建大型可训练系统的非常灵活的工具。然而，在前面的部分中，描述隐含地假定参数集合和模块之间通信的状态信息都是固定大小的向量。

对于许多应用而言，固定大小向量的数据表示的灵活性不足是一个严重的缺陷，特别是对于处理变长输入的任务（例如连续语音识别和手写词识别），或者对于需要编码对象或特征之间关系的任务，这些对象或特征的数量和性质可能会变化（不变感知、场景分析、复合对象识别）。

一个重要的特例是字符或单词串的识别。

更普遍地，固定大小向量在必须编码概率分布的任务中缺乏灵活性，这些分布涵盖了向量或符号序列，正如在语言处理中的情况一样。这些分布最好由随机语法表示，或者在更一般的情况下，由每个弧包含一个向量的有向图表示（随机语法是特殊情况，其中向量包含概率和符号信息）。图中的每条路径代表一个不同的向量序列。可以通过将与每个弧相关联的数据元素解释为概率分布的参数或简单地作为惩罚来表示序列上的分布。序列上的分布对于在语音或手写识别系统中建模语言知识特别方便：每个序列，即每条图中的路径，代表输入的一种替代解释。连续的处理模块逐步完善解释。

例如，语音识别系统可以从单个声学向量序列开始，将其转换为音素网格（音素序列分布），然后转换为单词网格（单词序列分布），最后转换为单词的单个序列，表示最佳解释。

在我们构建大规模手写识别系统的工作中，我们发现通过将系统视为以一个或多个图形作为输入和输出的模块网络来更容易地快速开发和设计这些系统。这样的模块称为图形变换器，完整的系统称为图形变换器网络，或GTN。GTN中的模块以形式为数值信息（标量或向量）的有向图的形式通信它们的状态和梯度。

从统计学的角度来看，传统网络的固定大小状态向量可以被看作是状态空间中分布的均值。在变大小网络中，例如第VII节中描述的空间位移神经网络中，状态是变长序列的固定大小向量。它们可以被视为代表变长序列的固定大小向量的概率分布的均值。在GTNs中，状态被表示为图形，它们可以被看作是在结构化集合（可能是序列）上的概率分布的混合物（图15）。

接下来几节的主要观点之一是显示梯度下降学习过程不仅限于通过固定大小向量通信的简单模块网络，而且可以推广到GTNs。通过图形变换器的梯度反向传播获取了关于输出图中的数值信息的梯度，并计算了关于附加到输入图形的数值信息以及模块的内部参数的梯度。只要使用可微函数将数值数据从输入图中产生到输出图中，并考虑到函数的参数，就可以应用梯度下降学习。

接下来几节的第二个观点是显示在典型文档处理系统（以及其他图像识别系统）中使用的许多模块实现的功能，虽然通常被认为是组合性质的，但实际上是关于其内部参数以及其输入可微分的，并且因此可以作为全局可训练系统的一部分使用。

在接下来的大部分内容中，我们将有意避免参考概率理论。所有操作的量都被视为惩罚，或成本，如果必要，可以通过取指数和归一化来将其转换为概率。

# 参考资料

http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf

* any list
{:toc}
