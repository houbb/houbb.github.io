---
layout: post
title: 第5章　优化与数值计算
date: 2025-11-03 20:40:12 +0800
categories: [AI]
tags: [ai, learn-note]
published: true
---


## 第5章　优化与数值计算

优化问题是机器学习的核心。

学习算法的本质是 **寻找最优参数，使模型在给定任务上表现最佳**。

这一过程几乎总可以形式化为一个“优化问题”：

[
\min_\theta ; L(\theta)
]

其中 ( \theta ) 是模型参数，( L(\theta) ) 是损失函数或目标函数。
本章将介绍从损失函数设计到优化算法、再到数值陷阱与模型复杂度控制的系统思维。

---

### **5.1 损失函数与目标函数**

#### ✅ 一、定义与作用

损失函数（Loss Function）是衡量模型输出与真实标签之间误差的函数。
优化算法的任务，就是让损失函数尽可能小。

常见形式如下：

| 任务类型  | 损失函数                             | 数学表达式                                         |             |
| ----- | -------------------------------- | --------------------------------------------- | ----------- |
| 回归    | 均方误差 (MSE)                       | ( L = \frac{1}{N}\sum_i (y_i - \hat{y}_i)^2 ) |             |
| 分类    | 交叉熵损失 (Cross Entropy)            | ( L = -\sum_i y_i \log(\hat{y}_i) )           |             |
| 支持向量机 | 合页损失 (Hinge Loss)                | ( L = \max(0, 1 - y_i w^T x_i) )              |             |
| 概率建模  | 对数似然损失 (Negative Log-Likelihood) | ( L = -\log P(y                               | x;\theta) ) |

#### ✅ 二、目标函数与正则项

有时我们不仅希望拟合数据，还希望模型“泛化”得更好，于是引入正则化：

[
J(\theta) = L(\theta) + \lambda \Omega(\theta)
]

* ( L(\theta) )：经验风险（对训练样本的平均损失）
* ( \Omega(\theta) )：正则化项（控制模型复杂度）
* ( \lambda )：权衡因子

#### ✅ 三、风险最小化原则

机器学习中两种典型思想：

* **经验风险最小化 (ERM)**：直接最小化训练集损失
* **结构风险最小化 (SRM)**：在经验风险和模型复杂度之间做权衡（如SVM）

---

### **5.2 梯度下降与随机梯度下降**

#### ✅ 一、梯度下降（Gradient Descent, GD）

梯度下降是优化的核心算法。
其思想简单直接：**沿着梯度下降方向迭代更新参数**。

[
\theta_{t+1} = \theta_t - \eta \nabla_\theta L(\theta_t)
]

* ( \eta )：学习率 (Learning Rate)
* ( \nabla_\theta L )：损失函数对参数的梯度

当学习率太大 → 振荡甚至发散；
当学习率太小 → 收敛过慢。

#### ✅ 二、随机梯度下降（SGD）

全批量梯度下降对大数据集成本太高，于是出现了 **随机梯度下降**：

[
\theta_{t+1} = \theta_t - \eta \nabla_\theta L_i(\theta_t)
]

其中 ( L_i ) 是单个样本或小批量（mini-batch）的损失。
SGD 能够更快地更新参数、避免陷入局部最优。

#### ✅ 三、改进算法

现代优化算法在SGD的基础上做了改进：

| 算法           | 特点                            |
| ------------ | ----------------------------- |
| **Momentum** | 增加“惯性”，平滑梯度方向，加快收敛            |
| **AdaGrad**  | 自适应学习率，适合稀疏特征                 |
| **RMSProp**  | 改进AdaGrad的学习率衰减问题             |
| **Adam**     | 结合Momentum和RMSProp，成为深度学习默认选择 |

Adam的更新规则：

[
m_t = \beta_1 m_{t-1} + (1-\beta_1)\nabla_\theta L_t
]
[
v_t = \beta_2 v_{t-1} + (1-\beta_2)(\nabla_\theta L_t)^2
]
[
\theta_{t+1} = \theta_t - \eta \frac{m_t}{\sqrt{v_t} + \epsilon}
]

---

### **5.3 正则化与模型复杂度控制**

#### ✅ 一、为什么需要正则化

在高维空间中，模型往往可以“完美”拟合训练数据，但在测试数据上表现很差——即 **过拟合**。
正则化就是控制这种“过度灵活性”的手段。

#### ✅ 二、常见正则化方法

| 方法                 | 数学形式               | 作用          |        |   |        |              |
| ------------------ | ------------------ | ----------- | ------ | - | ------ | ------------ |
| **L2 正则化 (Ridge)** | ( \Omega(\theta) = |             | \theta |   | _2^2 ) | 平滑参数，抑制大权重   |
| **L1 正则化 (Lasso)** | ( \Omega(\theta) = |             | \theta |   | _1 )   | 稀疏化参数，实现特征选择 |
| **Dropout**        | 随机丢弃神经元            | 提高模型鲁棒性     |        |   |        |              |
| **Early Stopping** | 提前停止训练             | 避免模型对训练集过拟合 |        |   |        |              |
| **数据增强**           | 扩大样本空间             | 提高泛化能力      |        |   |        |              |

#### ✅ 三、模型复杂度与容量

* 模型容量太小 → 欠拟合（bias高）
* 模型容量太大 → 过拟合（variance高）
  → **偏差-方差权衡** 是正则化的核心思想。

---

### **5.4 优化陷阱：局部最优、鞍点、梯度消失**

#### ✅ 一、局部最优（Local Minima）

非凸优化问题（如神经网络）可能存在多个局部最优解。
幸运的是，**在高维空间中，局部最优通常表现接近全局最优**。

#### ✅ 二、鞍点（Saddle Point）

鞍点是指梯度为零但既非最小也非最大的位置。
它在高维空间中比局部最优更常见。
优化算法常会卡在鞍点处，导致训练停滞。

#### ✅ 三、梯度消失与梯度爆炸

尤其在深层网络中：

* 梯度**消失**：导致前层权重几乎不更新
* 梯度**爆炸**：导致训练不稳定

**解决思路：**

* 使用 **ReLU** 激活函数（避免梯度消失）
* 使用 **Batch Normalization**（稳定梯度分布）
* **梯度裁剪 (Gradient Clipping)**（控制爆炸）
* **残差结构 (ResNet)**（保证梯度可传播）

#### ✅ 四、数值稳定性与优化技巧

* 使用对数技巧：(\log(\sum e^x)) 替换 (\sum e^x)
* 在实现时避免浮点下溢/上溢
* 归一化输入特征，加快收敛
* 动态调整学习率（Learning Rate Scheduling）

---

### **总结**

| 模块    | 关键思想      | 实践意义     |
| ----- | --------- | -------- |
| 损失函数  | 定义“学习目标”  | 衡量模型误差   |
| 优化算法  | 找到最优参数    | 决定模型能否学好 |
| 正则化   | 控制复杂度     | 提高泛化能力   |
| 优化陷阱  | 理解局部/鞍点问题 | 指导算法改进   |
| 数值稳定性 | 避免计算问题    | 保证训练稳定性  |


* any list
{:toc}