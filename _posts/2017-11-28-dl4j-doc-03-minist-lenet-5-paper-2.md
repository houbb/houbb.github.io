---
layout: post
title:  dl4j doc-03-minist 手写识别 Lenet-5 paper 论文学习 《Gradient-Based Learning Applied to Document Recognition》 II
date:  2017-04-16 12:03:32 +0800
categories: [Deep Learning]
tags: [AI, DL, dl4j, neural network]
published: true
---

# 前言

书接上回，我们继续学习 lenet5.

# III 结果及与其他方法的比较

尽管识别单个数字只是设计实用识别系统中的众多问题之一，但它是比较形状识别方法的一个优秀基准。

虽然许多现有方法结合了手工制作的特征提取器和可训练的分类器，但本研究集中于直接操作大小归一化图像的自适应方法。

## A 一个数据库：修改后的NIST数据集

本文描述的系统所使用的训练和测试数据库是从NIST（美国国家标准与技术研究院）的特殊数据库1和特殊数据库2中构建的，其中包含手写数字的二进制图像。NIST最初将特殊数据库1标识为他们的训练集，特殊数据库2标识为他们的测试集。然而，特殊数据库1比特殊数据库2更清洁、更容易识别。这一原因在于特殊数据库1是从人口普查局的员工中收集的，而特殊数据库2是从高中学生中收集的。从学习实验中得出合理的结论要求结果独立于训练集和测试集的选择，包括完整样本集中的样本。因此，有必要通过混合NIST的数据集来构建一个新的数据库。

特殊数据库2包含由250位不同的写作者书写的810,000个数字图像。与特殊数据库1不同，在特殊数据库2中，来自每个作者的数据块并不是按顺序出现的，而是被打乱了。特殊数据库2的作者身份是已知的，我们利用了这些信息来还原作者的顺序。然后，我们将特殊数据库2分成两部分，前250位写作者书写的字符被放入我们的新训练集中，剩下的250位写作者被放入我们的测试集中。因此，我们有了两个几乎各含约405,000个示例的数据集。新的训练集被补充完整，其中包括从特殊数据库1开始的足够数量的示例，从第20,000个图案开始，以形成14,500个训练模式的完整集合。类似地，新的测试集被补充完整，其中包括从特殊数据库1开始的示例，从第20,000个图案开始，以形成14,500个测试模式的完整集合。在这里描述的实验中，我们仅使用了特殊数据库2中的部分测试图像和特殊数据库1中的部分测试图像，但我们使用了完整的训练样本集。得到的数据库被称为修改后的NIST或MNIST数据集。

原始的黑白双色图像被尺寸归一化为一个28x28像素的框，同时保持它们的长宽比。由于归一化算法使用了抗锯齿技术（图像插值），所得到的图像包含灰度级。数据库的三个版本分别是：在第一个版本中，图像被居中放置在一个28x28像素的图像中，通过计算像素的质心并将图像平移到使得该点位于28x28场的中心。在某些情况下，这个28x28场被扩展到32x32，以填充背景像素。这个版本的数据库将被称为常规数据库。在数据库的第二个版本中，字符图像被去斜并裁剪到28x28像素的图像。

去斜通过计算像素的惯性矩来完成（将前景像素计为1，背景像素计为0），并水平移动线条，使得主轴垂直。这个版本的数据库将被称为去斜数据库。在数据库的第三个版本中，用于一些早期实验的图像被缩小到14x14像素。常规数据库（包含6000个训练示例和1000个测试示例，尺寸归一化为28x28，并通过质心在32x32场中心）可在  http://www.research.att.com/yann/ocr/mnist 获得。

图中显示了从测试集中随机选取的示例。


# B. 结果

在常规的MNIST数据库上训练了几个版本的LeNet-5。

每个会话通过整个训练数据进行了20次迭代。全局学习率的值（见附录C中的方程21的定义）按照以下时间表递减：在前两次迭代中为0.0005，接下来的三次为0.0002，再接下来的三次为0.0001，再接下来的四次为0.00005，之后为0.00001。在每次迭代之前，对500个样本重新评估对角Hessian近似，如附录C中所述，并在整个迭代过程中保持不变。参数设置为0.02。

在第一次迭代期间，结果有效学习率在参数集合上变化在大约7 x 10^(-5) 到 0.016之间。通过训练集的大约10次迭代后，测试误差率稳定在0.95%左右。训练集上的误差率在经过19次迭代后达到0.35%。许多作者报道了在训练神经网络或其他自适应算法时观察到的过拟合的普遍现象。

当发生过拟合时，训练误差随着时间的推移而持续下降，但测试误差在一定次数的迭代后经历最小值，并开始增加。虽然这种现象非常普遍，但在我们的情况中并未观察到，如图5所示的学习曲线。

一个可能的原因是学习率被保持相对较大。这样做的效果是权重永远不会稳定在局部最小值上，而是保持随机振荡。

由于这些波动，平均成本将在更广泛的最小值上更低。因此，随机梯度将具有类似于**偏爱更广泛最小值的正则化项的效果**。

更广泛的最小值对参数分布的熵具有较大的好处，这对于泛化误差是有利的。


训练集大小的影响通过使用15,000、30,000和60,000个示例对网络进行训练来进行测量。结果显示在图6中。显然，即使使用像LeNet-5这样的专用架构，**更多的训练数据也会提高准确性**。

为了验证这一假设，我们通过随机扭曲原始训练图像来人工生成更多的训练示例。

增加的训练集由60,000个原始模式和540,000个扭曲模式的实例组成，这些扭曲模式具有随机选择的扭曲参数。

**扭曲是以下平面仿射变换的组合：水平和垂直平移、缩放、挤压（同时水平压缩和垂直拉伸，或者相反），以及水平剪切**。

PS: 可以利用这种技巧拓展训练集合，但是有时候可能效果一般。不过这个论文的结果是效果更好了。

图7显示了用于训练的扭曲模式示例。当使用扭曲数据进行训练时，测试误差率降至0.8%（无变形时为0.95%）。与无变形时相同的训练参数被使用。训练会话的总长度保持不变（每个包含60,000个模式的20次迭代）。有趣的是，这20次迭代中，网络实际上只看到每个个体样本两次。

图8显示了所有82个被错误分类的测试示例。其中一些示例确实存在歧义，但有几个示例在人类看来是完全可识别的，尽管它们是以一种不常见的风格书写的。

这表明使用更多的训练数据可以期待进一步的改进。

- F5/6/7/8

![!F5-8](https://img-blog.csdnimg.cn/direct/894fb12cc2b542d6bb78e7bfd8121771.png#pic_center)


# C. 与其他分类器的比较

为了进行比较，对同一数据库进行了多种其他可训练分类器的训练和测试。这些结果的早期子集已在[51]中展示过。

各种方法在测试集上的错误率如图9所示。

- F9

![F9](https://img-blog.csdnimg.cn/direct/1d005926ef9e42d4a74f986199161416.png#pic_center)

## C.1 线性分类器和成对线性分类器 Linear Classifer, and Pairwise Linear Classifer

可能考虑的最简单的分类器之一是线性分类器。每个输入像素值都对每个输出单元的加权和有贡献。具有最高总和（包括偏置常量的贡献）的输出单元指示输入字符的类别。在常规数据上，错误率为12%。网络有7850个自由参数。在去斜图像上，测试错误率为8.4%。网络有4010个自由参数。线性分类器的不足已经得到了充分的记录，它在这里只是为了与更复杂的分类器进行比较。各种组合的sigmoid单元、线性单元、梯度下降学习和直接求解线性系统的学习给出了类似的结果。

对基本线性分类器的简单改进进行了测试。其想法是训练单层网络的每个单元，将每个类与其他每个类区分开来。在我们的情况下，该层包括45个标记为0/1、0/2、...0/9、1/2...8/9的单元。单元i=j被训练为在类别i的模式上产生+1，在类别j的模式上产生-1，并且不会在其他模式上进行训练。类i的最终得分是所有标记为i=x的单元输出的总和减去所有标记为y=i的单元输出的总和，其中x和y表示所有可能的类别。在常规测试集上的错误率为7.6%。

## C.2 基准最近邻分类器 Baseline Nearest Neighbor Classifer

另一个简单的分类器是使用输入图像之间的欧几里德距离的K最近邻分类器。该分类器的优点是不需要训练时间和设计者的大脑。

然而，内存需求和识别时间较大：完整的60,000个20x20像素训练图像（每个像素一个字节，约24兆字节）必须在运行时可用。可以设计出更紧凑的表示方式，稍微增加错误率。在常规测试集上，错误率为5.0%。在去斜数据上，错误率为2.4%，k = 3。自然地，一个实际的欧几里德距离最近邻系统将在特征向量上而不是直接在像素上进行操作，但由于本研究中所提出的其他所有系统都是直接在像素上操作，因此这个结果对于基准比较是有用的。


## C.3 主成分分析（PCA）和多项式分类器  Principal Component Analysis (PCA) and Polynomial

根据[53]、[54]的方法，构建了一个预处理阶段，该阶段计算输入模式在训练向量集的40个主成分上的投影。

为了计算主成分，首先计算每个输入分量的均值，并从训练向量中减去。然后计算所得向量的协方差矩阵，并使用奇异值分解进行对角化。40维特征向量被用作二次多项式分类器的输入。

这个分类器可以看作是一个线性分类器，有821个输入，前面有一个模块计算所有输入变量对的乘积。在常规测试集上的错误率为3.3%。

## C.4 径向基函数网络 Radial Basis Function Network

根据[55]的方法，构建了一个RBF网络。

第一层由1,000个高斯RBF单元组成，具有28x28的输入，第二层是一个简单的1000个输入/10个输出的线性分类器。

RBF单元被分为10组，每组包含100个。每组单元使用自适应K均值算法对对应类别的所有训练示例进行训练。

第二层权重使用正则化伪逆方法计算。在常规测试集上的错误率为3.6%。

## C.5 单隐藏层全连接多层神经网络  One-Hidden Layer Fully Connected Multilayer Neural Network

我们测试的另一个分类器是一个具有两层权重（一个隐藏层）的全连接多层神经网络，使用附录C中描述的反向传播版本进行训练。在常规测试集上，对于具有300个隐藏单元的网络，错误率为4.7%，对于具有1000个隐藏单元的网络，错误率为4.5%。使用人工扭曲生成更多的训练数据只带来了边际改善：对于300个隐藏单元，错误率为3.6%，对于1000个隐藏单元，错误率为3.8%。

当使用去斜图像时，对于具有300个隐藏单元的网络，测试错误率降至1.6%。

对于具有如此多自由参数的网络能够实现相当低的测试错误仍然有些神秘。

我们推测，在多层网络中，梯度下降学习的动态具有“自我正则化”效应。因为权重空间的原点是几乎每个方向都有吸引力的鞍点，所以在前几个时期内权重无论如何都会缩小（最近的理论分析似乎证实了这一点）。小权重使得S型函数在准线性区域操作，使得网络本质上等效于低容量的单层网络。随着学习的进行，权重增长，逐渐增加了网络的有效容量。

这似乎是对Vapnik的“结构风险最小化”原则的一个几乎完美的，尽管是偶然的实现。对这些现象的更好的理论理解和更多的经验证据肯定是需要的。

## C.6 两个隐藏层的全连接多层神经网络 Two-Hidden Layer Fully Connected Multilayer Neural Network

为了观察架构的效果，训练了几个两个隐藏层的多层神经网络。理论结果表明，任何函数都可以由一个隐藏层的神经网络来近似。

然而，一些作者观察到，两个隐藏层的架构有时在实际情况下表现更好。这种现象在这里也观察到了。

一个28x28-300-100-10网络的测试错误率为3.05%，比一个隐藏层网络的结果要好得多，而且使用了稍微更多的权重和连接。将网络大小增加到28x28-1000-150-10只能带来略微改善的错误率：2.95%。使用扭曲模式进行训练在一定程度上改善了性能：28x28-300-100-10网络的错误率为2.50%，28x28-1000-150-10网络的错误率为2.45%。
  
## C.7 一个小的卷积网络：LeNet-1 A Small Convolutional Network: LeNet-1

卷积网络是为了解决小网络无法学习训练集和大网络似乎参数过多的困境而设计的。

LeNet-1是卷积网络架构的早期体现，这里包括了它进行比较的目的。图像被下采样为16x16像素，并居中放置在28x28的输入层中。尽管评估LeNet-1需要约100,000次乘加操作，但它的卷积性质使得自由参数数量仅约为2600。

LeNet-1架构是使用我们自己版本的USPS（美国邮政服务邮政编码）数据库开发的，并且其大小经过调整以匹配可用数据。

LeNet-1达到了1.7%的测试错误率。一个参数数量如此少的网络能够达到如此好的错误率，表明这种架构对于任务是合适的。

## C.8 LeNet-4

通过对LeNet-1的实验，明确了需要一个更大的卷积网络来充分利用大规模训练集的优势。LeNet-4和后来的LeNet-5就是为了解决这个问题而设计的。

LeNet-4与LeNet-5非常相似，除了架构的细节之外。它包含4个第一级特征图，然后是8个子采样图，每对连接到第一层特征图，然后是16个特征图，接着是16个子采样图，然后是一个具有120个单元的全连接层，最后是输出层（10个单元）。LeNet-4包含大约260,000个连接，大约有17,000个自由参数。

测试错误率为1.1%。在一系列实验中，我们将LeNet-4的最后一层替换为欧几里得最近邻分类器，并使用Bottou和Vapnik的“局部学习”方法[58]，即每次显示新的测试模式时重新训练一个局部线性分类器。

这两种方法都没有改善原始的错误率，尽管它们提高了拒绝性能。

## C.9 提升的LeNet-4

根据R. Schapire的理论工作[59]，Drucker等人[60]开发了将多个分类器组合的“提升”方法。三个LeNet-4被组合在一起：第一个是以通常的方式训练的。

第二个是在第一个网络的过滤模式上进行训练，以便第二台机器看到一种混合模式，其中50%是第一个网络正确的，50%是错误的。

最后，第三个网络在第一个和第二个网络不一致的新模式上进行训练。

在测试期间，三个网络的输出简单相加。由于LeNet-4的错误率非常低，必须使用人工扭曲的图像（与LeNet-5一样）才能获得足够的样本来训练第二个和第三个网络。测试错误率为0.7%，是我们所有分类器中最好的。乍一看，提升似乎比单个网络昂贵三倍。事实上，当第一个网络产生高置信度的答案时，不会调用其他网络。平均计算成本约为单个网络的1.75倍。


## C.10 切线距离分类器（TDC） Tangent Distance Classifer 

切线距离分类器（TDC）是一种最近邻方法，其中距离函数对输入图像的小扭曲和平移不敏感[61]。

如果我们将图像视为高维像素空间中的一个点（其中维数等于像素数），那么字符的演变失真将在像素空间中绘制出一条曲线。所有这些失真结合在一起，定义了像素空间中的低维流形。对于小的扭曲，在原始图像的邻近区域，这个流形可以用一个平面来近似，称为切线平面。对字符图像“接近度”的一个很好的度量是它们切线平面之间的距离，用于生成这些平面的失真集合包括平移、缩放、倾斜、挤压、旋转和线条粗细的变化。使用16x16像素图像达到了1.1%的测试错误率。

使用简单的欧几里得距离在多个分辨率上进行预处理技术，可以减少必要的切线距离计算数量。

## C.11 支持向量机（SVM）

多项式分类器是用于生成复杂决策面的经过深入研究的方法。不幸的是，对于高维问题，它们是不切实际的，因为乘积项的数量是禁止的。支持向量技术是在高维空间中表示复杂表面的极其经济的方法，包括多项式和许多其他类型的表面[6]。

特别有趣的决策面的一个子集是与高维空间中两个类的凸壳之间的最大距离对应的超平面。

Boser、Guyon和Vapnik[62]意识到，这个“最大间隔”集合中的任何k次多项式都可以通过首先计算输入图像与一组训练样本（称为“支持向量”）的点积，然后将结果提升到k次幂，并线性组合得到的数字来计算。

找到支持向量和系数等于解一个具有线性不等式约束的高维二次最小化问题。为了进行比较，我们在这里包括了Burges和Scholkopf在[63]中报告的结果。使用常规的SVM，他们在常规测试集上的错误率为1.4%。Cortes和Vapnik报告说，使用略有不同的技术，SVM在相同数据上的错误率为1.1%。这种技术的计算成本非常高：每次识别大约需要1400万次乘加操作。使用Scholkopf的虚拟支持向量技术（V-SVM），达到了1.0%的错误率。最近，Scholkopf（个人通信）使用了V-SVM的修改版本，达到了0.8%的错误率。

不幸的是，V-SVM的成本非常高：比常规SVM高出约两倍。

为了缓解这个问题，Burges提出了减少集支持向量技术（RS-SVM），它在常规测试集上达到了1.1%的错误率[63]，每次识别的计算成本只有650,000次乘加操作，即比LeNet-5贵大约60%。

- F10 F11

![F10](https://img-blog.csdnimg.cn/direct/5ed57d87571048eea0ae44f8f5c6570a.png#pic_center)


- F12

![F12](https://img-blog.csdnimg.cn/direct/95a1d5bfdcc64a5a90cca21799b40b6f.png#pic_center)


# D. 讨论

图9至图12显示了各分类器性能的总结。图9显示了在10,000个示例测试集上的原始错误率。增强的LeNet-4表现最佳，达到了0.7%，紧随其后的是LeNet-5，达到了0.8%。

图10显示了一些方法在达到0.5%错误率时必须拒绝的测试集模式数量。当相应输出的值小于预定义的阈值时，将拒绝模式。在许多应用中，拒绝性能比原始错误率更重要。用于决定拒绝模式的得分是顶部两个类的得分之间的差异。同样，增强的LeNet-4具有最佳性能。改进后的LeNet-4比原始的LeNet-4效果更好，即使原始精度相同。

图11显示了对于每种方法识别单个尺寸归一化图像所需的乘累加操作数量。可预期的是，神经网络比基于内存的方法要求少得多。由于其规则的结构和对权重的低内存需求，卷积神经网络特别适合硬件实现。已经显示出LeNet-5的前身的单芯片混合模拟数字实现的速度超过每秒1000个字符[64]。然而，主流计算机技术的迅速发展使得这些异域技术迅速过时。由于其巨大的内存需求和计算需求，基于内存的技术的成本效益实现更加难以实现。

还测量了训练时间。K最近邻和TDC的训练时间基本上为零。尽管单层网络、成对网络和PCA+二次网络的训练时间不到一小时，但多层网络的训练时间预计会更长，但只需要10到20次通过训练集。这相当于在Silicon Graphics Origin 2000服务器上用单个200MHz R10000处理器训练LeNet-5需要2到3天的CPU时间。值得注意的是，尽管训练时间对设计者来说有些相关，但对系统的最终用户来说并不感兴趣。在现有技术和以较大的训练时间为代价的新技术之间选择时，任何最终用户都会选择后者，即使它带来了边际精度的提高。

图12显示了以需要存储的变量数量（因此是自由参数数量）衡量的各种分类器的内存需求。大多数方法对于获得良好性能仅需要大约每个变量一个字节。然而，最近邻方法可能需要每个像素4位来存储模板图像。不足为奇，神经网络所需的内存远远少于基于内存的方法。

总体性能取决于许多因素，包括精度、运行时间和内存需求。随着计算机技术的改进，更大容量的识别器变得可行。反过来，更大的识别器需要更大的训练集。

LeNet-1在1989年适合当时的技术，就像现在的LeNet-5一样。

在1989年，一个像LeNet-5这样复杂的识别器需要几周的训练，以及更多的数据，因此甚至没有被考虑过。相当长一段时间，LeNet-1被认为是最先进的技术。局部学习分类器、最优间隔分类器和切线距离分类器的开发是为了改进LeNet-1——它们成功了。然而，它们反过来又激发了对改进的神经网络架构的搜索。这种搜索在一定程度上是根据从训练和测试错误的测量中得出的各种学习机的容量估计来引导的。我们发现需要更多的容量。通过一系列架构实验，以及对识别错误特征的分析，我们创造了LeNet-4和LeNet-5。

我们发现增强技术在精度上有显著提高，而在内存和计算开销上有相对较小的惩罚。

此外，失真模型可以用来增加数据集的有效大小，而无需实际收集更多的数据。

支持向量机具有优异的精度，这是非常了不起的，因为与其他高性能分类器不同，它不包括关于问题的先验知识。

实际上，如果图像像素被固定映射置换并丢失其图像结构，这个分类器也能表现得很好。

然而，要达到与卷积神经网络相媲美的性能水平，只能以巨大的内存和计算需求为代价。减少集SVM的要求与卷积网络相差不到两倍，错误率非常接近。预计这些结果的改进，因为这种技术是相对较新的。

当有大量数据可用时，许多方法都可以达到令人满意的精度。

**与基于内存的技术相比，神经网络的运行速度更快，所需空间更少。随着训练数据库的不断增加，神经网络的优势将变得更加明显**。



# E. 不变性和抗噪性

卷积网络特别适用于识别或拒绝具有广泛变化的大小、位置和方向的形状，例如实际世界中的字符串识别系统通常产生的形状。

在上面描述的实验中，噪声抗性和变形不变性的重要性并不明显。

大多数实际应用中的情况大不相同。字符通常必须在识别之前从其上下文中分割出来。

分割算法很少是完美的，通常会在字符图像中留下杂音、下划线、相邻字符等不必要的标记，有时会过度切割字符并产生不完整的字符。这些图像无法可靠地进行尺寸归一化和居中。

对不完整字符进行归一化可能非常危险。例如，放大的杂点可能看起来像真正的数字1。因此，许多系统已经采用在字段或单词级别上对图像进行归一化的方法。

在我们的情况下，检测到整个字段的上下轮廓（支票中的金额）并用于将图像归一化到固定高度。虽然这确保了杂点不会被放大成类似字符的图像，但这也在分割后产生了大小和垂直位置的大幅变化。因此，最好使用对此类变化具有鲁棒性的识别器。图13显示了LeNet-5正确识别的几个扭曲字符的示例。估计在大约两倍的尺度变化、加减大约字符高度一半的垂直移位变化以及加减30度的旋转情况下都可以进行准确识别。尽管完全不变形的复杂形状识别仍然是一个难以实现的目标，但看起来卷积网络对于几何变形的不变性或鲁棒性问题提供了部分答案。

图13还包括LeNet-5在极端嘈杂条件下的稳健性的示例。对这些图像进行处理对于许多方法来说会造成无法克服的分割和特征提取问题，但LeNet-5似乎能够从这些混乱的图像中稳健地提取显著特征。此处显示的网络的训练集是MNIST训练集，其中添加了椒盐噪声。每个像素以0.1的概率随机翻转。 

LeNet-5的更多示例操作请参见 http://www.research.att.com/~yann/ocr


# 参考资料

http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf

* any list
{:toc}
