---
layout: post
title: dive-into-llms-02-deeplearning 深度学习基础知识入门
date: 2025-11-03 20:40:12 +0800
categories: [AI]
tags: [ai, learn-note]
published: true
---


# 背景

学习 [dive-into-llms](https://github.com/Lordog/dive-into-llms)

# chat

## 深度学习基础知识入门

这一步正是「从会用大模型 → 理解大模型」的关键转折。

你想要快速建立「深度学习基础认知」——不是搞学术，而是能**听懂大模型背后的逻辑**。

那我们就用最通俗易懂的方式讲清楚。

## 🧠 一、深度学习到底是什么？

一句话讲清楚：

> 深度学习就是「让计算机通过大量数据自动学出规律」的过程。

比如：

* 传统写规则：`if "error" in log: return "异常"`
* 深度学习：给模型几百万条日志，它自己学出什么样的句子意味着“异常”。

它的核心就是一个「神经网络（Neural Network）」——一种数学函数拟合器。
它能“看例子”，自动调节参数，让输出更接近目标。

---

## 🧩 二、神经网络是什么？

我们来看一张简化图👇

```
输入层 → 隐藏层 → 输出层
```

举个例子：
你要让模型判断一条报警日志是不是「磁盘问题」。

| 输入（特征）             | 输出（预测）  |
| ------------------ | ------- |
| “磁盘已满” → [1,0,0]   | ✅ 磁盘问题  |
| “CPU负载高” → [0,1,0] | ❌ CPU问题 |
| “网络中断” → [0,0,1]   | ❌ 网络问题  |

模型内部其实是很多个加权求和 + 激活函数：

```
y = f(Wx + b)
```

* **x**：输入（比如句子向量）
* **W**：权重（模型学习的参数）
* **b**：偏置（修正项）
* **f()**：非线性函数（ReLU / Sigmoid）

每个神经元都像是在做一个“加权投票”：

> 各个特征加权求和 → 判断结果 → 传递到下一层。

层层堆叠，形成“深度网络”。

---

## 🔁 三、反向传播（Backpropagation）

反向传播是训练神经网络的核心机制。

想象你教小孩投篮：

* 他第一次扔太低 → 你告诉他“往上扔点”
* 第二次太远 → “往回一点”
* 经过反复试错，他学会合适的力度。

神经网络也是这样：

1. **前向传播（Forward）**：计算预测结果；
2. **计算损失（Loss）**：预测和真实差多少；
3. **反向传播（Backward）**：根据差距更新权重（往“更好”的方向修正）。

公式上：

```
w_new = w_old - learning_rate * d(loss)/dw
```

这就是所谓的“梯度下降法（Gradient Descent）”。

---

## ⚙️ 四、优化器（Optimizer）

优化器负责“怎么更新权重”，也就是训练策略。

| 优化器      | 特点       | 场景          |
| -------- | -------- | ----------- |
| SGD      | 最基础的梯度下降 | 小模型、理论教学    |
| Momentum | 加速度，避免震荡 | 中小模型        |
| Adam     | 动态调节学习率  | 主流模型几乎都用    |
| RMSprop  | 稳定收敛     | RNN/LSTM 常用 |

现在几乎所有 Transformer 模型都默认用 **AdamW**（Adam + 权重衰减）。

---

## 💔 五、损失函数（Loss Function）

损失函数告诉模型“你离正确答案还有多远”。

| 类型 | 示例任务       | 常用损失函数                 |
| -- | ---------- | ---------------------- |
| 分类 | 日志分类、情感分析  | 交叉熵（CrossEntropy）      |
| 回归 | 异常值预测      | 均方误差（MSE）              |
| 生成 | 语言模型预测下一个词 | 交叉熵                    |
| 嵌入 | 相似度计算      | 对比损失（Contrastive Loss） |

例子：

```python
Loss = -Σ (真实概率 × log(预测概率))
```

模型训练的目标就是：

> 不断调整参数，使 Loss 越来越小。

---

## 🎢 六、过拟合与欠拟合

| 情况  | 含义              | 特征         | 解决办法             |
| --- | --------------- | ---------- | ---------------- |
| 过拟合 | 模型记住了训练数据，没学到规律 | 训练集很好，测试集差 | 增加数据、正则化、Dropout |
| 欠拟合 | 模型太简单，学不动       | 训练和测试都差    | 加层数、换更复杂模型       |

直观比喻👇

* **过拟合**：背题（考试换题就不会）
* **欠拟合**：连题都没看懂

---

## 🧮 七、深度学习工作流程全景图

1️⃣ 准备数据

* 收集、清洗、标注数据
* 转成张量格式（Tensor）

2️⃣ 定义模型

* 用 PyTorch / TensorFlow 定义结构
* 比如输入层、隐藏层、输出层

3️⃣ 定义损失函数 & 优化器

* `loss_fn = nn.CrossEntropyLoss()`
* `optimizer = torch.optim.Adam(model.parameters())`

4️⃣ 训练循环（epoch）

```python
for batch in data:
    pred = model(batch.x)
    loss = loss_fn(pred, batch.y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

5️⃣ 评估与保存模型

* 计算准确率、召回率
* 保存 `model.save_pretrained()`

---

## 📘 八、两本经典又易懂的“深度学习入门”资料

### ✅ 1. 《动手学深度学习（Dive into Deep Learning）》

* 作者：李沐、阿斯顿张等（亚马逊科学家）
* 免费在线阅读：[https://zh.d2l.ai/](https://zh.d2l.ai/)
* 特点：

  * 理论+代码结合，PyTorch/MXNet 双版本
  * 从感知机讲到卷积、RNN、Transformer
  * 中文解释非常通俗
* 建议读：第 1～5 章（基础部分）+ 第 9 章（Transformer）

---

### ✅ 2. 《深度学习入门：基于Python的理论与实现》

* 作者：斋藤康毅
* 特点：

  * 从零手写神经网络
  * 不用任何框架，理解反向传播、梯度下降原理
  * 代码短小易读
* 适合：想真正理解“为什么模型能学”的人
* 可配合 B 站视频《3小时入门神经网络》一起看。

---

## 🚀 九、建议学习顺序（快速通关版）

| 周     | 主题             | 学习重点                   | 实践               |
| ----- | -------------- | ---------------------- | ---------------- |
| 第 1 周 | 神经网络与反向传播      | 理解神经元怎么“学”             | 用 `numpy` 实现两层网络 |
| 第 2 周 | 损失函数与优化器       | 梯度下降、Adam 原理           | 可视化 loss 下降过程    |
| 第 3 周 | 过拟合、正则化        | Dropout、Early stopping | 训练一个简单分类模型       |
| 第 4 周 | Transformer 概念 | 注意力机制、预训练思想            | 跑一个 BERT 中文分类任务  |


* any list
{:toc}