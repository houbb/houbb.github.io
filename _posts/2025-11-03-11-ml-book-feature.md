---
layout: post
title: 第11章　特征工程与数据处理
date: 2025-11-03 20:40:12 +0800
categories: [AI]
tags: [ai, learn-note]
published: true
---


# 第11章　特征工程与数据处理

机器学习的成败，往往不在模型复杂度，而在于数据质量与特征表达能力。

这一章聚焦于如何「让模型看懂数据」，从特征提取、选择、编码到特征交互与自动化管理，系统解析特征工程的全流程。

---

## **11.1 特征提取、选择与编码**

### 🧱 1. 特征提取（Feature Extraction）

**目标：** 将原始数据（文本、图像、日志、信号等）转化为机器可理解的数值表示。

**典型方式：**

* **文本数据：**

  * Bag of Words（词袋模型）
  * TF-IDF（词频-逆文档频率）
  * Word2Vec / FastText（分布式词向量）
  * BERT embedding（上下文语义特征）
* **图像数据：**

  * 手工特征：SIFT、HOG、LBP
  * 深度特征：CNN 提取的高维嵌入
* **时间序列 / 传感器：**

  * 滑动窗口、FFT（频域变换）、统计量（均值、方差、峰度）
* **结构化数据：**

  * 比例、差值、交叉项、聚合统计（groupby + mean）

💡 *特征提取的本质：用数学结构描述现实世界的规律。*

---

### 🧮 2. 特征选择（Feature Selection）

**目标：** 在海量特征中挑出最有用的子集，降低过拟合、提升训练速度。

**方法分类：**

* **Filter（过滤法）**：独立评估单个特征与目标的关系
  如：相关系数、卡方检验、互信息、方差选择。
* **Wrapper（包装法）**：用模型性能来评估特征子集
  如：递归特征消除（RFE）、前向/后向选择。
* **Embedded（嵌入法）**：模型训练过程中自动筛选特征
  如：L1 正则（Lasso）、树模型的特征重要性（XGBoost feature_importances_）。

💡 *高维不等于高效，选择比堆砌更重要。*

---

### 🔠 3. 特征编码（Feature Encoding）

**目标：** 将非数值型或类别特征转换为数值表示。

**常见方式：**

| 类型    | 编码方法               | 说明                      |
| ----- | ------------------ | ----------------------- |
| 类别型   | One-Hot            | 为每个类别创建独立维度             |
| 类别型   | Label Encoding     | 将类别映射为整数（有序性假设风险）       |
| 类别型   | Target Encoding    | 用类别对应的平均目标值代替           |
| 高基数类别 | Embedding Encoding | 用低维向量表示（常用于推荐系统）        |
| 时间型   | 周期编码               | 将时间特征映射为 sin/cos（处理周期性） |

💡 *正确的编码方式决定了模型是否能“理解”特征的语义结构。*

---

## **11.2 标准化、归一化、缺失值处理**

### ⚖️ 1. 标准化（Standardization）

将数据转化为均值为 0、方差为 1 的分布。
公式：
[
x' = \frac{x - \mu}{\sigma}
]
适用于梯度敏感算法（如线性回归、SVM、神经网络）。

---

### 📏 2. 归一化（Normalization）

将数值压缩到指定区间（通常是 [0,1]）。
公式：
[
x' = \frac{x - \min(x)}{\max(x) - \min(x)}
]
适用于基于距离的模型（如 KNN、K-means）。

---

### ⚠️ 3. 缺失值处理

**缺失的原因**：系统故障、用户跳过输入、采样问题。

**常见策略：**

* 删除：样本或特征缺失太多。
* 填充：均值/中位数/众数/前值/模型预测。
* 增强：用缺失标志特征（is_null）标识。

💡 *缺失值往往蕴含隐含信息，比如“没填性别”可能代表某类用户。*

---

## **11.3 特征交互与高维稀疏化**

### 🔄 1. 特征交互（Feature Interaction）

通过组合多个特征，捕捉非线性关系。

**示例：**

* 二阶交互项：`x1 * x2`
* 逻辑交互：`(province, gender)` → “广东+女”
* 特征交叉编码（Cross Feature Embedding）：推荐系统中常见

**自动化生成方式：**

* PolynomialFeatures（sklearn）
* DeepFM / Wide&Deep（深度模型自动学习交互项）

---

### 🕳️ 2. 高维稀疏问题

特征交叉和编码会让特征空间爆炸，比如 One-Hot 后从几百维变成几十万维。

**常见解决策略：**

* 降维（PCA、SVD、AutoEncoder）
* 特征选择（L1正则）
* Hash Trick（哈希技巧）
* 稀疏矩阵存储（scipy.sparse）

💡 *高维并非高效，机器学习模型更怕噪声特征。*

---

## **11.4 自动特征工程与 Feature Store**

### 🤖 1. 自动特征工程（Auto Feature Engineering）

随着 AutoML 的兴起，特征工程也进入自动化时代。

**典型方法：**

* **FeatureTools**：基于时间序列关系自动生成交互特征。
* **Auto-sklearn / TPOT**：自动搜索特征组合 + 模型管道。
* **深度特征学习**：通过深度神经网络自动学习最优特征空间。

**优点：**

* 减少人工试错
* 适应大规模异构数据
* 支撑快速实验与生产部署

---

### 🧱 2. Feature Store（特征存储系统）

在企业级机器学习平台中，**Feature Store** 是特征工程的中枢。

**功能：**

* 特征计算与管理（版本、权限、监控）
* 在线/离线特征一致性保障
* 跨模型特征共享与复用
* 特征数据血缘追踪

**代表系统：**

* Uber Michelangelo Feature Store
* Feast（Google 开源）
* Tecton、Databricks Feature Store

💡 *Feature Store 让“特征”成为可复用的资产，而不只是训练阶段的临时产物。*

---

## ✅ 本章小结

| 模块            | 目标         | 关键思想   |
| ------------- | ---------- | ------ |
| 特征提取          | 从原始数据中提炼信号 | “看懂”数据 |
| 特征选择          | 去除无效与冗余特征  | 降噪提效   |
| 特征编码          | 转化非数值型特征   | 可计算化   |
| 标准化与归一化       | 保持尺度一致     | 提升优化效率 |
| 特征交互          | 捕捉非线性模式    | 提升表达力  |
| 自动特征工程        | 让系统自动挖掘特征  | 提升迭代速度 |
| Feature Store | 管理特征全生命周期  | 工程化落地  |


* any list
{:toc}