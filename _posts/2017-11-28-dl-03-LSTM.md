---
layout: post
title:  DL4j-03-LSTM
date:  2017-04-16 20:21:23 +0800
categories: [Deep Learning]
tags: [AI, DL, dl4j, neural network]
header-img: "static/app/res/img/kon-bg.jpeg"
published: true
---

# LSTM和循环网络基础教程

[循环网络](https://deeplearning4j.org/cn/lstm.html)是一类人工神经网络，用于识别诸如文本、基因组、手写字迹、语音等序列数据的模式，或用于识别传感器、股票市场、政府机构产生的数值型时间序列数据。

循环网络可以说是最强大的神经网络，甚至可以将图像分解为一系列图像块，作为序列加以处理。

由于循环网络拥有一种特定的记忆模式，而记忆也是人类的基本能力之一，所以下文会时常将循环网络与人脑的记忆活动进行类比。


# 前馈网络回顾

在前馈网络中，样例输入网络后被转换为一项输出；在进行有监督学习时，输出为一个标签。也就是说，前馈网络将原始数据映射到类别，识别出信号的模式，例如一张输入图像应当给予“猫”还是“大象”的标签。

我们用带有标签的图像定型一个前馈网络，直到网络在猜测图像类别时的错误达到最少。将参数，即权重定型后，网络就可以对从未见过的数据进行分类。

已定型的前馈网络可以接受任何随机的图片组合，而输入的第一张照片并不会影响网络对第二张照片的分类。看到一张猫的照片不会导致网络预期下一张照片是大象。

这是因为网络并**没有时间顺序**的概念，它所考虑的唯一输入是当前所接受的样例。前馈网络仿佛患有短期失忆症；它们**只有早先被定型时的记忆**。


# 循环网络

循环网络与前馈网络不同，其输入不仅包括当前所见的输入样例，还包括网络在上一个时刻所感知到信息。

循环网络在第 **t-1** 个时间步的判定会影响其在随后第 **t** 个时间步的判定。所以循环网络有来自当下和不久之前的两种输入，此二者的结合决定了网络对于新数据如何反应，与人类日常生活中的情形颇为相似。

循环网络与前馈网络的区别便在于这种不断将自身上一刻输出当作输入的反馈循环。人们常说循环网络是有记忆的。为神经网络添加记忆的目的在于：序列本身即带有信息，而循环网络能利用这种信息完成前馈网络无法完成的任务。

这些顺序信息保存在循环网络隐藏状态中，不断向前层层传递，跨越许多个时间步，影响每一个新样例的处理。

由于循环网络具有时间维度，所以可能用动画示意最为清楚（最先出现的节点垂直线可被视为一个前馈网络，随时间展开后变为循环网络）。

![kpZBDfV](https://raw.githubusercontent.com/houbb/resource/master/img/DL/LSTM/2017-04-16-kpZBDfV.gif)


在上图中，每一个x都是一个输入样例，w 是用于筛选输入的权重，a 是隐藏层的激活状态（附加权重后的输入与上一个隐藏状态之和），而b则是隐藏层用修正线性或sigmoid单元进行变换（或称“挤压”）后的输出。


# 沿时间反向传播（BPTT）

前文提到，循环网络的目的是准确地对序列输入进行分类。我们依靠误差反向传播和梯度下降来达成这一目标。

前馈网络的反向传播从最后的误差开始，经每个隐藏层的输出、权重和输入反向移动，将一定比例的误差分配给每个权重，方法是计算权重与误差的偏导数`－∂E/∂w`，即两者变化速度的比例。
随后，梯度下降的学习算法会用这些偏导数对权重进行上下调整以减少误差。

循环网络则使用反向传播的一种扩展方法，名为沿时间反向传播，或称BPTT。在这里，时间其实就表示为一系列定义完备的有序计算，将时间步依次连接，而这些计算就是反向传播的全部内容。

无论循环与否，神经网络其实都只是形如 `f(g(h(x)))` 的嵌套复合函数。增加时间要素仅仅是扩展了函数系列，我们用链式法则计算这些函数的导数。

> 截断式BPTT

截断式BPTT是完整BPTT的近似方法，也是处理较长序列时的优先选择，因为时间步数量较多时，完整BPTT每次参数更新的正向/反向运算量会变的非常高。

该方法的缺点是，由于截断操作，梯度反向移动的距离有限，因此网络能够学习的依赖长度要短于完整的BPTT。

# 梯度消失（与膨胀）

正如直线表示x如何随着y的变化而改变，梯度表示所有权重随误差变化而发生的改变。如果梯度未知，则无法朝减少误差的方向调整权重，网络就会停止学习。

由于深度神经网络的层和时间步通过乘法彼此联系，导数有可能消失或膨胀。(一直乘以一个大于1/小于1的数字)

梯度膨胀的问题相对比较容易解决，因为可以将其截断或挤压。而消失的梯度则有可能变得过小，以至于计算机**无法处理**，网络无法学习－这个问题更难解决。

# 长短期记忆单元（LSTM）

LSTM可保留误差，用于沿时间和层进行反向传递。LSTM将误差保持在更为恒定的水平，让循环网络能够进行许多个时间步的学习（超过1000个时间步），从而打开了建立远距离因果联系的通道。

LSTM将信息存放在循环网络正常信息流之外的门控单元中。这些单元可以存储、写入或读取信息，就像计算机内存中的数据一样。
单元通过门的开关判定存储哪些信息，以及何时允许读取、写入或清除信息。但与计算机中的数字式存储器不同的是，这些门是模拟的，包含输出范围全部在0～１之间的sigmoid函数的逐元素相乘操作。
相比数字式存储，模拟值的优点是`可微分`，因此适合反向传播。

这些门依据接收到的信号而开关，而且与神经网络的节点类似，它们会用自有的权重集对信息进行筛选，根据其强度和导入内容决定是否允许信息通过。
这些权重就像调制输入和隐藏状态的权重一样，会通过循环网络的学习过程进行调整。也就是说，记忆单元会通过猜测、误差反向传播、用梯度下降调整权重的迭代过程学习何时允许数据进入、离开或被删除。


![lstm](https://raw.githubusercontent.com/houbb/resource/master/img/DL/LSTM/2017-04-16-gers_lstm.png)



* any list
{:toc}






