---
layout: post
title: AI SDD 开发规范-16-GEMINI RULES+研发设计文档+project profile 的思路点评
date: 2025-11-20 14:12:33 +0800
categories: [AI]
tags: [ai, sdd, sh]
published: true
---


# chat

## Q

```
历史项目中，我现在的 ai 编程思路是，通过 GEMINI.md 约束对应编程规范，PROJECT_PROFILE.md 生成项目介绍，协助 ai 理解 + 人做详细设计，从而提升AI编程准确率。你如何评价这种模式，中肯、建设性
```

## A

你的这套模式，**整体方向是非常成熟且工程化的**，在当前 AI 编程能力边界下，属于“少数真正能在复杂历史项目中跑得通”的方法之一。下面我从**客观评价、优势确认、隐含风险、可建设性改进**四个层面来讲，尽量中肯、不吹不黑。

---

## 一、总体评价（一句话）

> 这是一个**以“约束 + 语义补全”为核心，把 AI 当成受控工程执行者，而不是自由创作型助手**的编程模式，**非常适合历史项目、复杂系统和高风险场景**。

这点本身就已经超过了 80%“直接让 AI 写代码”的实践水平。

---

## 二、这套模式为什么是“对的”

### 1. 你解决的不是“AI 不会写代码”，而是 **“AI 不理解上下文”**

历史项目里，AI 最大的问题通常不是语法或算法，而是：

* 项目约定隐式存在（命名、分层、风格、异常处理）
* 设计决策分散在 README / 代码 / 人脑中
* 历史包袱导致“正确实现 ≠ 最优实现”

你用：

* **GEMINI.md → 约束 AI 的行为边界**
* **PROJECT_PROFILE.md → 给 AI 一个“压缩过的项目世界观”**
* **人来做 Detailed Design → 把模糊语言解压成可执行指令**

本质上是在做一件非常关键的事：

> **把“人类脑内隐式知识”显式化，转成 AI 可消费的结构化上下文。**

这是第一性原理层面上完全正确的。

---

### 2. 你正确区分了三种职责边界

你当前的角色划分是清晰的：

| 角色 | 职责                  |
| -- | ------------------- |
| 人  | 需求理解、设计决策、权衡、澄清歧义   |
| 文档 | 语义稳定器（长期记忆、约束来源）    |
| AI | 受约束的执行器（代码生成、补全、重构） |

这点非常重要，因为你**没有让 AI 去做它最不擅长的事**（权衡历史决策、业务灰度、技术债判断）。

---

### 3. GEMINI.md 的价值被你用在了“正确位置”

很多人把这类文件当成：

> “提示词大全 / 风格指南”

但你实际在做的是：

* 行为约束（Do / Don’t）
* 优先级排序（安全 > 一致性 > 性能 > 优雅）
* 禁止发散（禁止引入新框架、新模式）

这使得 AI 的“创造性”被**收敛成确定性输出**，这在历史项目中是极其重要的。

---

## 三、客观风险与隐含问题（不回避）

下面这些不是“你做错了”，而是**这个模式天然存在的风险点**。

### 1. 文档正在承担“事实真相源”的角色

一旦出现以下情况，风险会迅速放大：

* GEMINI.md / PROJECT_PROFILE.md 与真实代码不一致
* 历史代码已经偏离最初设计
* 文档更新滞后于代码演进

**后果是：**
AI 会“非常自信地写出错误代码”，而且错误具有一致性与迷惑性。

> 换句话说：
> **你提高了 AI 的准确率，也提高了它“稳定犯错”的能力。**

---

### 2. 人的详细设计成本在上升

你的模式实际上是：

> 把“人 review AI 代码”的成本，前移到了“人写设计”的阶段。

这在复杂系统中是值得的，但需要警惕两点：

* 详细设计是否真的被严格执行？
* 设计是否变成“为了 AI 而写”，而不是“为了系统而写”？

如果设计本身质量下降，AI 的输出只会更糟。

---

### 3. AI 仍然可能“表面服从，实则偏航”

即使有 GEMINI.md，AI 仍可能：

* 形式上遵守规范，语义上偏离意图
* 在边界条件、异常流中“自行脑补”
* 对历史 Hack / 特例处理理解不足

这不是提示词问题，而是**模型对“负例”和“反模式”的理解仍然有限**。

---

## 四、建设性改进建议（重点）

下面是我认为**最值得你考虑的三个升级方向**。

---

### 建议一：引入「设计 → 约束 → 校验」的闭环

当前是：

> 设计 → AI 生成 → 人 review

可以升级为：

> **设计 → AI 生成 → AI 自检 → 人 review**

例如在 GEMINI.md 中明确要求：

* 输出前，必须逐条自检：

  * 是否违反项目分层
  * 是否引入新依赖
  * 是否覆盖历史兼容逻辑
* 明确列出「我可能不确定的地方」

这一步的价值不在于“让 AI 更聪明”，而是：

> **强制它暴露不确定性，减少 silent failure。**

---

### 建议二：区分「不可变约束」与「可演进约束」

建议在 GEMINI.md 中**显式区分两类规则**：

* **Hard Rules（不可违背）**

  * 架构分层
  * 数据一致性
  * 向后兼容
* **Soft Rules（可讨论）**

  * 代码风格
  * 性能优化策略
  * 技术债容忍度

这样可以避免 AI 在所有规则上“一刀切”，也避免你每次都要手动 override。

---

### 建议三：把“历史包袱”结构化，而不是只写叙述

你之前已经在做“有历史包袱的项目文档”，这点非常好。

可以进一步结构化为：

* 已知反模式（为什么不能改）
* 已知妥协点（如果有机会可以重构）
* 禁区代码（动了就出事）

**AI 对“不能做什么”的理解，往往比“应该怎么做”更重要。**

---

## 五、一个更高层的评价（站在 2～3 年后看）

从趋势上看，你现在这套方法论，本质上是：

> **在为未来的“多智能体 / 规范驱动开发”做铺垫。**

无论是 Gemini CLI、Claude、还是未来更强的 Coding Agent，**最终都会走向：**

* 规范显式化
* 设计结构化
* 执行自动化

你已经提前把“人和 AI 的协作接口”设计好了。

---

## 六、结论（中肯）

* 这不是一个“取巧”的 AI 编程模式
* 这是一个**偏重工程纪律、牺牲部分速度、换取确定性的成熟方案**
* 非常适合你现在这种：**历史系统 + 高复杂度 + 高风险演进**的场景

# 参考资料

* any list
{:toc}