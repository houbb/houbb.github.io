---
layout: post
title: dive-into-llms-03-自然语言处理／大模型的一些背景概念
date: 2025-11-03 20:40:12 +0800
categories: [AI]
tags: [ai, learn-note]
published: true
---


# 背景

学习 [dive-into-llms](https://github.com/Lordog/dive-into-llms)

# chat

## 对自然语言处理／大模型的一些背景概念有初步认识：什么是预训练、微调、Transformer 架构、模型推理、量化、部署等等。

非常棒 👍，你现在已经到了学习大模型最关键的一步 —— 理解 **自然语言处理（NLP）与大模型的核心概念体系**。

我来用尽可能通俗、系统、工程化的方式，帮你快速建立一个「从 0 到能看懂大模型原理」的完整认知地图👇

---

## 🧩 一、自然语言处理（NLP）是什么？

一句话解释：

> **NLP 是让计算机理解、生成、分析人类语言的技术。**

比如：

* **理解类**：情感分析、意图识别、问答、信息抽取
* **生成类**：写摘要、翻译、写文章、对话
* **推理类**：推断上下文含义、改写句子、总结逻辑

最早的 NLP 是基于规则的（if-else / 正则表达式），后来是机器学习（SVM、CRF），直到 2017 年 Transformer 出现，才进入了现在的「大模型时代」。

---

## ⚙️ 二、预训练（Pre-training）

**核心思想：先学习语言规律，再去做任务。**

在深度学习里，训练一个模型从零开始（随机参数）需要巨量数据。但语言数据太多太复杂，于是人们想到：

> 让模型先“读懂整个世界的语言”，再针对具体任务调整。

所以：

1. 收集大规模语料（比如 Wikipedia、书籍、网页等）；
2. 让模型通过“自监督”任务学习语言结构；
3. 得到一个**通用语言模型（pretrained model）**。

例子（自监督任务）：

* **掩码语言建模（MLM）**：
  “我今天去[MASK]超市” → 模型预测[MASK]=“了”。
* **下句预测（NSP）**：
  “我今天去超市。→ 我买了牛奶。”是否是相邻句。

🔸 代表模型：**BERT、GPT、RoBERTa、T5**。

预训练模型相当于一个「通用语言大脑」。

---

## 🧠 三、微调（Fine-tuning）

预训练模型懂语言，但不知道怎么完成具体任务。
于是我们对它进行「微调」：

> 在你自己的任务数据上，轻微更新模型权重。

比如：

* 你要做报警分类模型；
* 你拿到报警日志 + 标签；
* 用预训练的 BERT，再在这些数据上训练几个 epoch；
* 模型学会“报警 → 原因分类”的映射。

🎯 优点：

* 不用从零训练，节省资源；
* 保留语言知识，适应任务。

🔸 现在常见的「ChatGPT」「通义千问」「Claude」等，
其实都是基于「大规模预训练 + 多轮微调（SFT、RLHF）」形成的。

---

## 🧱 四、Transformer 架构（大模型的心脏）

Transformer 是 2017 年 Google 提出的架构论文
**《Attention Is All You Need》**，
意思是“注意力机制就够了”。

### 核心思想：

> 模型可以根据上下文「注意」到关键部分。

举个例子：

> “我昨天去银行钓鱼。”

传统模型不知道“银行”是指“金融机构”还是“河岸”。
Transformer 会自动学习“钓鱼”这个上下文，从而判断“bank”是“河岸”。

### 架构结构：

```
输入 → Embedding → Self-Attention → FeedForward → 输出
```

### 模型类型：

| 类型              | 方向     | 代表模型    |
| --------------- | ------ | ------- |
| Encoder-only    | 理解类    | BERT    |
| Decoder-only    | 生成类    | GPT     |
| Encoder-Decoder | 翻译/摘要类 | T5、BART |

### 优点：

✅ 并行计算快
✅ 能捕捉远程依赖（长句关系）
✅ 可扩展（堆更多层、参数更多）
✅ 模块化设计便于微调与扩展

现在几乎所有大模型（GPT、Claude、通义千问、Gemini、Yi、ChatGLM）都基于 Transformer。

---

## 🚀 五、模型推理（Inference）

“推理”就是 **让训练好的模型开始工作** 的阶段。

训练是「学规律」；推理是「用规律」。

流程：

1. 加载模型权重（通常是几 GB 的 `.bin` 文件）；
2. 输入文本（tokenize → 向量）；
3. 前向计算，生成输出；
4. 结果解码成文字。

例如：

```python
from transformers import pipeline

qa = pipeline("question-answering", model="uer/roberta-base-chinese-extractive-qa")
res = qa(question="磁盘已满会导致什么？", context="系统无法写入文件，进程挂起")
print(res)
```

输出：

```python
{'score': 0.99, 'answer': '系统无法写入文件'}
```

---

## ⚖️ 六、量化（Quantization）

量化 = 压缩 + 加速。

因为大模型太大了（动辄几十亿参数）。

例如：

* 原始模型：每个参数是 32 位浮点数（float32）；
* 量化后：每个参数只保留 8 位（int8）或 4 位（int4）。

🎯 这样可以：

* 减少显存占用；
* 加快推理速度；
* 损失少量精度。

常用量化技术：

| 方法          | 简介                   |
| ----------- | -------------------- |
| INT8        | 最常见、兼顾精度             |
| INT4        | 更极致压缩（用于LLaMA等）      |
| GPTQ        | 专为Transformer定制的量化方案 |
| AWQ / QLoRA | 结合量化与低秩微调的混合方法       |

---

## 🧰 七、部署（Deployment）

训练完或微调好的模型，要让其他系统能访问，就需要部署。
部署就是「把模型变成一个可访问的服务」。

典型方式有几种：

| 类型     | 举例                                               | 特点            |
| ------ | ------------------------------------------------ | ------------- |
| 本地推理   | 用 `transformers` 在本机跑                            | 方便开发调试        |
| Web 服务 | FastAPI + Uvicorn                                | 可封装成 REST API |
| 模型服务框架 | TorchServe、Triton、vLLM、Text-Generation-Inference | 专业推理性能优化      |
| 云端部署   | Hugging Face Spaces、AWS SageMaker                | 自动化、弹性伸缩      |

💡 实际部署时，还会配合：

* 模型并行 / 张量并行
* 批量推理（batching）
* 缓存 / 流式输出（streaming）

---

## 🧮 八、总结：大模型完整生命周期图

```
                ┌──────────────────┐
                │ 预训练 (Pretrain) │  —— 从大规模语料学习语言规律
                └───────┬──────────┘
                        │
                        ▼
                ┌──────────────────┐
                │ 微调 (Fine-tune) │  —— 让模型适配具体任务
                └───────┬──────────┘
                        │
                        ▼
                ┌──────────────────┐
                │ 推理 (Inference) │  —— 让模型输出结果
                └───────┬──────────┘
                        │
                        ▼
                ┌──────────────────┐
                │ 量化 / 部署 (Deploy) │ —— 压缩 + 服务化
                └──────────────────┘
```

---

## 📚 九、推荐入门资料（实用+中文+免费）

| 主题             | 资源                                                                                                 | 说明              |
| -------------- | -------------------------------------------------------------------------------------------------- | --------------- |
| Transformer 基础 | [The Illustrated Transformer（图解 Transformer）](https://jalammar.github.io/illustrated-transformer/) | 动画图讲得极通俗（建议看英文） |
| 系统教程           | [李沐《动手学深度学习》 第9章：Transformer](https://zh.d2l.ai/chapter_attention-mechanisms/index.html)           | 理论+PyTorch 实践   |
| 实战             | [Hugging Face Transformers 中文文档](https://huggingface.co/docs/transformers/main/zh/index)           | 跑模型、部署、微调       |
| 扩展             | [LLM Bootcamp（吴恩达）](https://learn.deeplearning.ai/)                                                | 面向工程师的 LLM 实战课  |

---

## 🚀 十、学习建议（适合你这种架构师背景）

| 阶段        | 目标                     | 推荐实践                        |
| --------- | ---------------------- | --------------------------- |
| 1️⃣ 第 1 周 | 理解 Transformer + 预训练思想 | 看图解 Transformer + 跑 BERT 推理 |
| 2️⃣ 第 2 周 | 掌握微调流程                 | 用 Hugging Face 微调中文分类模型     |
| 3️⃣ 第 3 周 | 学习推理与量化                | 尝试 int8 推理、比较速度差异           |
| 4️⃣ 第 4 周 | 学习部署                   | 用 FastAPI 封装模型服务，暴露接口       |


* any list
{:toc}