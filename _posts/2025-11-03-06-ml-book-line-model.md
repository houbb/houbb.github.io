---
layout: post
title: 第6章　线性模型家族
date: 2025-11-03 20:40:12 +0800
categories: [AI]
tags: [ai, learn-note]
published: true
---

## 第6章　线性模型家族

线性模型是机器学习中最早、最经典、也是最具代表性的算法家族。

几乎所有复杂模型（包括神经网络）在本质上都可以看作“非线性的线性组合”。

线性模型的魅力在于：

* **简单而强大** —— 可以解释、可计算、可扩展；
* **理论完备** —— 有清晰的概率解释和几何意义；
* **实用性极高** —— 仍被广泛用于工业界的特征工程、基线模型和可解释建模。

---

### **6.1 线性回归**

#### ✅ 一、问题定义

线性回归（Linear Regression）用于解决**连续值预测问题**。
其假设是：输出 (y) 与输入特征 (x) 之间呈线性关系：

[
\hat{y} = w^T x + b
]

其中：

* (x = (x_1, x_2, ..., x_n))：输入特征向量
* (w = (w_1, w_2, ..., w_n))：权重参数
* (b)：偏置项（bias）

#### ✅ 二、目标函数

为了让模型预测值 (\hat{y}) 尽可能接近真实值 (y)，
最常用的损失函数是 **均方误差（MSE）**：

[
L(w,b) = \frac{1}{N} \sum_{i=1}^{N} (y_i - (w^T x_i + b))^2
]

#### ✅ 三、解析解（Normal Equation）

当特征维度不太高时，可以直接通过矩阵求导获得最优参数：

[
w^* = (X^T X)^{-1} X^T y
]

其中 (X) 是特征矩阵，(y) 是目标向量。
这种闭式解（Closed Form）虽然简单，但在高维或大数据集上会导致计算瓶颈，因此实践中常用 **梯度下降（GD / SGD）**。

#### ✅ 四、几何与统计解释

* 几何视角：线性回归相当于在高维空间中找到一个**最贴近样本点的超平面**。
* 统计视角：线性回归假设误差项服从高斯分布，等价于最大似然估计（MLE）。

#### ✅ 五、优缺点

✅ 优点：

* 可解释性强；
* 计算效率高；
* 在噪声小、线性关系明显时表现好。

❌ 缺点：

* 无法建模非线性关系；
* 对异常值敏感；
* 可能发生多重共线性（特征相关）。

---

### **6.2 逻辑回归与分类边界**

#### ✅ 一、从回归到分类的跨越

逻辑回归（Logistic Regression）用于**二分类任务**。
虽然名字里有“回归”，但本质上是一个**分类模型**。

#### ✅ 二、模型假设

我们希望预测样本属于正类（y=1）的概率：

[
P(y=1|x) = \sigma(w^T x + b)
]

其中 (\sigma(z)) 是 Sigmoid 函数：

[
\sigma(z) = \frac{1}{1 + e^{-z}}
]

它将线性输出 (w^T x + b) 压缩到 (0, 1) 区间。

#### ✅ 三、决策边界

分类边界由方程 (w^T x + b = 0) 决定。
几何上，这是一个将样本空间划分为两部分的超平面。

* (w)：决定超平面的方向；
* (b)：决定平面的位置。

#### ✅ 四、损失函数：对数似然

逻辑回归通过极大化样本出现的**对数似然**来训练：

[
L(w) = \sum_i [y_i \log p_i + (1 - y_i) \log(1 - p_i)]
]

通常优化其负数（即交叉熵损失）：

[
\text{Loss} = -\frac{1}{N}\sum_i [y_i \log \hat{y}_i + (1 - y_i)\log(1 - \hat{y}_i)]
]

#### ✅ 五、多分类扩展

* **一对多 (OvR)**：训练多个二分类器，每次区分“某一类 vs 其他类”；
* **Softmax 回归**：广义逻辑回归，使用 Softmax 函数输出多类概率：

[
P(y=k|x) = \frac{e^{w_k^T x}}{\sum_j e^{w_j^T x}}
]

#### ✅ 六、概率与几何统一视角

逻辑回归既是**线性分类器**（超平面决策边界），
又是**概率模型**（输出类别的概率）。
这是线性模型家族的重要特征。

---

### **6.3 多项式回归与岭回归**

#### ✅ 一、多项式回归（Polynomial Regression）

线性模型的核心假设是输入特征与输出呈线性关系。
但通过构造“非线性特征”，它也能处理非线性问题。

例如：
[
y = w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \ldots
]

实际上它仍然是**线性模型**，只是对“特征”线性，而非对“输入”线性。

这种做法体现了机器学习的核心思想：

> “用非线性特征去弥补线性模型的不足。”

#### ✅ 二、岭回归（Ridge Regression）

当特征之间存在高度相关（多重共线性）时，普通最小二乘解不稳定。
岭回归通过在损失函数中加入 L2 正则项进行约束：

[
L(w) = ||y - Xw||^2 + \lambda ||w||^2
]

解析解为：

[
w^* = (X^T X + \lambda I)^{-1} X^T y
]

* 当 (\lambda = 0)，退化为普通线性回归；
* 当 (\lambda) 较大，抑制权重震荡，提高泛化能力。

#### ✅ 三、Lasso 与 Elastic Net

* **Lasso（L1正则化）**：鼓励稀疏性，能自动实现特征选择；
* **Elastic Net**：结合 L1 + L2，兼顾平滑与稀疏。

---

### **6.4 判别式与生成式模型比较**

#### ✅ 一、两种学习范式

| 模型类型                       | 目标                          | 代表算法             | 核心思想          |          |
| -------------------------- | --------------------------- | ---------------- | ------------- | -------- |
| **判别式模型 (Discriminative)** | 直接学习条件概率 (P(y               | x)) 或决策边界        | 逻辑回归、SVM、神经网络 | 关注“如何区分” |
| **生成式模型 (Generative)**     | 学习联合分布 (P(x, y)) 并通过贝叶斯公式推断 | 朴素贝叶斯、LDA、高斯混合模型 | 关注“如何生成”      |          |

#### ✅ 二、比较分析

| 维度   | 判别式模型                    | 生成式模型            |
| ---- | ------------------------ | ---------------- |
| 目标   | 学分类边界                    | 建模数据分布           |
| 表现   | 通常分类效果更好                 | 样本少时鲁棒性强         |
| 可解释性 | 弱（黑箱）                    | 强（有生成过程）         |
| 计算   | 训练更快                     | 通常更复杂            |
| 典型模型 | Logistic Regression, SVM | Naive Bayes, GMM |

#### ✅ 三、统一视角

逻辑回归与朴素贝叶斯，虽然看似不同，但在数学上可以看作同一问题的两种路径：

* **生成式路径**：先学 (P(x|y))，再用贝叶斯定理求 (P(y|x))；
* **判别式路径**：直接学 (P(y|x))。

这也是机器学习理论发展的一个缩影：

> 从“生成数据” → “区分数据” → “理解数据”。

---

### **总结**

| 小节          | 核心思想           | 关键公式 / 方法           |                      |      |   |            |   |   |   |      |
| ----------- | -------------- | ------------------- | -------------------- | ---- | - | ---------- | - | - | - | ---- |
| 6.1 线性回归    | 用超平面拟合连续输出     | ( \hat{y}=w^T x+b ) |                      |      |   |            |   |   |   |      |
| 6.2 逻辑回归    | 概率化的线性分类       | ( P(y=1             | x)=\sigma(w^T x+b) ) |      |   |            |   |   |   |      |
| 6.3 多项式与岭回归 | 用非线性特征或正则化提升泛化 | ( L=                |                      | y-Xw |   | ^2+\lambda |   | w |   | ^2 ) |
| 6.4 判别式与生成式 | 两种建模范式         | 学 (P(y              | x)) vs 学 (P(x,y))    |      |   |            |   |   |   |      |



* any list
{:toc}