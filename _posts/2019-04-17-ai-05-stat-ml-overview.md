---
layout: post
title: 老马学机器学习-06-PLA
date:  2019-4-16 10:55:13 +0800
categories: [ML]
tags: [ML, ai, math, sh]
published: true
---

# 序章

机器学习的入门经典书籍有《机器学习》（西瓜书）和《统计学习方法》。

此处老马优先选择《统计学习方法》，因为基于概率的部分更加符合人的直觉，我们把神经网络部分放在统计算法之后。

![书籍](https://images.gitee.com/uploads/images/2021/0413/204233_070beb21_508704.jpeg "ml-book.jpg")

当然，两本书有很多重合的地方，我们可以交叉学习。

所有理论篇前人已经总结的很好了，理论系列只做摘录，不作为原创。

# 四种学习形式

训练深度学习网络的方式主要有四种：监督、无监督、半监督和强化学习。

## 监督学习（Supervised Learning）

监督学习是使用已知正确答案的示例来训练网络的。

## 无监督学习（Unsupervised Learning）

无监督学习适用于你具有数据集但无标签的情况。

无监督学习采用输入集，并尝试查找数据中的模式。

常见算法：

自编码（Autoencoding）

主成分分析（Principal components analysis）

随机森林（Random forests）

K均值聚类（K-means clustering）

## 半监督学习（Semi-supervised Learning）

半监督学习在训练阶段结合了大量未标记的数据和少量标签数据。

与使用所有标签数据的模型相比，使用训练集的训练模型在训练时可以更为准确，而且训练成本更低。

## 强化学习（Reinforcement Learning）

强化学习是针对你再次没有标注数据集的情况而言的，但你还是有办法来区分是否越来越接近目标（回报函数（reward function））。

# 统计学方法的三要素

三要素：模型、策略、算法。

这个是《统计学习方法》中的基础。

```
方法=模型+策略+算法
```

![三要素](https://images.gitee.com/uploads/images/2021/0413/204323_53a55233_508704.png)

## 模型（Model）

统计学习首要考虑的问题是学习什么样的模型。

在监督学习过程中，模型就是所要学习的条件概率分布或决策函数。

数据构成假设空间，在这个假设空间中包含所有可能的条件概率分布或者决策函数，每一个条件概率分布或者决策函数对应一个模型，那么这个样本空间中的模型个数有无数个。

怎样理解模型呢？

简单来说就是使用什么映射函数来表示特征X和Y标签之间的关系F，F有两种形式：`F={f|y=f(x)}` 或者 `F={P|P(Y|X)}`

`F={f|y=f(x)}` 为决策函数，它表示的模型为非概率模型。

`F={P|P(Y|X)}` 是条件概率表示，它的模型为概率模型。

## 策略（Strategy）

策略即从假设空间中挑选出参数最优的模型的准则。

模型的分类或预测结果与实际情况的误差(损失函数)越小，模型就越好。

我们前面已经知道在样本空间中有无数的模型，但模型有好有坏，现在的问题考虑的是按照什么样的准则学习或者选择最优模型，而**策略就是通过引入损失函数的方式来度量模型的好坏**。

设定损失函数，这样监督学习问题就变成了最小化损失函数，那么按照这样的策略，就可以求解出最优化的模型了。

## 算法（Algorithm）

算法是指学习模型的具体计算方法，也就是如何求解全局最优解，并使得这个过程高效而且准确，本质上就是计算机算法，怎么去求数学问题的最优化解。

前面我们知道了模型有无数种，获取最好模型的方法就是最小化损失函数，那么此时的模型就是最好的，现在的问题就是如何才能获取到这个最优化的解呢？

是正规方程还是梯度下降等等。

# 泛化能力

## 过拟合产生

如果只是一味的追求最小的误差，可能会导致过拟合（overfitting）的问题。

![输入图片说明](https://images.gitee.com/uploads/images/2021/0413/203339_2f526772_508704.png "屏幕截图.png")

解决的方法通常有 2 种：正则化和交叉验证。

## 正则化(regularization)

正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或惩罚项。

正则化一般是模型复杂度的单调递增函数，模型越越复杂，正则化的值就越大。

正则化符合奥卡姆剃刀原理，奥卡姆剃刀原理应用于模型选择时变为一下想法：

**在所有可能选择的模型中，能够很好的解释已知数据并且十分简单才是最好的模型，也就是应该选择的模型。**

从贝叶斯的角度来看，正则化项对应于模型的先验概率，可以假设复杂模型有较好的先验概率，简答的模型有较大的先验概率。

比如：

```
1 2 3 4
```

问下一个数是什么？

答案应该是 5。

虽然还有可能有很多种函数，但是 5 是最符合奥卡姆剃刀原理的。

## 交叉验证（cross validation）

另一种常用的模型选择方法是交叉验证

如果给定的样本数据充足，进行模型选择的一种简单的方法是随机将数据集切分成三份，分别是训练集，验证集和测试集。训练集用于训练模型，验证集用于模型的选择，而测试集用于最终对学习方的评估，在学习到的不同复杂度的模型中，选择验证集有最小预测误差的模型。由于验证集有足够多的数据，用它对模型进行选择也是有效的。

但是，在许多实际应用中数据是不充足的，为了选择好的模型，可以采用交叉验证方法。交叉验证的基本思想是重复地使用数据，给定的数据进行切分，将切分的数据集组合为训练集和测试集，在此基础上反复的进行训练，测试以及模型选择。

### 简单交叉验证

首先随机地将已给的数据分为两部分，一部分作为训练集，另一部分作为测试集(例如，70%的数据训练集，30%的数据测试集)；然后用训练集在各种条件下训练模型，从而得到不同的模型，在测试集上评价各个模型的测试误差，选择测试误差最小的模型。

### S 折交叉验证（S-fold cross validation）

应用最多的是S折交叉验证，方法如下：首先随机地将已给数据切分为S个互不相同的大小相同的子集，然后利用S-1个子集的数据训练模型，利用余下的子集测试模型，将这一过程对可能的S种选择重复进行，最后选出S次评测误差最小的模型。

### 留一交叉验证（leave-oneout cross validation）

S折交叉验证的特殊情形是S=N，称为留一交叉验证，往往在数据缺乏的情况下使用，这里，N是给定数据集的容量。

## 其他方法

当然，如果我们自己查一下，可以发现其他书中没有提到的方法。

### 获取更多数据

般情况下直接增加数据是很困难的，因此我们需要通过一定的规则来扩充训练数据。

比如，在图像分类问题上，我们可以使用数据增强的方法，通过对图像的平移、旋转、缩放等方式来扩充数据；更进一步地，可以使用生成式对抗网络来合成大量新的训练数据。

### 降低模型复杂度

模型复杂度过高是数据量较小时过拟合的主要原因。适当降低模型的复杂度可以避免模型拟合过多的噪声。

比如，在神经网络模型中减少网络层数、神经元个数等；在决策树模型中降低树的深度、进行剪枝等。

### 集成学习

即把多个模型集成在一起，从而降低单一模型的过拟合风险。

主要有Bagging（bootstrap aggregating）和Boosting（adaptive boosting）这两种集成学习方法。

# 小结

是不是感觉特别简单？

答案是因为老马做了简化，把所有的数学公式都删除了。

让大家对统计机器学习有一个最基本的印象，如果想深入学习，可以看原书，这里特意留了英文，也可以自己 google 一下。

希望本文对你有所帮助，如果喜欢，欢迎点赞收藏转发一波。

我是老马，期待与你的下次相遇。

# 参考资料

《统计学习方法》

[一文读懂监督学习、无监督学习、半监督学习、强化学习四种方式](https://www.jianshu.com/p/6833f81b8b95)

[统计机器学习方法的三要素：模型、策略和算法](https://zhuanlan.zhihu.com/p/363344172)

[模型选择之交叉验证](https://zhuanlan.zhihu.com/p/32627500)

[正则化和交叉验证在组合预测模型中的应用](http://www.c-s-a.org.cn/csa/article/abstract/7254)

[模型选择的方法——正则化与交叉验证](https://blog.csdn.net/qq_16608563/article/details/88953052)

[机器学习——正则化与交叉验证](https://www.cnblogs.com/baby-lily/p/10816157.html)

[machine learning笔记：过拟合与欠拟合](https://gsy00517.github.io/machine-learning20191001104538/)

* any list
{:toc}