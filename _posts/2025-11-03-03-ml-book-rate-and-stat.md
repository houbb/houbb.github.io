---
layout: post
title: 第3章　概率与统计
date: 2025-11-03 20:40:12 +0800
categories: [AI]
tags: [ai, learn-note]
published: true
---


# 第3章　概率与统计

这一章可以说是机器学习的数学“灵魂”章节——**概率与统计**是理解一切模型（从朴素贝叶斯到深度神经网络）的底层逻辑。

## 🌟 引言

机器学习的核心任务其实就是“在不确定性中做决策”。

而**概率论**提供了处理不确定性的语言，**统计学**提供了从数据中估计规律的方法。

如果说：

* 代数 → 是确定世界的数学；
* 概率与统计 → 就是“不确定世界的数学”。

---

## 3.1 随机变量与分布

### ✅ 随机变量（Random Variable）

* **定义**：随机变量是一个用数字表示随机事件结果的函数。
  比如：

  * 掷骰子 → 可能结果 {1,2,3,4,5,6}
  * 把“点数”定义为随机变量 X，那 X 就是一个离散随机变量。

### 🧩 两大类：

1. **离散型随机变量（Discrete）**

   * 取值是有限或可数的
   * 如骰子点数、硬币正反面
   * 常用分布：伯努利分布、二项分布、泊松分布

2. **连续型随机变量（Continuous）**

   * 取值是连续的（可取任意实数）
   * 如人的身高、温度
   * 常用分布：正态分布、均匀分布、指数分布

### 📊 概率分布（Probability Distribution）

概率分布定义了随机变量的取值“可能性”：

* 对离散变量 → 概率质量函数（PMF）
  ( P(X=x_i) )
* 对连续变量 → 概率密度函数（PDF）
  ( f(x) )，且 ( P(a \le X \le b) = \int_a^b f(x) dx )

### 🎯 期望与方差

* **期望**：平均值（模型预测的平均输出）
  [
  E[X] = \sum_i x_i P(x_i)
  ]
* **方差**：不确定性的量化
  [
  Var(X) = E[(X - E[X])^2]
  ]

这些是机器学习里「损失函数」与「不确定性」的数学基石。

---

## 3.2 极大似然估计（MLE）与贝叶斯推断

### 🎯 极大似然估计（Maximum Likelihood Estimation, MLE）

**目标**：在已知数据的情况下，找到最可能生成这些数据的模型参数。

假设我们有样本数据 ( D = {x_1, x_2, ..., x_n} )，
模型的参数为 ( \theta )，则似然函数为：

[
L(\theta) = P(D|\theta) = \prod_i P(x_i|\theta)
]

取对数方便计算：

[
\hat{\theta} = \arg\max_\theta \log L(\theta)
]

🧠 举个例子：

* 掷硬币 n 次，结果正面次数 k
* 假设正面概率为 ( p )，似然函数：
  [
  L(p) = p^k (1-p)^{n-k}
  ]
* 最大化后得到：
  [
  \hat{p} = \frac{k}{n}
  ]
  这就是最直观的「极大似然估计」。

---

### 🧮 贝叶斯推断（Bayesian Inference）

贝叶斯思想强调「先验知识 + 数据更新」：
[
P(\theta|D) = \frac{P(D|\theta) P(\theta)}{P(D)}
]

* **先验（Prior）**：在看到数据前对参数的信念。
* **似然（Likelihood）**：数据在参数下出现的可能性。
* **后验（Posterior）**：看到数据后的新信念。

📘 举例：
如果你认为硬币可能是公平的（先验 ( p=0.5 )），
但你掷了10次出现8次正面，贝叶斯更新后你会认为“可能稍偏正面”。

👉 贝叶斯方法在现代机器学习中非常重要：

* 朴素贝叶斯分类器
* 贝叶斯网络
* 高斯过程（Gaussian Process）
* LLM 先验知识建模

---

## 3.3 条件概率与朴素贝叶斯模型

### 🧩 条件概率（Conditional Probability）

[
P(A|B) = \frac{P(A,B)}{P(B)}
]

理解为“在 B 发生的前提下，A 发生的概率”。

比如：

* 事件 A：邮件是垃圾邮件
* 事件 B：邮件中出现“中奖”一词
  则 ( P(A|B) ) 表示：出现“中奖”的邮件是垃圾邮件的概率。

---

### 📘 朴素贝叶斯分类器（Naive Bayes）

假设输入特征之间相互独立（朴素假设）：

[
P(y|x_1, ..., x_n) \propto P(y) \prod_i P(x_i|y)
]

算法流程：

1. 从训练数据估计先验 ( P(y) )
2. 估计条件概率 ( P(x_i|y) )
3. 对新样本，计算各类的后验概率并选取最大者。

📊 应用场景：

* 垃圾邮件分类
* 文本情感分析
* 新闻主题分类

🧠 尽管“朴素”，但在高维稀疏数据（如文本词袋模型）上效果惊人好。

---

## 3.4 信息论基础：熵、KL散度、交叉熵

### 🔹 熵（Entropy）

衡量不确定性的数学量：

[
H(X) = - \sum_x P(x) \log P(x)
]

* 熵越大 → 不确定性越高
* 熵越小 → 越有序

📘 举例：

* 公平硬币 ( H = 1 )
* 总是正面的硬币 ( H = 0 )

---

### 🔹 相对熵（KL 散度）

衡量两个分布的“差距”：
[
D_{KL}(P||Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}
]

在机器学习中：

* 衡量模型分布 ( Q ) 与真实分布 ( P ) 的差距。
* 是许多损失函数（例如交叉熵）的理论来源。

---

### 🔹 交叉熵（Cross Entropy）

[
H(P,Q) = - \sum_x P(x) \log Q(x)
]

当 ( P ) 是真实分布、( Q ) 是模型预测分布时，
最小化交叉熵 ≈ 让模型预测尽可能接近真实。

📘 应用：

* 分类任务中的损失函数（Softmax + CrossEntropyLoss）
* 信息压缩与语言模型的困惑度（Perplexity）计算

---

## 🌍 小结

| 概念              | 核心作用     | 在机器学习中的体现   |
| --------------- | -------- | ----------- |
| 随机变量            | 建模不确定性   | 特征、标签的概率表达  |
| MLE             | 参数估计     | 逻辑回归、GMM    |
| 贝叶斯推断           | 融合先验与数据  | 朴素贝叶斯、贝叶斯网络 |
| 条件概率            | 推断关系     | 分类与推荐       |
| 熵 / KL 散度 / 交叉熵 | 信息量与分布差异 | 损失函数、模型评估   |

* any list
{:toc}