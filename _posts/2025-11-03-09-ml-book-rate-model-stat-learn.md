---
layout: post
title: 第9章　概率模型与统计学习
date: 2025-11-03 20:40:12 +0800
categories: [AI]
tags: [ai, learn-note]
published: true
---


## **第9章　概率模型与统计学习**

### **9.1 朴素贝叶斯（Naive Bayes）**

#### **（1）核心思想**

朴素贝叶斯是一种基于 **贝叶斯定理（Bayes’ theorem）** 和 **特征条件独立假设** 的概率分类方法。

贝叶斯定理：

[
P(y|x) = \frac{P(x|y)P(y)}{P(x)}
]

其中：

* (P(y))：先验概率（类别的总体概率）；
* (P(x|y))：似然（在类别 y 下出现特征 x 的概率）；
* (P(y|x))：后验概率（给定特征后属于类别 y 的概率）。

**核心假设**：
所有特征 (x_i) 条件独立：
[
P(x|y) = \prod_i P(x_i|y)
]
这就是“朴素（Naive）”的由来。

---

#### **（2）分类过程**

1. 统计每个类别的先验概率 (P(y))；
2. 统计每个特征在类别下的条件概率 (P(x_i|y))；
3. 对新的样本 x，计算每个类别的后验概率：
   [
   P(y|x) \propto P(y)\prod_i P(x_i|y)
   ]
4. 选取后验概率最大的类别。


#### **（3）常见变体**

* **高斯朴素贝叶斯（GaussianNB）**：特征服从正态分布；
* **多项式朴素贝叶斯（MultinomialNB）**：用于文本词频；
* **伯努利朴素贝叶斯（BernoulliNB）**：用于二值特征。

---

#### **（4）优缺点**

| 优点            | 缺点        |
| ------------- | --------- |
| 简单高效，训练和预测速度快 | 条件独立假设过强  |
| 对小数据集鲁棒       | 不能捕捉特征间依赖 |
| 适合高维稀疏数据（如文本） | 精度不及复杂模型  |

---

#### **（5）典型应用**

* 垃圾邮件识别；
* 情感分类；
* 文本主题识别；
* 文档分类（如新闻、评论分类）。

---

### **9.2 隐马尔可夫模型（Hidden Markov Model, HMM）**

#### **（1）核心思想**

HMM 是一个 **带有隐状态的随机过程**，假设：

1. 存在一个不可见的状态序列 (S = (s_1, s_2, ..., s_T))；
2. 每个状态会生成一个可见的观测序列 (O = (o_1, o_2, ..., o_T))；
3. 状态转移满足马尔可夫性：
   [
   P(s_t | s_{t-1}, ..., s_1) = P(s_t | s_{t-1})
   ]

换句话说：**未来的状态只与当前状态有关，而与过去无关**。

---

#### **（2）模型参数**

HMM 通常由三个参数定义（记为 λ）：

[
\lambda = (A, B, \pi)
]

* (A)：状态转移概率矩阵；
* (B)：观测概率矩阵；
* (\pi)：初始状态分布。

---

#### **（3）三个核心问题**

1. **评估问题（Evaluation）**
   计算给定模型和观测序列的概率 (P(O|\lambda))。
   → 用 **前向-后向算法（Forward-Backward Algorithm）**。

2. **解码问题（Decoding）**
   寻找最可能的状态序列。
   → 用 **维特比算法（Viterbi Algorithm）**。

3. **学习问题（Learning）**
   根据观测数据估计模型参数 (A, B, \pi)。
   → 用 **Baum-Welch 算法**（EM思想的具体实现）。

---

#### **（4）典型应用**

* **语音识别**（最早的成功应用）；
* **词性标注（POS tagging）**；
* **命名实体识别（NER）**；
* **手写识别、基因序列建模**。

---

### **9.3 条件随机场（Conditional Random Field, CRF）**

#### **（1）HMM 的局限**

HMM 是生成式模型，只能建模：
[
P(X, Y) = P(Y)P(X|Y)
]
无法直接建模条件概率 (P(Y|X))，并且要求状态转移和观测独立。

#### **（2）CRF 的思想**

CRF 是一种 **判别式的概率图模型**，直接建模：
[
P(Y|X) = \frac{1}{Z(X)} \exp \left( \sum_k \lambda_k f_k(Y, X) \right)
]
其中：

* (f_k(Y, X))：特征函数；
* (\lambda_k)：特征权重；
* (Z(X))：归一化因子。

**核心思想**：不用关心数据是怎么生成的，只需建模“在给定输入 X 时，标签序列 Y 的条件概率”。

---

#### **（3）与 HMM 的关系**

| 比较项  | HMM    | CRF     |
| ---- | ------ | ------- |
| 模型类型 | 生成式    | 判别式     |
| 依赖假设 | 强独立性   | 放宽独立性   |
| 特征使用 | 仅限观测概率 | 可引入任意特征 |
| 性能   | 一般     | 更高      |

---

#### **（4）CRF 的训练与推断**

* **训练**：最大化对数似然函数；
* **推断**：常用维特比算法或前向-后向算法。

---

#### **（5）典型应用**

* 序列标注任务（词性标注、命名实体识别、分词）；
* 生物信息学序列分析；
* OCR（光学字符识别）；
* 对话状态追踪。

---

### **9.4 EM 算法与期望最大化思想**

#### **（1）核心思想**

当数据中包含 **隐变量（latent variables）** 时，直接求解最大似然变得困难。
**EM算法（Expectation-Maximization）** 提供了一种 **迭代优化方法**。

它交替执行两步：

1. **E步（期望步）**：在当前参数下，计算隐变量的期望；
2. **M步（最大化步）**：在期望的基础上，重新估计模型参数以最大化似然。

[
\text{E-step: } Q(\theta | \theta^{(t)}) = E_{Z|X,\theta^{(t)}}[\log P(X,Z|\theta)]
]
[
\text{M-step: } \theta^{(t+1)} = \arg\max_\theta Q(\theta | \theta^{(t)})
]

---

#### **（2）直观理解**

你可以把 EM 看作“猜测 + 修正”的过程：

* 猜测隐变量的分布（E步）；
* 根据猜测重新估计模型参数（M步）；
* 反复执行，直到收敛。

---

#### **（3）典型应用**

* 高斯混合模型（GMM）聚类；
* 隐马尔可夫模型（HMM）参数学习；
* 缺失数据的估计；
* 图像分割；
* 用户画像聚类（如软聚类）。

---

### ✅ **总结：概率模型的演进逻辑**

| 模型    | 类型   | 核心思想          | 典型应用        |
| ----- | ---- | ------------- | ----------- |
| 朴素贝叶斯 | 生成式  | 条件独立假设        | 文本分类、垃圾邮件检测 |
| HMM   | 生成式  | 序列状态转移 + 发射概率 | 语音识别、词性标注   |
| CRF   | 判别式  | 条件概率建模 + 全局最优 | NER、分词、序列标注 |
| EM算法  | 优化方法 | 隐变量 + 期望最大化   | GMM、HMM、聚类  |

> 📘 **一句话总结**：

> 朴素贝叶斯是“概率学习的起点”，HMM 是“序列概率的里程碑”，CRF 是“结构化预测的代表”，而 EM 是“隐变量问题的通用解法”。

* any list
{:toc}