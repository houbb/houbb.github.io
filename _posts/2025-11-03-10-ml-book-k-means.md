---
layout: post
title: 第10章 无监督学习与聚类（Unsupervised Learning & Clustering）
date: 2025-11-03 20:40:12 +0800
categories: [AI]
tags: [ai, learn-note]
published: true
---


## **第10章　无监督学习与聚类**

---

### **10.1 K-Means 与高维空间的挑战**

#### **（1）核心思想**

K-Means 是最经典的无监督学习算法之一。它通过 **最小化簇内样本的平方误差**，将数据划分为 K 个相对紧密的簇。

其优化目标函数为：
[
J = \sum_{i=1}^{K} \sum_{x_j \in C_i} ||x_j - \mu_i||^2
]
其中：

* (C_i)：第 i 个簇；
* (\mu_i)：第 i 个簇的中心（均值）。

---

#### **（2）算法流程**

1. 随机选取 K 个样本作为初始中心；
2. 计算每个样本到各中心的距离，分配到最近的中心；
3. 更新每个簇的中心为簇内样本均值；
4. 重复步骤2-3，直到收敛（中心不再变化或误差变化很小）。

---

#### **（3）优缺点**

| 优点         | 缺点            |
| ---------- | ------------- |
| 简单高效、计算快速  | 需要指定 K        |
| 收敛性好（有限步内） | 依赖初始中心，易陷局部最优 |
| 适合球形簇      | 对异常值敏感        |

---

#### **（4）改进方法**

* **K-Means++**：通过距离加权的概率选择初始中心，提升稳定性；
* **Mini-Batch K-Means**：适合大规模数据；
* **Bisecting K-Means**：层次化地分裂簇，更稳健。

---

#### **（5）高维空间的挑战（Curse of Dimensionality）**

随着维度升高：

* 距离的差异变小 → “最近点”和“最远点”几乎等距；
* 数据稀疏，聚类边界模糊；
* 相似性度量失效。

**解决思路**：

* 降维（PCA、AutoEncoder）；
* 特征选择；
* 归一化或使用余弦相似度。

---

### **10.2 层次聚类与密度聚类（DBSCAN）**

#### **（1）层次聚类（Hierarchical Clustering）**

基于样本间的距离层层合并或拆分，形成一棵“聚类树（dendrogram）”。

* **凝聚式（自下而上）**：每个样本起初是一个簇，不断合并；
* **分裂式（自上而下）**：所有样本起初是一个大簇，不断拆分。

常用距离度量：

* 单链（最短距离）
* 全链（最长距离）
* 平均距离
* Ward 距离（最小化方差增加）

**特点**：

* 无需指定簇数；
* 适用于可解释分析；
* 计算复杂度高 (O(n^3))。

---

#### **（2）密度聚类（DBSCAN）**

DBSCAN（Density-Based Spatial Clustering of Applications with Noise）通过“**密度可达性**”定义簇，而非距离阈值。

**核心参数：**

* `ε (eps)`：邻域半径；
* `minPts`：形成簇的最小点数。

**基本概念：**

* **核心点**：邻域内样本数 ≥ minPts；
* **边界点**：邻域内样本数 < minPts，但在核心点邻域内；
* **噪声点**：既非核心也非边界点。

---

#### **（3）DBSCAN 的优势**

| 优点        | 缺点               |
| --------- | ---------------- |
| 可发现任意形状的簇 | 对参数敏感（ε, minPts） |
| 自动识别噪声    | 难处理不同密度的簇        |
| 不需指定簇数    | 在高维数据上表现较差       |

---

#### **（4）典型应用**

* 空间数据聚类（地理、图像）；
* 异常检测；
* 轨迹聚类（如用户行为路径分析）。

---

### **10.3 PCA、LDA、ICA、t-SNE**

#### **（1）PCA（主成分分析）**

**目标**：在保留数据主要方差的同时，找到最优的低维线性子空间。

数学形式：
[
\max_W ; \text{Var}(W^T X) \quad s.t. ; W^T W = I
]
等价于求解协方差矩阵的特征向量问题。

**结果解释**：

* 主成分 = 特征空间中最大方差方向；
* 保留前几个主成分 ≈ 保留主要信息。

**应用场景**：

* 降维与可视化；
* 去噪；
* 特征压缩（图像、文本）。

---

#### **（2）LDA（线性判别分析）**

与 PCA 不同，LDA 是 **有监督的降维方法**。
它最大化类间距离、最小化类内距离：
[
W = \arg\max_W \frac{|W^T S_B W|}{|W^T S_W W|}
]
其中：

* (S_B)：类间散度矩阵；
* (S_W)：类内散度矩阵。

**应用**：人脸识别（Fisherfaces）、分类前的特征压缩。

---

#### **（3）ICA（独立成分分析）**

目标：找到线性变换，使得输出分量之间 **统计独立**。

[
X = A S \Rightarrow S = W X
]

* (X)：观测信号；
* (S)：独立信号；
* (A)：混合矩阵。

**应用**：

* 信号分离（盲源分离，如鸡尾酒会问题）；
* 图像特征提取。

---

#### **（4）t-SNE（t-Distributed Stochastic Neighbor Embedding）**

一种非线性降维与可视化方法，特别适合高维数据（如词向量、图像特征）。

核心思想：

* 在高维空间中计算相似度（高斯分布）；
* 在低维空间中重建相似关系（t分布）；
* 通过最小化 KL 散度使局部结构保持一致。

**优点**：

* 保持局部结构；
* 可视化高维聚类效果佳。

**缺点**：

* 非线性、不可解释；
* 计算复杂；
* 不适合大规模数据。

---

### **10.4 异常检测与流形学习**

#### **（1）异常检测（Anomaly Detection）**

目标：发现那些“不符合常规模式”的样本。

**典型方法：**

* **基于距离**：KNN、LOF（局部异常因子）；
* **基于密度**：Isolation Forest；
* **基于概率模型**：高斯分布建模；
* **基于自编码器（AutoEncoder）**：学习重建误差。

**应用场景**：

* 欺诈检测；
* 设备故障预测；
* 网络入侵检测；
* 医学异常筛查。

---

#### **（2）流形学习（Manifold Learning）**

核心思想：
高维数据往往“嵌在”低维流形中（例如 3D 螺旋在 2D 平面上可展平）。

**常见方法：**

| 方法          | 核心思想        | 特点      |
| ----------- | ----------- | ------- |
| Isomap      | 保持全局测地距离    | 适合平滑流形  |
| LLE（局部线性嵌入） | 保持局部邻域线性结构  | 保持局部关系  |
| t-SNE       | 保持局部相似性（概率） | 可视化效果最好 |

---

#### **（3）无监督学习的应用案例**

| 场景      | 任务        | 算法                           |
| ------- | --------- | ---------------------------- |
| 用户分群    | 客户细分、精准营销 | K-Means、DBSCAN               |
| 降维可视化   | 特征压缩      | PCA、t-SNE                    |
| 异常检测    | 欺诈识别、设备监控 | Isolation Forest、AutoEncoder |
| 推荐系统    | 用户兴趣建模    | 聚类 + 协同过滤                    |
| NLP 预训练 | 词向量学习     | Word2Vec、BERT（自监督）           |

---

### ✅ **总结：无监督学习的本质**

| 方法               | 类型    | 核心目标    | 特点       |
| ---------------- | ----- | ------- | -------- |
| K-Means          | 划分聚类  | 最小化簇内方差 | 简单高效     |
| DBSCAN           | 密度聚类  | 基于密度可达性 | 自动识别噪声   |
| PCA              | 线性降维  | 最大方差保留  | 可解释性强    |
| t-SNE            | 非线性降维 | 保持局部相似度 | 可视化强     |
| AutoEncoder      | 深度无监督 | 重建输入特征  | 表示学习     |
| Flow / Diffusion | 概率生成  | 学习数据分布  | 现代生成模型基础 |

---

> 📘 **一句话总结**：
> 无监督学习的使命是“发现数据中隐藏的秩序”。
> 它让模型从“标签依赖”走向“结构理解”，是通往表示学习与自监督学习的必经之路。


* any list
{:toc}