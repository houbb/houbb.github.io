---
layout: post
title: 朴素贝叶斯算法 
date:  2020-1-28 10:09:32 +0800
categories: [Math]
tags: [math, random, sf]
published: true
---

# 朴素贝叶斯

朴素贝叶斯（Naive Bayes）是一种简单的分类算法，它的经典应用案例为人所熟知：文本分类（如垃圾邮件过滤）。

很多教材都从这些案例出发，本文就不重复这些内容了，而把重点放在理论推导（其实很浅显，别被“理论”吓到），三种常用模型及其编码实现（Python）。

如果你对理论推导过程不感兴趣，可以直接逃到三种常用模型及编码实现部分，但我建议你还是看看理论基础部分。

# 朴素贝叶斯的理论基础

朴素贝叶斯算法是基于贝叶斯定理与特征条件独立假设的分类方法。

这里提到的贝叶斯定理、特征条件独立假设就是朴素贝叶斯的两个重要的理论基础。

## 1.1 贝叶斯定理

先看什么是条件概率。

`P(A|B)` 表示事件B已经发生的前提下，事件A发生的概率，叫做事件B发生下事件A的条件概率。

其基本求解公式为：`P(A|B)=P(AB)/P(B)`

贝叶斯定理便是基于条件概率，通过 `P(A|B)` 来求 `P(B|A)`：`P(B|A)=P(A|B)P(B)/P(A)`

顺便提一下，上式中的分母 P(A),可以根据全概率公式分解为：

![image](https://user-images.githubusercontent.com/18375710/73417414-f1b87b80-4352-11ea-9fd8-f59a3a35108f.png)

## 1.2 特征条件独立假设

这一部分开始朴素贝叶斯的理论推导，从中你会深刻地理解什么是特征条件独立假设。

给定训练数据集（X,Y），其中每个样本x都包括n维特征，即x=(x1,x2,x3,...,xn)，类标记集合含有k种类别，即y=(y1,y2,...,yk)。

如果现在来了一个新样本x，我们要怎么判断它的类别？从概率的角度来看，这个问题就是给定x，它属于哪个类别的概率最大。

那么问题就转化为求解 `P(y1|x),P(y2|x),...,P(yk|x)` 中最大的那个，即求后验概率最大的输出：`argmaxykP(yk|x)`，那 `P(yk|x)` 怎么求解？

答案就是贝叶斯定理：

`P(yk|x)=P(x|yk)P(yk)/P(x)`

根据全概率公式，可以进一步地分解上式中的分母：

【公式1】

![image](https://user-images.githubusercontent.com/18375710/73417562-71dee100-4353-11ea-944e-669f1cfa7d7c.png)

先不管分母，分子中的P(yk)是先验概率，根据训练集就可以简单地计算出来。

而条件概率 `P(x|yk)=P(x1,x2,...,xn|yk)`，它的参数规模是指数数量级别的，假设第i维特征xi可取值的个数有Si个，
类别取值个数为k个，那么参数个数为：![image](https://user-images.githubusercontent.com/18375710/73417634-c2eed500-4353-11ea-848d-557dd6df3b79.png)

这显然不可行。

### 解决特征值过高的问题

**针对这个问题，朴素贝叶斯算法对条件概率分布作出了独立性的假设**，通俗地讲就是说假设各个维度的特征x1,x2,...,xn互相独立，在这个假设的前提上，条件概率可以转化为：

【公式2】

![image](https://user-images.githubusercontent.com/18375710/73417729-1e20c780-4354-11ea-90e9-58572ecf5291.png)

这样，参数规模就降到 ![image](https://user-images.githubusercontent.com/18375710/73417753-3a246900-4354-11ea-9a7d-a7f370c9ce64.png)

### 后验概率的公式

以上就是针对条件概率所作出的特征条件独立性假设，至此，先验概率P(yk)和条件概率 `P(x|yk)` 的求解问题就都解决了。

那么我们是不是可以求解我们所要的后验概率 `P(yk|x)` 了？

答案是肯定的。

我们继续上面关于 `P(yk|x)` 的推导，将【公式2】代入【公式1】得到：

![image](https://user-images.githubusercontent.com/18375710/73417851-88396c80-4354-11ea-93e8-9f83562ec1ef.png)

于是朴素贝叶斯分类器可表示为：

![image](https://user-images.githubusercontent.com/18375710/73417890-aacb8580-4354-11ea-82fa-7629b3ee31ea.png)

### 最终表示

因为对所有的yk，上式中的分母的值都是一样的（为什么？注意到全加符号就容易理解了），所以可以忽略分母部分，朴素贝叶斯分类器最终表示为：

![image](https://user-images.githubusercontent.com/18375710/73417965-df3f4180-4354-11ea-8a94-7877d1c2cc28.png)

关于P(yk)，`P(xi|yk)`的求解，有以下三种常见的模型.

# 常见模型

## 多项式模型

当特征是离散的时候，使用多项式模型。

多项式模型在计算先验概率P(yk)和条件概率 `P(xi|yk)`时，会做一些**平滑处理**，具体公式为：

![image](https://user-images.githubusercontent.com/18375710/73418160-88863780-4355-11ea-81e9-569aefa9a6c0.png)

N是总的样本个数，k是总的类别个数，Nyk是类别为yk的样本个数，α是平滑值。

![image](https://user-images.githubusercontent.com/18375710/73418269-ea46a180-4355-11ea-95e9-87090b58ed8e.png)

Nyk是类别为yk的样本个数，n是特征的维数，Nyk,xi是类别为yk的样本中，第i维特征的值是xi的样本个数，α是平滑值。

当α=1α=1时，称作Laplace平滑，当0<α<10<α<1时，称作Lidstone平滑，α=0α=0时不做平滑。

如果不做平滑，当某一维特征的值xixi没在训练样本中出现过时，会导致 `P(xi|yk)=0P(xi|yk)=0`，从而导致后验概率为0。

加上平滑就可以克服这个问题。

## 高斯模型

当特征是连续变量的时候，运用多项式模型就会导致很多 `P(xi|yk)=0`（不做平滑的情况下），此时即使做平滑，所得到的条件概率也难以描述真实情况。

所以处理连续的特征变量，应该采用高斯模型。

高斯模型假设每一维特征都服从**高斯分布（正态分布）**：

![image](https://user-images.githubusercontent.com/18375710/73418395-4b6e7500-4356-11ea-8c99-93a8723a8058.png)

## 伯努利模型

与多项式模型一样，伯努利模型适用于离散特征的情况，所不同的是，伯努利模型中每个特征的取值只能是1和0(以文本分类为例，某个单词在文档中出现过，则其特征值为1，否则为0).

伯努利模型中，条件概率 `P(xi|yk)` 的计算方式是：

当特征值xi为1时，`P(xi|yk)=P(xi=1|yk)`；

当特征值x为0时，`P(xi|yk)=1−P(xi=1|yk)`；

### 编程实现

伯努利模型和多项式模型是一致的，BernoulliNB需要比MultinomialNB多定义一个二值化的方法，该方法会接受一个阈值并将输入的特征二值化（1，0）。

当然也可以直接采用MultinomialNB，但需要预先将输入的特征二值化。写到这里不想写了，编程实现留给读者吧。


# 个人收获

分词中就可以根据贝叶斯定理去处理，二元乃至多元。

通过拼音推导汉字，也是同样的道理。

# 拓展阅读

[EM 最大概率算法]()

[HMM]()

[卡尔曼滤波]()

# 参考资料

《统计学习方法》，李航

《机器学习》，Tom M.Mitchell

[朴素贝叶斯理论推导与三种常见模型](https://www.cnblogs.com/xbinworld/p/4266146.html)

* any list
{:toc}