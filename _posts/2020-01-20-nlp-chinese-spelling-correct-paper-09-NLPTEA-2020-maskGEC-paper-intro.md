---
layout: post
title: NLP 中文拼写检测纠正论文-09-MaskGEC Improving Neural Grammatical Error Correction via Dynamic Masking
date:  2020-1-20 10:09:32 +0800
categories: [Data-Struct]
tags: [chinese, nlp, algorithm, csc, paper, sh]
published: true
---

# 拼写纠正系列

[NLP 中文拼写检测实现思路](https://houbb.github.io/2020/01/20/nlp-chinese-spelling-correct-01-intro)

[NLP 中文拼写检测纠正算法整理](https://houbb.github.io/2020/01/20/nlp-chinese-spelling-correct-02)

[NLP 英文拼写算法，如果提升 100W 倍的性能？](https://houbb.github.io/2020/01/20/nlp-chinese-spelling-correct-03-100w-faster)

[NLP 中文拼写检测纠正 Paper](https://houbb.github.io/2020/01/20/nlp-chinese-spelling-correct-paper)

[java 实现中英文拼写检查和错误纠正？可我只会写 CRUD 啊！](https://houbb.github.io/2020/01/20/nlp-chinese-word-checker-01-intro)

[一个提升英文单词拼写检测性能 1000 倍的算法？](https://houbb.github.io/2020/01/20/nlp-chinese-word-checker-02-1000x)

[单词拼写纠正-03-leetcode edit-distance 72.力扣编辑距离](https://houbb.github.io/2020/01/20/nlp-chinese-word-checker-03-edit-distance-intro)

## 开源项目

[nlp-hanzi-similar 汉字相似度](https://github.com/houbb/nlp-hanzi-similar)

[pinyin 汉字拼音](https://github.com/houbb/pinyin)

[word-checker 拼写检测](https://github.com/houbb/word-checker)

[sensitive-word 敏感词](https://github.com/houbb/sensitive-word)

# 前言

大家好，我是老马。

下面学习整理一些其他优秀小伙伴的设计、论文和开源实现。

# 摘要

语法错误修正（GEC）是一个前景广阔的自然语言处理（NLP）应用，其目标是将含有语法错误的句子转化为正确的句子。

神经机器翻译（NMT）方法已经被广泛应用于这一类似翻译的任务。

然而，这类方法需要大量的错误标注句对的并行语料，而这类语料特别在中文语法错误修正领域较难获取。

本文提出了一种简单而有效的方法，通过动态掩码来改进基于NMT的GEC模型。

通过在训练过程中动态地将随机掩码添加到原始源句子中，可以生成更多多样化的错误修正句对，从而增强语法错误修正模型的泛化能力，而无需额外的数据。

在NLPCC 2018任务2的实验结果表明，我们的MaskGEC模型提升了神经GEC模型的性能。

此外，我们的单模型在中文GEC任务中超越了NLPCC 2018任务2中的现有最先进集成系统，而无需任何额外的知识。

# 引言

近年来，语法错误修正（GEC）作为一种自然语言处理（NLP）应用，吸引了广泛的关注。

语法错误修正任务的定义是：给定一个可能包含语法错误的句子，要求检测并修正句子中的错误，返回其无错误的自然语言表示。将错误的句子视为源语言，修正后的句子视为目标语言，GEC任务可以被视为一种机器翻译（MT）任务。

例如，英语GEC可以看作是将“错误”英语翻译为“正确”英语。

随着深度学习的快速发展，基于序列到序列（seq2seq）模型的神经机器翻译（NMT）方法已成为机器翻译领域的主流。

近期，许多研究（如Yuan和Briscoe 2016年；Ji等人2017年；Chollampatt和Ng 2018年）将神经seq2seq模型应用于语法错误修正任务，并取得了一定进展。然而，这些基于NMT的GEC模型面临一个问题。由于语法错误修正句对的并行语料库规模有限，包含数百万参数的seq2seq模型难以得到充分训练。因此，即便是测试句子与训练实例略有不同，模型也可能无法修正。

为了解决上述神经语法错误修正模型的局限性，我们提出了一种简单而有效的动态掩码方法来提升神经GEC模型的性能。

在训练过程中，我们通过掩码向输入数据添加各种随机噪声，以动态生成噪声源句子，但保持目标句子不变。通过将新的源句子与对应的目标句子配对，我们可以获得更多的错误修正句对，如图1所示。为了方便起见，我们将通过随机噪声生成的新错误修正句对称为噪声句对。

我们不是将上述噪声句对作为额外的训练实例使用，而是直接用噪声句子替代源端的原始句子。通过这种方式，我们的语法错误修正模型在整个训练过程中能够获得更多的错误修正句对样本，而不增加训练集的大小。通过引入噪声，我们的方法增强了语法错误修正模型的泛化能力。

实验结果表明，采用动态掩码方法的语法错误修正模型优于基线的seq2seq模型，并在中文GEC任务中取得了最先进的结果。

简而言之，本文的贡献如下：

- 我们提出了一种简单而有效的动态掩码方法，以解决中文神经GEC模型的局限性。据我们所知，这是首次将动态掩码技术引入中文GEC任务。

- 我们的模型在没有额外资源的情况下，取得了NLPCC 2018任务2中的最先进结果，证明了我们方法在中文GEC任务中的有效性。

![F1](https://houbb.github.io//static/img/2024-12-23-nlp2020-maskGEC-F1.png)


### 图 2：我们动态掩码方法在中文语法错误修正中的训练过程示意图

该图展示了基于Transformer架构的seq2seq模型在动态掩码方法中的训练过程。‘@’是占位符，表示可能的替换词。`<s>` 和 `</s>` 分别表示‘BOS’（句子开始）和‘EOS’（句子结束）。

![F2](https://houbb.github.io//static/img/2024-12-23-nlp2020-maskEGC-F2.png)


# 模型

## 神经GEC模型

一个seq2seq模型基本上由编码器-解码器架构组成。

seq2seq模型已被证明在许多NLP任务中有效，如机器翻译（Sutskever, Vinyals, and Le 2014）、文本摘要（Rush, Chopra, and Weston 2015）、对话系统（Serban et al. 2016）等。

为了修正潜在的错误，GEC系统需要理解句子的意义。

由于自然语言句子中可能存在远距离依赖，这使得理解句子变得困难。

递归神经网络（RNN）擅长建模单词序列并捕捉句子的上下文。因此，RNN通常被前期的神经模型用于GEC（如Yuan和Briscoe 2016），尤其是其多种形式的门控循环单元（GRU）网络（如Xie et al. 2016；Ge, Wei, and Zhou 2018）。由于大多数语法错误是局部的，并且依赖于附近的词语，因此GEC系统必须捕捉局部上下文。卷积神经网络（CNN）通过窗口操作有效地捕获局部信息。通过层级多层卷积网络（Chollampatt和Ng 2018），较高层次的卷积层还可以捕获远距离词语之间的更宽广上下文。

自从提出以来，注意力机制（Bahdanau, Cho, and Bengio 2014；Luong, Pham, and Manning 2015）在序列学习任务中取得了巨大成功。近期的神经语法错误修正模型（如Xie et al. 2016；Ji et al. 2017；Chollampatt和Ng 2018；Ge, Wei, and Zhou 2018）已引入注意力机制，使得模型能够集中注意力于句子中具有语法错误的相关部分。

大多数前期的神经GEC模型使用RNN或CNN作为编码器和解码器，而Transformer（Vaswani et al. 2017）是一种新的编码器-解码器框架。最近由Google提出的Transformer模型完全基于注意力机制，展示了其强大的建模单词序列的能力，并在机器翻译任务中取得了最好的性能。

我们的语法错误修正模型采用了Transformer作为NMT框架。值得注意的是，NMT框架的选择并不是本文的重点。我们期望其他的seq2seq模型也能从我们的方法中获益。

给定一个源序列
\[ X = (x_1, x_2, ..., x_m) \]  
和其对应的修正序列  
\[ Y = (y_1, y_2, ..., y_n) \]  
其中，m和n分别是序列X和Y的长度，语法错误修正模型需要估计以下条件概率：
\[ P(Y | X) = \prod_{i=1}^{n} P(y_i | y_1, ..., y_{i-1}, X; \Theta) \]  
其中，Θ是模型参数。该模型通过最大似然估计（MLE）进行训练，即最小化负对数似然（NLL）损失：
\[ l(\Theta) = - \sum_{i=1}^{n} \log P(y_i | y_1, ..., y_{i-1}, X; \Theta) \]

## 动态掩码

对于神经网络模型，训练语料库的大小通常是影响模型性能的关键因素之一。

为了方便和高效地获取更多训练样本，我们在训练过程中动态地（如图2所示）以一定概率向源句子X添加噪声，并获得噪声文本：

\[ \tilde{X}^{(j)} = (\tilde{x}_1^{(j)}, ..., \tilde{x}_i^{(j)}, ..., \tilde{x}_m^{(j)}) \]  
其中，第i个词的替代方式如下：
\[ \tilde{x}_i^{(j)} = \begin{cases} 
f(x_i), & p > \delta \\
x_i, & p \leq \delta 
\end{cases} \]  

其中，f是词替代函数，p是一个从[0.0, 1.0]区间均匀分布生成的随机数，δ是替换概率的阈值（我们设置δ = 0.3）。

在t轮迭代中，生成一组噪声源文本集 \(\{\tilde{X}^{(1)}, \tilde{X}^{(2)}, ..., \tilde{X}^{(t)}\}\)。我们的GEC模型需要将这组噪声文本映射到目标句子Y。我们在算法1中描述了我们的噪声训练方法，其中S是训练语料库中原始错误修正句对的集合，S(t)是第t轮迭代中的噪声句对集合。

不同的噪声方案对模型性能有不同的影响。我们考虑了以下几种噪声方案，并进行了比较实验：

- **填充替换**：源句子中的每个词有一定概率（δ）被选中并替换为填充符号 `<pad>`。通过填充替换，我们可以在GEC模型的训练过程中指数级地增加训练样本，并减少训练实例的重复性。此外，通过将一些词替换为填充符号，我们可以减少GEC模型对特定词语的依赖，从而强迫模型在隐藏层中从上下文中学习被替换词的含义，帮助提升性能。
- **随机替换**：与填充替换类似，GEC模型以概率δ随机选择源句子中的一些词。然而，模型用词汇表V中的随机词代替它们，而不是填充符号。替代的词从词汇表V中均匀地随机抽取，概率为 `1/|V|`，其中 `|V|` 是V的大小。随机替换比填充替换更适合GEC任务，因为它可以生成与真实错误文本更接近的噪声样本。
- **词频替换**：一般来说，自然语言中的语法错误往往是高频词被低频词替代。因此，我们认为在错误的源句子中，高频词作为替代错误的概率应更高。基于此，我们提出了一个基于词频的替换方法。我们的GEC模型统计训练语料库中目标句子中每个词的出现次数，得到词频分布。在训练过程中，GEC模型根据词频而非均匀采样来选择替代词。
- **同音替换**：同音现象导致的错误在中文文本中的语法错误中占据了很大比例。中文中有许多同音字，它们发音相同，但形状和意义不同。图3展示了一些由同音现象引起的语法错误。我们使用pypinyin工具获取字符对应的拼音（标准汉语的官方拼音系统）。然后，我们根据拼音对目标句子中的词进行分类，并基于拼音类别统计词频，从而得到每种拼音类型的词汇概率分布。在训练过程中，我们选择要替换的词，并根据这些词的拼音查找同音词进行替换。
- **混合替换**：除了单一的噪声方案外，我们还提出了混合替换方法。对于每个训练实例，我们的中文GEC模型随机选择一个单一的噪声方案或保持原样（不做改变），并将其应用于训练过程中。通过这种方式，我们将所有单一噪声方案整合在一起，从而获得更多样化的噪声句对。

### 表1：NLPCC-2018数据集概览

| 分割    | 句子数   | 源语令牌数   | 目标语令牌数 |
| ------- | -------- | ------------ | ------------ |
| 训练集  | 120万    | 2370万       | 2500万       |
| 开发集  | 5000     | 99.3K        | 104.1K       |
| 测试集  | 2000     | 58.9K        | -            |

![F3](https://houbb.github.io//static/img/2024-12-23-nlp2020-maskEGC-F3.png)



# 实验

## 设置

为了验证我们方法在中文语法错误纠正任务中的有效性，我们在NLPCC 2018任务2的数据集上进行了实验（Zhao等人，2018）。数据集的统计信息如表1所示。该共享任务提供了中文GEC的第一个最新基准数据集。

该任务的语料来自Lang-84网站，这是一个语言学习平台，用户可以在上面纠正他人写作的内容。语料库中的文章由中文作为第二语言（CSL）学习者编写，并由中国的母语者进行纠正。需要注意的是，我们的模型没有使用任何额外的自然语言资源。

由于在这个数据集中一个错误的句子可能有多个纠正版本，我们将源句子与每个纠正句子一一结合，构建平行语料库。

这样，我们总共得到了120万对句子。由于没有官方的开发数据集，我们随机从训练集抽取了5000对句子作为开发集，这一点借鉴了之前的工作（Ren, Yang, 和 Xun 2018）。

官方测试集包含从PKU中文学习者语料库中提取的2000个句子，这些句子是由外国大学生写的。提供了两组注释，用以给出这些句子中语法错误的黄金标准修正。

我们使用官方的MaxMatch（M2）（Dahlmeier和Ng 2012）评分工具来评估我们的GEC模型，并将其与这个共享任务中的先前系统进行比较。给定一个源句子和系统假设，M2评分工具计算源句子和系统假设之间所有可能的短语级编辑序列，并找出与黄金标准注释的重叠度最高的编辑序列。然后使用该最优序列计算精度、召回率和F0.5值。官方的评估指标是M2评分工具提供的F0.5值，所有注释组都被考虑在内。

## 模型和训练细节

我们使用OpenNMT-py（一个由PyTorch开发的神经机器翻译工具包）实现了我们的中文语法错误纠正模型。我们的GEC模型的训练过程和详细信息如下：该seq2seq模型的架构是Transformer的基本模型。编码器是由6个相同的层堆叠而成，每层有两个子层，一个是多头自注意力层，另一个是逐位置的全连接前馈网络。解码器也是由6个相同的层堆叠而成。

然而，在每一层的中间有一个第三个子层，它对编码器堆栈的输出执行多头注意力操作。Transformer自注意力的头数设置为8。Transformer前馈网络的隐藏层大小为2048。

源侧和目标侧的词向量维度均为512。我们使用Xavier初始化方法（Glorot和Bengio 2010）对模型参数进行初始化，并应用位置编码。对于中文语法错误纠正任务，我们使用BERT项目中的分词脚本对中文文本进行分词，并保持非中文单词不变。我们在编码器和解码器上应用了dropout操作（Srivastava等人，2014），dropout概率为0.1。

我们的模型采用Adam优化器，初始学习率为2，beta值为(0.9, 0.998)。我们使用Noam的学习率衰减方案（Vaswani等人，2017），预热步数为8000。我们还添加了标签平滑，epsilon值为0.1。批处理大小设置为4096个token。模型推理时的beam大小为12。我们采用早停技术，并根据开发集上的验证困惑度选择最佳模型。



# 系统性能比较

### 表 2: NLPCC-2018 数据集上的系统性能

| 系统 | 类型 | 资源 | 精度 (P) | 召回率 (R) | F0.5 值 |
| --- | --- | --- | --- | --- | --- |
| YouDao (Fu, Huang, and Duan 2018) | 集成 | 语言模型 (LM), 相似字符集 (SCS) | 35.24 | 18.64 | 29.91 |
| AliGM (Zhou et al. 2018) | 集成 | 语言模型 (LM), 嵌入 (Emb.) | 41.00 | 13.75 | 29.36 |
| BLCU (Ren, Yang, and Xun 2018) | 单一 | 嵌入 (Emb.) | 41.73 | 13.08 | 29.02 |
| BLCU (集成) (Ren, Yang, and Xun 2018) | 集成 | 嵌入 (Emb.) | 47.63 | 12.56 | 30.57 |
| Char-Transformer | 单一 | - | 36.57 | 14.27 | 27.86 |
| Dropout-Src (Junczys-Dowmunt et al. 2018) | 单一 | - | 39.08 | 18.80 | 32.15 |
| Ours | 单一 | - | 44.36 | 22.18 | 36.97 |

**说明**：Ours（我们的模型）指的是基于Char-Transformer的动态掩码方法，采用了混合替代噪声方案。在资源列中，LM 表示语言模型，SCS 表示相似字符集，Emb. 表示预训练词向量。


## 基准对比

我们将我们的模型与NLPCC 2018任务2中的所有先前中文语法错误纠正（GEC）系统进行了比较，以下是评估在NLPCC 2018数据集上表现最佳的系统：

- **YouDao (Fu, Huang, and Duan 2018)**: 网易有道公司开发的中文GEC系统。该系统使用五个不同的混合模型，每个模型独立地阶段性地纠正句子，并使用语言模型重新排序输出以进行集成。

- **AliGM (Zhou et al. 2018)**: 阿里巴巴开发的中文GEC系统，结合了基于NMT的方法、基于SMT的方法和基于规则的方法，以及各种模块。

- **BLCU 和 BLCU (集成) (Ren, Yang, and Xun 2018)**: 北京语言大学开发的中文GEC系统。前者是基于多层卷积的seq2seq模型，后者是四个不同初始化的单一模型的集成。

为了验证我们的动态掩码方法在中文神经语法错误纠正模型上的有效性，我们实现了一个基于字符的Transformer模型（Char-Transformer）作为基准。基于Char-Transformer，我们还重新实现了Junczys-Dowmunt等人（2018）提出的源词丢弃方法。

按照他们的做法，我们设置源词的完整嵌入向量以概率psrc为0，其他所有嵌入值则按比例1/(1−psrc)进行缩放。他们提出，源词丢弃可以为神经语法错误纠正带来提升。通过引入源侧的噪声，模型被教导减少对输入的信任，更加积极地进行修正。

## 结果

我们将采用动态掩码方法并结合混合替代方案的最佳模型Char-Transformer与在NLPCC-2018数据集上评估的最先进的系统进行了对比。表2展示了我们的方法与先前系统在中文GEC基准数据集上的评估结果，使用的是官方评分工具。

| 掩码策略 | 精度 (P) | 召回率 (R) | F0.5 值 |
| --- | --- | --- | --- |
| 静态掩码 | 43.73 | 21.71 | 36.35 |
| 动态掩码 | 44.36 | 22.18 | 36.97 |

Char-Transformer模型的F0.5得分为27.86，这与该基准模型和领先的中文GEC系统之间存在较大差距。

然而，在应用了动态掩码和混合替代方案后，我们的模型达到了36.97的F0.5得分，显著超越了排名第一的YouDao系统（F0.5 = 29.91），领先幅度达7.06 F0.5。在BLCU系统的基础上，Ren, Yang, 和 Xun构建了一个集成模型，并达到了30.57的F0.5得分，这是迄今为止在NLPCC 2018任务2中发布的最佳结果。然而，我们提出的中文GEC模型仍然取得了比当前最佳结果更高的F0.5得分，创下了新的NLPCC-2018数据集上的最新成绩。

值得注意的是，NLPCC 2018任务2中的前三名模型都是集成模型，但我们的单一模型仍然超越了它们。

我们提出的方法与这些集成方法完全正交，这意味着我们的GEC模型可能通过集成方法取得更好的结果。

Junczys-Dowmunt等人（2018）的方法达到了32.15的F0.5，比Char-Transformer模型提高了4.29 F0.5。尽管如此，我们的动态掩码模型仍然以4.82 F0.5的显著优势超过了该方法。

原因在于我们提出的动态掩码方法生成了更多样化的噪声句子对，这可能有助于提升我们GEC模型的泛化能力。



# 动态掩码的影响

执行掩码操作的时机不同会导致两种噪声策略。

**静态掩码策略**：在数据预处理阶段进行一次掩码，生成一个静态掩码。训练数据可以复制k次，以避免每个训练实例在每个epoch中使用相同的掩码。

因此，每个源句子在几十个训练epoch中会以k种不同的方式进行掩码。

**动态掩码策略**：动态掩码策略根据设置的掩码概率（δ）进行逐步变化。

掩码概率（δ）从0到1变化。从图4可以看出，当δ为0.3时，模型的性能最佳。当δ减小时，生成的噪声源句子的多样性急剧减少，动态掩码逐渐退化为普通的seq2seq学习。

当δ增大时，噪声的指数级增长会损害训练过程。因此，替代概率的阈值应谨慎选择，以在泛化能力和鲁棒性之间取得平衡。

## 词汇切分的影响

由于中文的特点，我们的语法错误纠正模型采用了基于字符的NMT方法。为了进行比较，我们还实现了一个基于子词的Transformer模型（Subword-Transformer）。

在**Subword-Transformer**模型中，采用了BPE（字节对编码）算法（Sennrich, Haddow, 和 Birch 2016）将稀有词分割为子词单元。根据在开发集上的表现，BPE操作的数量设置为8,000。结果如表5所示。

我们可以发现，Char-Transformer模型的F0.5得分为27.86，优于Subword-Transformer模型（24.94 F0.5）2.92 F0.5分。

原因可能是，通过将中文字符视为分割单元，词汇表的大小可以减少到适合GEC任务的合适规模。此外，分词错误可能导致模型性能下降。

### 表 5: 基于子词和基于字符的中文GEC模型在NLPCC-2018数据集上的性能

| 模型 | 精度 (P) | 召回率 (R) | F0.5 值 |
| --- | --- | --- | --- |
| Subword-Transformer | 34.06 | 12.05 | 24.94 |
| Char-Transformer | 36.57 | 14.27 | 27.86 |

通过这些实验，我们可以看到基于字符的模型在中文语法错误纠正任务中表现出更好的性能。

这可能与中文的字符特性和模型处理输入的方式密切相关。


# 相关工作

早期的语法错误修正系统使用类型特定的分类器（De Felice 和 Pulman 2008；Rozovskaya 等 2014）。

统计机器翻译（SMT）方法的出现促进了语法错误修正系统的发展（Behera 和 Bhattacharyya 2013；Junczys-Dowmunt 和 Grundkiewicz 2016）。

此外，SMT-based 系统可以轻松与手动设计的规则（Felice 等 2014）、分类器（Rozovskaya 和 Roth 2016）以及神经模型（Chollampatt, Hoang 和 Ng 2016；Chollampatt, Taghipour 和 Ng 2016）结合，这有助于提高它们在 GEC 任务中的表现。

近年来，许多神经机器翻译（NMT）方法被提出用于 GEC 任务。

典型的神经 GEC 方法使用基于 RNN 的 seq2seq 模型（Xie 等 2016；Yuan 和 Briscoe 2016；Ji 等 2017）。

然而，基于 CNN 的 NMT 模型也在语法错误修正任务中展现了令人印象深刻的结果（Schmaltz 等 2017；Chollampatt 和 Ng 2018）。作为一种强大的编码器-解码器架构，Transformer 最近也被引入到 GEC 任务的 NMT 方法中（Junczys-Dowmunt 等 2018）。

为了应对数据稀疏问题，提出了几种方法用于在语法错误修正任务中合成平行语料库（Felice 和 Yuan 2014；Xie 等 2016）。这一过程也被称为错误生成，它通过生成人工数据作为数据增强的方法。Ge、Wei 和 Zhou（2018）提出了一种新颖的流畅度增强学习和推理机制，允许模型生成流畅度增强的句子对。

Xie 等（2018）提出了一种噪声和去噪方案，通过回译合成“真实的”静态平行数据用于语法错误修正。

实验表明，他们模型合成的人工数据与额外的非合成数据效果相同。我们动态掩码方法与上述方法的主要区别在于，我们的方法在训练 seq2seq 模型时充当了一种正则化手段。我们并没有显式地合成新的训练数据。

相反，我们的动态掩码方法更像是一个词元级的 dropout。

直到 NLPCC-2018 数据集发布之前，中文语法错误诊断（CGED）（Lee, Yu 和 Chang 2015；Lee 等 2016；Fung 等 2017；Rao 等 2018）一直是自动 CSL 错误修正领域的研究重点。

CGED 可以被视为一个序列标注问题（Zheng 等 2016；Yang 等 2017），可以使用 LSTM-CRF 架构来解决，后者将传统的条件随机场（CRF）方法和长短期记忆（LSTM）网络结合起来，预测输出标签的序列。

# 结论

本文提出了动态掩码方法能够促进传统的神经机器翻译（NMT）方法在中文语法错误修正（GEC）中的应用。

为了克服神经方法在 GEC 中的不足，我们提出了五种噪声方案，用于在 seq2seq 模型的训练过程中应用于源端。

这些噪声方案能够生成极为多样的错误修正句子对，显著提升了普通 seq2seq GEC 模型的表现。

我们简单而有效的动态掩码方法使得基于 NMT 的中文 GEC 系统在 NLPCC-2018 基准数据集上超越了所有已发布的结果，并为这一具有挑战性的任务树立了新的最先进水平。

# 致谢

感谢所有匿名评审人提供的建设性反馈。

我们的工作得到了中国国家自然科学基金（资助号：61433015）和中国国家重点研发计划（资助号：2017YFB1002101）的支持。本文的通讯作者为王侯峰。

# 参考资料

https://ojs.aaai.org/index.php/AAAI/article/view/5476

* any list
{:toc}