---
layout: post
title: openai sora 只能根据文本生成视频？不，TA 是通用物理世界模拟器
date: 2024-02-20 21:01:55 +0800
categories: [AI]
tags: [ai, paper, sh]
published: true
---

# 视频生成模型作为世界模拟器

我们探索了在视频数据上进行大规模生成模型的训练。

具体来说，我们联合在可变持续时间、分辨率和长宽比的视频和图像上训练文本条件扩散模型。

我们利用了一个在视频和图像潜在编码的时空补丁上操作的变压器架构。

我们最大的模型Sora能够生成一分钟的高保真视频。我们的结果表明，扩展视频生成模型是建立通用物理世界模拟器的有前途的途径。

> TODO: 视频

PS：其他视频省略。

这份技术报告着重介绍了两个方面：

(1) 我们将各种类型的视觉数据转化为统一表示的方法，从而实现了生成模型的大规模训练；

(2) 对Sora的能力和局限性进行了定性评估。模型和实现细节不包含在本报告中。

之前的研究已经探讨了利用各种方法对视频数据进行生成建模，包括循环网络、生成对抗网络、自回归变压器和扩散模型等。

这些工作通常专注于某一类视觉数据，或者是针对较短的视频，或者是针对尺寸固定的视频。

Sora是一种通用的视觉数据模型——它可以生成跨越不同持续时间、长宽比和分辨率的视频和图像，高清视频的长度可达一分钟。

# 将视觉数据转化为补丁 Turning visual data into patches

我们受到大型语言模型的启发，这些模型通过在互联网规模的数据上进行训练获得了通用能力。

语言模型范式的成功部分得益于优雅地统一了文本、代码、数学和各种自然语言等多种形式的令牌的使用。

在这项工作中，我们考虑了生成视觉数据模型如何继承这些好处。

而语言模型有文本令牌，Sora有视觉补丁。已经有研究表明，补丁是视觉数据模型的有效表示。我们发现，补丁是一种高度可扩展且有效的表示，可用于训练不同类型的视频和图像的生成模型。

![视觉补丁](https://images.openai.com/blob/1d2955dd-9d05-4f33-b346-be531d2a7737/figure-patches.png?trim=0,0,0,0&width=2000)

在高层次上，我们通过首先将视频压缩成低维潜在空间，然后将表示分解为时空补丁来将视频转化为补丁。

# 视频压缩网络 Video compression network

我们训练了一个网络来降低视觉数据的维度。该网络将原始视频作为输入，并输出一个在时间和空间上都被压缩的潜在表示。

Sora在这个压缩的潜在空间上进行训练，并随后生成视频。我们还训练了一个相应的解码器模型，将生成的潜在表示映射回像素空间。

# 时空潜在补丁 Spacetime latent patches

给定一个压缩的输入视频，我们提取一系列时空补丁，这些补丁充当变压器令牌。

这个方案对图像也适用，因为图像只是具有单个帧的视频。我们基于补丁的表示使得Sora能够在分辨率、持续时间和长宽比各异的视频和图像上进行训练。

在推理时，我们可以通过将随机初始化的补丁排列成适当大小的网格来控制生成视频的大小。

# 对视频生成进行变压器的扩展 Scaling transformers for video generation

Sora是一个扩散模型；给定输入的噪声补丁（以及诸如文本提示等的条件信息），它被训练成预测原始的“干净”补丁。

值得注意的是，Sora是一个扩散变压器。变压器在各种领域展示了显著的扩展特性，包括语言建模、计算机视觉和图像生成。

![t](https://images.openai.com/blob/aa8b687c-bee5-4d72-a1c8-1350d33c80d3/figure-diffusion.png?trim=0,0,0,0&width=2000)

在这项工作中，我们发现扩散变压器（diffusion transformers）在视频模型中也能有效扩展。

下面，我们展示了随着训练计算量增加，使用固定种子和输入的视频样本的比较。

随着训练计算量的增加，样本质量显著提高。

# 变化的持续时间、分辨率、长宽比 Variable durations, resolutions, aspect ratios

过去的图像和视频生成方法通常会将视频调整大小、裁剪或修剪到标准大小——例如，256x256分辨率的4秒视频。

我们发现，与其在标准大小上进行训练，不如在数据的原始大小上进行训练具有几个好处。

## 采样灵活性 Sampling flexibility

Sora可以对宽屏的1920x1080p视频、垂直的1080x1920视频以及两者之间的所有内容进行采样。

这使得Sora能够直接按照其原生长宽比为不同设备创建内容。这也使我们能够在生成全分辨率之前快速原型化较低大小的内容——而所有这些都是使用同一个模型完成的。

## 改善构图和构图 Improved framing and composition

我们凭经验发现，使用视频的原始长宽比进行训练可以改善构图和构图。

我们将Sora与我们模型的一个版本进行比较，该版本将所有训练视频裁剪为正方形，这在训练生成模型时是常见的做法。

在使用正方形裁剪训练的模型（左）中，有时会生成主体仅部分可见的视频。相比之下，来自Sora（右）的视频具有改善的构图。

## 语言理解 Language understanding

训练文本到视频生成系统需要大量带有相应文本标题的视频。我们应用了DALL·E 3中引入的重新标题技术到视频中。我们首先训练一个高度描述性的标题生成模型，然后使用它为我们训练集中的所有视频生成文本标题。我们发现，在高度描述性的视频标题上进行训练不仅可以提高文本的准确性，还可以提高视频的整体质量。

类似于DALL·E 3，我们还利用GPT将短用户提示转化为更长、详细的标题，然后将其发送到视频模型。这使得Sora能够生成高质量的视频，并准确地遵循用户的提示。

# 使用图像和视频提示

上面所有的结果以及我们网页上的展示都是文本到视频样本。但是Sora也可以使用其他输入来提示，比如现有的图像或视频。这种能力使得Sora能够执行各种图像和视频编辑任务——创建完美循环的视频、将静态图像动画化、将视频向前或向后延长等。

## 将DALL·E图像动画化

Sora能够根据图像和提示生成视频。

下面我们展示了根据DALL·E 231和DALL·E 330图像生成的示例视频。

## 扩展生成的视频

Sora还能够扩展视频，无论是向前还是向后延长时间。

以下是四个视频，它们都是从一个生成视频的片段开始向时间向后延长的。因此，这四个视频中的每一个都从不同的起点开始，但最终都会导向同一个结尾。

## 视频到视频编辑

扩散模型已经为从文本提示编辑图像和视频提供了大量的方法。

下面我们将其中一种方法，SDEdit，应用到Sora上。这种技术使得Sora能够零样本地转换输入视频的风格和环境。

## 连接视频

我们还可以使用Sora逐渐插值两个输入视频之间，创建完全不同主题和场景构图的视频之间的无缝过渡。在下面的示例中，中间的视频是左侧和右侧对应视频之间的插值。

# 图像生成能力

Sora也能够生成图像。

我们通过在一个帧的时间范围内以空间网格的形式排列高斯噪声的补丁来实现这一点。该模型可以生成各种尺寸的图像，分辨率高达2048x2048。

# 新兴的模拟能力

我们发现，当在大规模训练时，视频模型表现出许多有趣的新兴能力。

这些能力使得Sora能够模拟物理世界中的一些人、动物和环境的某些方面。这些属性在没有任何明确的归纳偏差的情况下出现，比如对3D、物体等，它们纯粹是规模效应的现象。

3D一致性。Sora可以生成具有动态摄像机运动的视频。随着摄像机的移动和旋转，人物和场景元素在三维空间中以一致的方式移动。


长程一致性和对象持久性（Long-range coherence and object permanence.）。

视频生成系统面临的一个重要挑战是在采样长视频时保持时间一致性。我们发现，尽管不总是如此，但Sora通常能够有效地模拟短程和长程依赖关系。例如，我们的模型可以在人、动物和物体被遮挡或离开画面时仍能持续存在。同样，它可以在单个样本中生成同一角色的多个镜头，并在整个视频中保持其外观。

与世界互动。Sora有时可以模拟以简单方式影响世界状态的动作。

例如，一个画家可以在画布上留下新的笔触，并随着时间的推移而持续存在，或者一个人可以吃汉堡并留下咬痕。

模拟数字世界。Sora还能够模拟人工过程——一个例子是视频游戏。

Sora可以同时使用基本策略控制Minecraft中的玩家，并以高保真度渲染世界及其动态。这些能力可以通过提示Sora并提到“Minecraft”来零样本激发。

这些能力表明，持续扩展视频模型是通向开发高度能力的物理世界和数字世界以及其中的物体、动物和人类的模拟器的有前途的路径。

# Discussion

Sora目前作为模拟器表现出许多限制。例如，它不能准确地模拟许多基本交互的物理现象，比如玻璃破碎。其他交互，比如吃食物，并不总是产生正确的物体状态变化。我们在我们的首页上列举了模型的其他常见故障模式，比如在长时间样本中发展出的不一致性或物体的突然出现。

我们相信Sora今天所具有的能力表明，继续扩展视频模型是通往开发能够模拟物理世界和数字世界以及其中生活的物体、动物和人类的有能力模拟器的有前途的道路。

# 参考资料

https://cdn.openai.com/papers/dall-e-3.pdf

* any list
{:toc}
