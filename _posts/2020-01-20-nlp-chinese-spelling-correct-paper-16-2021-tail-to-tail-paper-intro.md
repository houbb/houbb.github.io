---
layout: post
title: NLP 中文拼写检测纠正论文-16-Tail-to-Tail Non-Autoregressive Sequence Prediction for Chinese Grammatical Error Correction
date:  2020-1-20 10:09:32 +0800
categories: [Data-Struct]
tags: [chinese, nlp, algorithm, csc, paper, sh]
published: true
---

# 拼写纠正系列

[NLP 中文拼写检测实现思路](https://houbb.github.io/2020/01/20/nlp-chinese-spelling-correct-01-intro)

[NLP 中文拼写检测纠正算法整理](https://houbb.github.io/2020/01/20/nlp-chinese-spelling-correct-02)

[NLP 英文拼写算法，如果提升 100W 倍的性能？](https://houbb.github.io/2020/01/20/nlp-chinese-spelling-correct-03-100w-faster)

[NLP 中文拼写检测纠正 Paper](https://houbb.github.io/2020/01/20/nlp-chinese-spelling-correct-paper)

[java 实现中英文拼写检查和错误纠正？可我只会写 CRUD 啊！](https://houbb.github.io/2020/01/20/nlp-chinese-word-checker-01-intro)

[一个提升英文单词拼写检测性能 1000 倍的算法？](https://houbb.github.io/2020/01/20/nlp-chinese-word-checker-02-1000x)

[单词拼写纠正-03-leetcode edit-distance 72.力扣编辑距离](https://houbb.github.io/2020/01/20/nlp-chinese-word-checker-03-edit-distance-intro)

# NLP 开源项目

[nlp-hanzi-similar 汉字相似度](https://github.com/houbb/nlp-hanzi-similar)

[word-checker 中英文拼写检测](https://github.com/houbb/word-checker)

[pinyin 汉字转拼音](https://github.com/houbb/pinyin)

[opencc4j 繁简体转换](https://github.com/houbb/opencc4j)

[sensitive-word 敏感词](https://github.com/houbb/sensitive-word)

# 前言

大家好，我是老马。

下面学习整理一些其他优秀小伙伴的设计、论文和开源实现。

## 论文 & 源码

论文：https://aclanthology.org/2021.acl-long.385.pdf

源码：https://github.com/lipiji/TtT

# 摘要

我们研究了中文语法错误修正（CGEC）的问题，并提出了一种新的框架——**Tail-to-Tail (TtT)** 非自回归序列预测方法，以解决CGEC中的深层次问题。

考虑到大多数词汇是正确的，可以直接从源语言传递到目标语言，而错误的位置可以通过双向上下文信息进行估计和修正，因此我们采用了BERT初始化的Transformer编码器作为骨干模型来进行信息建模和传递。

由于仅依赖相同位置的替换无法处理变长的修正情况，因此需要联合使用多种操作，如替换、删除、插入和局部改写。

因此，在上尾部堆叠了条件随机场（CRF）层，通过建模词汇依赖关系来进行非自回归序列预测。

由于大多数词汇是正确的，且容易预测/传递到目标语言，模型可能会面临严重的类别不平衡问题。

为了解决这个问题，我们将焦点损失（focal loss）惩罚策略集成到损失函数中。

此外，除了典型的固定长度错误修正数据集外，我们还构建了一个变长语料库进行实验。

在标准数据集上，特别是在变长数据集上，实验结果表明，TtT在错误检测和修正任务上，在句子级别的准确率、精度、召回率和F1值方面表现出色，验证了其有效性。

### 1 引言

语法错误修正（Grammatical Error Correction，GEC）旨在自动检测和修正句子中的语法错误（Wang et al., 2020c）。

它是许多自然语言处理应用中的一个关键任务，例如写作助手（Ghufron 和 Rosyida, 2018；Napoles et al., 2017；Omelianchuk et al., 2020）、搜索引擎（Martins 和 Silva, 2004；Gao et al., 2010；Duan 和 Hsu, 2011）、语音识别系统（Karat et al., 1999；Wang et al., 2020a；Kubis et al., 2020）等。

语法错误可能出现在所有语言中（Dale et al., 2012；Xing et al., 2013；Ng et al., 2014；Rozovskaya et al., 2015；Bryant et al., 2019），本文重点讨论中文语法错误修正（Chinese Grammatical Error Correction，CGEC）问题（Chang, 1995）。

我们仔细研究了SIGHAN（Tseng et al., 2015）和NLPCC（Zhao et al., 2018）相关的中文语法错误修正语料库，并得出结论：语法错误类型以及相应的修正操作可以分为三种类型，如图1所示：

1. **替换（Substitution）**  
   现实中，拼音是中文写作最常用的输入法。因此，同音字混淆（例如，在类型I中，错误词和正确词的发音都是“FeiChang”）是导致语法错误（或拼写错误）的根本原因，并且可以通过替换操作进行修正，而不需要改变整个序列的结构（例如，长度）。因此，替换是固定长度（FixLen）操作。

2. **删除（Deletion）和插入（Insertion）**  
   这两种操作用于处理冗余词和遗漏词的情况。

3. **局部改写（Local Paraphrasing）**  
   有时，简单的替换、删除和插入操作无法直接修正错误，因此需要对句子的部分词序进行轻微的重新排序。
   
   图1中的类型III即为此类情况。删除、插入和局部改写可以视为变长（VarLen）操作，因为它们可能会改变句子的长度。

通过这些操作，可以更准确地修正中文中的语法错误。

- Figure 1: Illustration for the three types of operations to correct the grammatical errors: Type I-substitution; Type II-deletion and insertion; Type III-local paraphrasing.

![F1](http://houbb.github.io/static/img/2024-12-24-2020-TtT-F1.png)

- Figure 2: Illustration of the token information flows from the bottom tail to the up tail

![F2](http://houbb.github.io/static/img/2024-12-24-2020-TtT-F2.png)


# 2 相关工作

近年来，尽管已经有许多方法被提出以解决中文语法错误修正（CGEC）问题，但一些关键和本质方面仍未得到充分解决。

一般来说，序列翻译（sequence translation）和序列标注（sequence tagging）是解决CGEC问题的两种最典型技术范式。

随着神经机器翻译（Bahdanau et al., 2015；Vaswani et al., 2017）的发展，基于注意力机制的seq2seq编码器-解码器框架被引入，旨在以序列翻译的方式处理CGEC问题（Wang et al., 2018；Ge et al., 2018；Wang et al., 2019，2020b；Kaneko et al., 2020）。

基于seq2seq的翻译模型容易训练，并且能够处理上述所有类型的修正操作。

然而，考虑到暴露偏差问题（Ranzato et al., 2016；Zhang et al., 2019），生成的结果通常会受到虚假生成（hallucination）现象的影响（Nie et al., 2019；Maynez et al., 2020），即便加入了复制机制（Gu et al., 2016）（Wang et al., 2019），生成的文本也未必忠实于源文本。

因此，Omelianchuk et al.（2020）和Liang et al.（2020）提议通过纯粹的标注策略来处理GEC问题，而不是生成任务。

所有修正操作，如删除、插入和替换，都可以通过预测标签来引导。

然而，纯粹的标注策略需要将词汇表V扩展约三倍，通过为原始词添加“插入-”和“替换-”前缀（例如，“insertion-good”，“substitution-paper”），这显著降低了计算效率。此外，纯标注框架需要进行多次预测，直到没有更多操作被预测出来，这种方法既低效又不够优雅。

最近，许多研究者在中文语法错误修正任务中微调预训练语言模型，如BERT，取得了较好的结果（Zhao et al., 2019；Hong et al., 2019；Zhang et al., 2020b）。然而，由于BERT框架的局限性，它们大多只能处理固定长度的修正场景，无法灵活地进行删除、插入和局部改写操作。

此外，在我们的研究中，我们还观察到一个明显但关键的现象：句子中的大部分词是正确的，不需要改变。该现象在图2中得到了体现，其中操作流从下尾到上尾。灰色虚线表示“保持”操作，红色实线表示上述三种修正操作。直观上，目标CGEC模型应具备直接将正确的词从下尾传递到上尾的能力，因此基于Transformer（Vaswani et al., 2017）的编码器（例如BERT）似乎是一个优选方案。另一方面，考虑到几乎所有典型的CGEC模型都基于序列标注或序列翻译范式，最大似然估计（Maximum Likelihood Estimation，MLE）（Myung, 2003）通常作为参数学习方法，但在CGEC场景下会遇到严重的类/标签不平衡问题。然而，现有的研究并未深入探讨CGEC任务中的这个问题。

为了克服上述挑战，我们提出了一种新的框架，称为尾到尾非自回归序列预测（tail-to-tail non-autoregressive sequence prediction，TtT），用于解决CGEC问题。

具体来说，为了直接将词汇信息从下尾传递到上尾，我们引入了一个基于BERT的序列编码器来进行双向表示学习。为了同时进行替换、删除、插入和局部改写操作，受（Sun et al., 2019）的启发，我们在上尾堆叠了一个条件随机场（Conditional Random Fields，CRF）层，通过建模相邻词的依赖关系来进行非自回归序列预测。为了缓解类不平衡问题，我们采用了焦点损失（Focal Loss）策略（Lin et al., 2020），考虑到句子中大多数词不会被更改。

### 本文贡献：

- 提出了一个新的框架，名为尾到尾非自回归序列预测（TtT），用以处理CGEC问题。

- 使用基于BERT的编码器与CRF层作为骨干网络，能够同时进行替换、删除、插入和局部改写操作。

- 采用焦点损失策略来缓解类不平衡问题，考虑到句子中大多数词不会改变。

- 在多个基准数据集上，特别是在变长语法修正数据集上，进行了广泛的实验，验证了所提出方法的有效性。


# 2 提出的TtT框架

## 2.1 概述

图3展示了我们提出的TtT框架的基本组件。输入是一个包含语法错误的句子X = (x₁, x₂, ..., xₜ)，其中xᵢ表示句子中的每个词（汉字），T是X的长度。语法错误修正任务的目标是纠正X中的所有错误，并生成一个新的句子Y = (y₁, y₂, ..., yT₀)。在这里，需要强调的是T不一定等于T₀，因此T₀可以等于、长于或短于T。

通过多个Transformer（Vaswani等人，2017）层进行双向语义建模和自下而上的直接词元信息传递。

一个条件随机场（CRF）（Lafferty等人，2001）层堆叠在上端，用于通过建模相邻词元之间的依赖关系，进行非自回归的序列生成。

引入低秩分解和束搜索维特比算法来加速计算。

在训练阶段，采用聚焦损失（Focal Loss）惩罚策略（Lin等人，2020）来缓解类别不平衡问题。

## 2.2 变长输入

由于目标句子Y的长度T₀不一定等于输入序列X的长度T。

因此，在训练和推理阶段，长度的不同会影响预测句子的完整性，尤其是当T < T₀时。

为了解决这个问题，我们设计了几种简单的技巧来预处理样本。

```
假设X = (x₁, x₂, x₃, <eos>)：

1. 当T = T₀，即Y = (y₁, y₂, y₃, <eos>)时，什么都不做；

2. 当T > T₀时，例如Y = (y₁, y₂, <eos>)，这意味着在修正过程中，X中的一些词元会被删除。那么在训练阶段，我们可以将T - T₀个特殊词元<pad>填充到Y的尾部，使得T = T₀，变为Y = (y₁, y₂, <eos>, <pad>)；

3. 当T < T₀时，例如Y = (y₁, y₂, y₃, y₄, y₅, <eos>)，这意味着需要在原句X中插入更多信息。此时，我们会将特殊符号<mask>填充到X的尾部，表示这些位置可能会被转换成一些新的真实词元：X = (x₁, x₂, x₃, <eos>, <mask>, <mask>)。
```


## 2.3 双向语义建模

Transformer层（Vaswani等人，2017）特别适合用于进行双向语义建模和自下而上的信息传递。如图3所示，在准备输入样本后，嵌入层和一堆初始化为预训练中文BERT（Devlin等人，2019）的Transformer层随后进行语义建模。具体而言，对于输入，我们首先通过将词嵌入和位置嵌入相加来获得表示：

```
H₀ₜ = Eₓᵗ + Eₚₜ   (1)
```

其中，0是层索引，t是状态索引。Eₓᵗ和Eₚₜ分别是词元和位置的嵌入向量。

然后，获得的嵌入向量H₀会输入到多个Transformer层中。使用多头自注意力机制进行双向表示学习：

```
H₁ₜ = LN(FFN(H₁ₜ) + H₁ₜ)  
H₁ₜ = LN(SLF-ATT(Q₀ₜ, K₀, V₀) + H₀ₜ)  
Q₀ = H₀Wᵩ  
K₀, V₀ = H₀Wₖ, H₀Wᵥ  
(2)
```

其中，SLF-ATT(·)、LN(·)和FFN(·)分别表示自注意力机制、层归一化和前馈网络（Vaswani等人，2017）。

请注意，我们的模型是一个非自回归序列预测框架，因此我们使用所有的序列状态K₀和V₀作为注意力上下文。

然后，每个节点将双向吸收上下文信息。

经过L层Transformer后，我们得到最终的输出表示向量Hᴸ ∈ ℝ^(max(T, T₀)×d)。

## 2.4 非自回归序列预测

#### 直接预测

我们模型的目标是将包含语法错误的输入句子X转换为正确的句子Y。

然后，由于我们已经获得了序列表示向量Hᴸ，我们可以直接添加一个softmax层来预测结果，类似于非自回归神经机器翻译（Gu和Kong，2020）和基于BERT的语法错误修正任务的微调框架（Zhao等人，2019；Hong等人，2019；Zhang等人，2020b）。

具体来说，插入一个线性变换层，并使用softmax操作生成目标词汇V上的概率分布Pᵈᵖ(yₜ)：

```
sₜ = hₜᵀWˢ + bˢ  
Pᵈᵖ(yₜ) = softmax(sₜ)  

(3)
```


其中，`hₜ ∈ ℝᵈ，Wˢ ∈ ℝᵈ×|V|，bˢ ∈ ℝ|V|，sₜ ∈ ℝ|V|`。

然后，我们根据预测分布为每个状态获得结果：

```
y₀ₜ = argmax(Pᵈᵖ(yₜ))   (4)
```


然而，尽管这种直接预测方法在固定长度的语法错误修正问题上有效，但它只能执行相同位置的替换操作。

对于需要删除、插入和局部改写的复杂修正情况，其性能不可接受。

这种性能不佳的现象在非自回归神经机器翻译任务中也有讨论（Gu和Kong，2020）。导致性能不佳的一个重要原因是忽略了相邻词元之间的依赖信息。因此，需要调用依赖建模来提高生成性能。

自然地，引入了线性链条件随机场（CRF）（Lafferty等人，2001）来解决这个问题，幸运的是，Sun等人（2019）也使用CRF来解决非自回归序列生成问题，这给我们带来了很大的启发。

#### 通过CRF进行依赖建模

然后，给定输入序列X，在CRF框架下，目标序列Y的似然性 `Pᶜʳᶠ(Y | X)` 构建为：

```
Pᶜʳᶠ(Y | X) =  
(1 / Z(X)) exp{ ∑(t=1 to T) s(yₜ) + ∑(t=2 to T) ϴ(yₜ₋₁, yₜ) }

(5)
```


其中，Z(X)是归一化因子，s(yₜ)表示位置t的标签得分，可以通过预测的对数向量 `sₜ ∈ ℝ|V|` 从公式（3）中获得，即sₜ(Vᵧᵗ)，其中Vᵧᵗ是词元yₜ的词汇索引。

值ϴ(yₜ₋₁, yₜ) = Mᵧᵗ₋₁,ᵧᵗ表示从词元yₜ₋₁到yₜ的转换得分，其中 `M ∈ ℝ|V|×|V|` 是转换矩阵，是进行依赖建模的核心项。

通常，M可以作为神经网络参数在端到端训练过程中学习。

然而，`|V|` 通常非常大，尤其是在文本生成场景中（超过32k），因此在实践中高效获取M和Z(X)是不可行的。

为了解决这个问题，正如(Sun等人，2019)中使用的方法，我们引入了两个低秩神经参数矩阵 `E₁，E₂ ∈ ℝ|V|×dₘ` 来逼近全秩转换矩阵M：

```
M = E₁E₂ᵀ  (6)
```

其中，`dₘ ≪ |V|`。

为了计算归一化因子Z(X)，原始的维特比算法（Forney，1973；Lafferty等人，2001）需要搜索所有路径。

为了提高效率，我们在每个时间步只访问约束的前k个节点（Sun等人，2019）。

## 2.5 使用焦点损失进行训练

考虑到任务CGEC的直接自下而上的信息传递特性，因此，直接预测任务和基于CRF的依赖建模任务可以在训练阶段共同纳入统一框架。

原因直观上是，直接预测将集中于每个位置的细粒度预测，而CRF层则更关注整个全局序列的高级质量。

我们采用最大似然估计（MLE）进行参数学习，并将负对数似然（NLL）作为损失函数。

因此，直接预测的优化目标Lᵈᵖ为：

```
Lᵈᵖ = −∑ₜ=₁ᵀ log Pᵈᵖ(yₜ | X)   (7)
```

而基于CRF的依赖建模的损失函数Lᶜʳᶠ为：

```
Lᶜʳᶠ = −log Pᶜʳᶠ(Y | X)   (8)
```

最终的优化目标为：

```
L = Lᵈᵖ + Lᶜʳᶠ   (9)
```

如第1节所述，CGEC的一个明显但至关重要的现象是，大多数句子中的词语是正确的，不需要更改。

考虑到最大似然估计是用于这两个任务的参数学习方法，那么简单的复制策略会导致损失函数的急剧下降。

直观地说，实际需要修正的语法错误词元，在训练过程中往往会吸引较少的关注。

实际上，这些词元应该被视为焦点，并且在优化目标中贡献更多。

然而，先前的工作没有在CGEC任务中深入探讨这个问题。

为了解决这个问题，我们在直接预测和CRF的损失函数中引入了一种有效的技巧——焦点损失（Lin等人，2020）：

```
Lᶠᴸᵈᵖ = −∑ₜ=₁ᵀ (1 − Pᵈᵖ(yₜ | X))ᵞ log Pᵈᵖ(yₜ | X)  
Lᶠᴸᶜʳᶠ = −(1 − Pᶜʳᶠ(Y | X))ᵞ log Pᶜʳᶠ(Y | X)  
(10)
```

其中，γ是控制惩罚权重的超参数。显然，Lᶠᴸᵈᵖ在词元级别进行惩罚，而Lᶠᴸᶜʳᶠ则在样本级别加权，并且将在批量训练条件下发挥作用。

带有焦点惩罚策略的最终优化目标为：

```
Lᶠᴸ = Lᶠᴸᵈᵖ + Lᶠᴸᶜʳᶠ    (11)
```

### 2.6 推理

在推理阶段，对于输入的源句子X，我们可以采用原始的|V|节点维特比算法来获得目标全局最优结果。

我们也可以利用截断的前k个维特比算法来提高计算效率（Sun等人，2019）。

### 数据集统计

| 语料库       | #训练  | #验证  | #测试  | 类型   |
|--------------|--------|--------|--------|--------|
| SIGHAN15     | 2,339  | -      | 1,100  | 固定长度 |
| HybirdSet    | 274,039| 3,162  | 3,162  | 固定长度 |
| TtTSet       | 539,268| 5,662  | 5,662  | 可变长度 |


# 3 实验设置

## 3.1 设置
我们提出的TtT模型的核心技术组件是Transformer（Vaswani等人，2017）和CRF（Lafferty等人，2001）。预训练的中文BERT-base模型（Devlin等人，2019）用于初始化模型。为了逼近CRF层中的转移矩阵，我们设置矩阵E1和E2的维度d为32。对于归一化因子Z(X)，我们预定义的束大小k设置为64。用于加权焦点惩罚项的超参数γ通过参数调优设置为0.5。训练批次大小为100，学习率为1e-5，dropout率为0.1。我们使用Adam优化器（Kingma和Ba，2015）进行参数学习。

## 3.2 数据集

我们在实验中使用的所有数据集的统计信息见表1。

**SIGHAN15 (Tseng等人，2015)**  
这是一个用于评估CGEC的基准数据集，包含2,339个训练样本和1,100个测试样本。与一些典型的前期工作（Wang等人，2019；Zhang等人，2020b）一样，我们也使用SIGHAN15测试集作为基准数据集，评估我们的模型及基线方法在固定长度（FixLen）错误修正设置下的表现。

**HybirdSet (Wang等人，2018)**  
这是一个新发布的数据集，基于ASR（Yu和Deng，2014）和OCR（Tong和Evans，1996）结果，构造了一个预先准备的混淆集。该数据集包含约270K个配对样本，也是一个FixLen数据集。

**TtTSet**  
考虑到SIGHAN15和HybirdSet数据集都是FixLen类型的数据集，为了展示我们的模型TtT在可变长度（VarLen）CGEC场景中的能力，基于HybirdSet语料库，我们构建了一个新的VarLen数据集。具体来说，我们对原始句子进行删除、插入和局部重排操作，以生成错误样本。每种操作覆盖三分之一的样本，最终得到约540K个样本。

## 3.3 比较方法

我们将TtT的表现与几个强大的基线方法在FixLen和VarLen设置下进行比较。

- **NTOU**：使用n-gram语言模型和重新排序策略进行预测（Tseng等人，2015）。
- **NCTU-NTUT**：也使用CRF进行标签依赖建模（Tseng等人，2015）。
- **HanSpeller++**：采用隐马尔可夫模型和重新排序策略进行预测（Zhang等人，2015）。
- **Hybrid**：使用基于LSTM的seq2seq框架进行生成（Wang等人，2018），而**Confusionset**则在seq2seq框架中引入了复制机制（Wang等人，2019）。
- **FASPell**：将BERT引入seq2seq框架中以获得更好的性能（Hong等人，2019）。
- **SoftMask-BERT**：首先使用基于GRU的模型进行错误检测，然后利用软掩码策略将预测结果与BERT模型结合（Zhang等人，2020b）。注意，SoftMask-BERT的最佳结果是在一个包含5亿配对样本的大规模数据集上进行预训练后获得的。
- **SpellGCN**：提出通过专用的图卷积网络将语音和视觉相似度知识引入语言模型中（Cheng等人，2020）。
- **Chunk**：提出了一种基于块的解码方法，结合全局优化来统一修正单字符和多字符的拼写错误（Bao等人，2020）。

我们还实现了一些经典方法用于比较和消融分析，特别是针对VarLen修正问题。

- **Transformer-s2s**：是基于Transformer的典型seq2seq框架，用于序列预测（Vaswani等人，2017）。
- **GPT2-finetune**：基于预训练的中文GPT2模型进行微调的序列生成框架（Radford等人，2019；Li，2020）。
- **BERT-finetune**：直接在CGEC语料库上微调中文BERT模型。

在Transformer-s2s和GPT2-finetune中，采用束搜索解码策略进行生成，束大小为5。

值得注意的是，上述某些原始方法（如SoftMask-BERT和BERT-finetune）只能在FixLen设置下工作。

### 模型检测和修正结果

#### 表2：在SIGHAN2015测试集（1100个样本）上的检测和修正结果

| 模型                    | 检测准确率 (ACC.) | 检测精确率 (PREC.) | 检测召回率 (REC.) | 检测F1值 (F1) | 修正准确率 (ACC.) | 修正精确率 (PREC.) | 修正召回率 (REC.) | 修正F1值 (F1) |
|-------------------------|-------------------|--------------------|-------------------|----------------|-------------------|--------------------|-------------------|----------------|
| NTOU (2015)              | 42.2              | 42.2               | 41.8              | 42.0           | 39.0              | 38.1               | 35.2              | 36.6           |
| NCTU-NTUT (2015)         | 60.1              | 71.7               | 33.6              | 45.7           | 56.4              | 66.3               | 26.1              | 37.5           |
| HanSpeller++ (2015)      | 70.1              | 80.3               | 53.3              | 64.0           | 69.2              | 79.7               | 51.5              | 62.5           |
| Hybird (2018)            | -                 | 56.6               | 69.4              | 62.3           | -                 | -                  | -                 | 57.1           |
| FASPell (2019)           | 74.2              | 67.6               | 60.0              | 63.5           | 73.7              | 66.6               | 59.1              | 62.6           |
| Confusionset (2019)      | -                 | 66.8               | 73.1              | 69.8           | -                 | 71.5               | 59.5              | 64.9           |
| SoftMask-BERT (2020b)    | 80.9              | 73.7               | 73.2              | 73.5           | 77.4              | 66.7               | 66.2              | 66.4           |
| Chunk (2020)             | 76.8              | 88.1               | 62.0              | 72.8           | 74.6              | 87.3               | 57.6              | 69.4           |
| SpellGCN (2020)          | -                 | 74.8               | 80.7              | 77.7           | -                 | 72.1               | 77.7              | 75.9           |
| Transformer-s2s (Sec.3.3) | 67.0              | 73.1               | 52.2              | 50.9           | 66.2              | 72.5               | 50.6              | 59.6           |
| GPT2-finetune (Sec.3.3)  | 65.1              | 70.0               | 51.9              | 59.4           | 64.6              | 69.1               | 50.7              | 58.5           |
| BERT-finetune (Sec.3.3)  | 75.4              | 84.1               | 61.5              | 71.1           | 71.6              | 82.2               | 53.9              | 65.1           |
| TtT (Sec.2)              | 82.7              | 85.4               | 78.1              | 81.6           | 81.5              | 85.0               | 75.6              | 80.0           |

#### 表3：在TtTSet测试集（5662个样本）上的检测和修正结果

| 模型                    | 检测准确率 (ACC.) | 检测精确率 (PREC.) | 检测召回率 (REC.) | 检测F1值 (F1) | 修正准确率 (ACC.) | 修正精确率 (PREC.) | 修正召回率 (REC.) | 修正F1值 (F1) |
|-------------------------|-------------------|--------------------|-------------------|----------------|-------------------|--------------------|-------------------|----------------|
| Transformer-s2s (Sec.3.3) | 25.6              | 65.6               | 16.1              | 25.9           | 24.6              | 63.6               | 14.8              | 24.0           |
| GPT2-finetune (Sec.3.3)  | 51.3              | 85.2               | 47.9              | 61.3           | 45.1              | 82.8               | 40.2              | 54.1           |
| BERT-finetune (Sec.3.3)  | 46.8              | 89.0               | 38.9              | 54.1           | 36.9              | 84.8               | 26.7              | 40.7           |
| TtT (Sec.2)              | 55.6              | 89.8               | 50.4              | 64.6           | 60.6              | 88.5               | 44.2              | 58.9           |

### 结果分析

- **检测结果**：TtT模型在SIGHAN2015和TtTSet测试集上的检测准确率分别为82.7%和55.6%，显示了其在检测任务中的出色表现。与其他基线方法相比，TtT在检测任务中领先。
  
- **修正结果**：TtT在修正任务中的表现也非常强大，尤其是在TtTSet测试集上，修正准确率为60.6%，精确率为88.5%，召回率为44.2%，F1值为58.9%。在SIGHAN2015测试集上，TtT也取得了81.5%的准确率，85.0%的精确率，75.6%的召回率和80.0%的F1值，表现出色。

与其他方法相比，TtT在多个评价指标上表现更优，尤其是在修正任务的准确性和精确性方面，显示了其强大的修正能力。

## 3.4 评估指标

为了与先前的研究（如Wang et al., 2019；Hong et al., 2019；Zhang et al., 2020b）保持一致，我们采用以下句子级评估指标来自动评估所有系统的表现：

- **准确率（Accuracy）**：衡量模型在所有预测中正确的比例。

- **精确率（Precision）**：衡量模型预测为正类的样本中有多少是真正的正类。

- **召回率（Recall）**：衡量模型识别出的所有正类样本占真实正类样本的比例。

- **F1值（F1-Measure）**：精确率和召回率的调和平均数，是对二者的综合考虑，常用来衡量模型的总体表现。

此外，我们还分别报告错误 **检测** 和 **修正** 的详细结果：

- **检测（Detection）**：模型在给定句子中错误字符的所有位置应与黄金标准完全一致。

- **修正（Correction）**：模型在给定句子中错误字符的所有位置及其对应修正应与黄金标准完全一致。

这些指标的选择有助于全面评估模型在检测和修正错误方面的能力，从而更加准确地反映出模型的实际表现。


# 4 结果与讨论

## 4.1 固定长度（FixLen）场景下的结果

表2展示了我们提出的TtT框架与比较基准方法的主要评估结果。

需要强调的是，**SoftMask-BERT**是在500M规模的配对数据集上预训练的，而我们的模型TtT以及其他基准方法（如Transformer-s2s、GPT2-finetune、BERT-finetune和Hybird）都是在270k规模的HybirdSet数据集上训练的。

尽管如此，TtT在错误检测（F1: 77.7 → 81.6）和修正（F1: 75.9 → 80.0）任务中相较于所有强基准方法都取得了提升，这表明我们提出的方案具有优势。

## 4.2 变长（VarLen）场景下的结果

得益于基于CRF的依赖建模组件，TtT可以联合进行删除、插入和局部改写操作，以解决变长错误修正问题。

表3描述了实验结果。考虑到像Transformer-s2s和GPT2-finetune这样的序列生成方法也能执行变长修正操作，因此我们也报告了它们的结果。从结果来看，TtT在变长场景下也能取得优越的性能。原因显而易见：BERT-finetune及相关方法在变长场景下不适用，尤其是当目标句子比输入长时。

像Transformer-s2s和GPT2-finetune这样的文本生成模型会出现**幻觉问题**（Maynez et al., 2020）和重复问题，这在CGEC问题上不稳定。

## 4.3 消融分析

- **不同训练数据集**：我们引入了不同规模的训练数据集（见表1），并且在这些数据集上进行了训练，最终在SIGHAN2015测试集上报告了结果。结果显示，无论数据集的规模如何，TtT始终表现出最佳的性能。

- **Ldp和Lcrf的影响**：表5展示了TtT模型与去除Ldp（TtT w/o Ldp）和Lcrf（TtT w/o Lcrf）后的变体的性能。我们得出结论，直接预测和基于CRF的依赖建模这两项任务的融合确实能够提高性能。

- **焦点损失的参数调优**：焦点损失惩罚的超参数γ对损失函数L = Ldp + Lcrf至关重要，应根据具体任务进行调整（Lin et al., 2020）。我们对γ ∈ (0, 0.1, 0.5, 1, 2, 5)进行了网格搜索，表6提供了相应的结果。最终，我们选择了γ = 0.5作为TtT在CGEC任务中的焦点损失超参数。

#### 4.4 计算效率分析

实际上，CGEC是一个重要且有用的任务，相关技术可以应用于写作助手、自动语音识别（ASR）和光学字符识别（OCR）的后处理、搜索引擎等多个实际应用中。

因此，模型的时间成本效率是一个关键点。表7展示了我们模型TtT与一些基准方法的单样本计算时间对比。

结果表明，TtT在预测性能上优于其他方法，并且计算时间复杂度较低，具有较高的成本效益，能够直接在线部署。

| Model               | Time (ms) | Speedup |
|---------------------|-----------|---------|
| Transformer-s2s      | 815.40    | 1x      |
| GPT2-finetune       | 552.82    | 1.47x   |
| TtT                 | 39.25     | 20.77x  |
| BERT-finetune       | 14.72     | 55.35x  |

### 讨论

1. **TtT的优势**：TtT不仅在FixLen和VarLen场景下表现出了较强的性能，还在计算效率上做出了优化，尤其在大规模数据处理时，能够有效地减少时间成本。它结合了Transformer和CRF模型的优势，在语法纠错任务中展示了卓越的能力。

2. **变长修正的挑战**：尽管现有方法如BERT-finetune、Transformer-s2s和GPT2-finetune能够处理变长问题，但它们在面对输入和目标长度不一致时经常遇到生成错误（如幻觉和重复）。而TtT通过CRF组件有效缓解了这一问题，表现更为稳健。

3. **焦点损失的作用**：焦点损失惩罚项的引入有助于增强对难修正错误的关注，优化了模型在错误检测和修正任务中的表现。

### 表4：在不同数据集上训练的模型性能

| TrainSet    | Model             | Detection ACC. | Detection PREC. | Detection REC. | Detection F1 | Correction ACC. | Correction PREC. | Correction REC. | Correction F1 |
|-------------|-------------------|----------------|-----------------|----------------|---------------|-----------------|------------------|-----------------|---------------|
| SIGHAN15    | Transformer-s2s    | 46.5           | 42.2            | 23.6           | 30.3          | 43.4            | 34.9             | 17.3            | 23.2          |
|             | GPT2-finetune      | 45.2           | 42.3            | 30.8           | 35.7          | 42.6            | 37.7             | 25.5            | 30.4          |
|             | BERT-finetune      | 35.8           | 34.1            | 32.8           | 33.4          | 31.3            | 27.1             | 23.6            | 25.3          |
|             | TtT                | 51.3           | 50.6            | 38.0           | 43.4          | 45.8            | 41.9             | 26.7            | 32.7          |
| HybirdSet   | Transformer-s2s    | 67.0           | 73.1            | 52.2           | 50.9          | 66.2            | 72.5             | 50.6            | 59.6          |
|             | GPT2-finetune      | 65.1           | 70.0            | 51.9           | 59.4          | 64.6            | 69.1             | 50.7            | 58.5          |
|             | BERT-finetune      | 75.4           | 84.1            | 61.5           | 71.1          | 71.6            | 82.2             | 53.9            | 65.1          |
|             | TtT                | 82.7           | 85.4            | 78.1           | 81.6          | 81.5            | 85.0             | 75.6            | 80.0          |

### 表5：Ldp和Lcrf的消融分析

| TrainSet    | Model             | Detection ACC. | Detection PREC. | Detection REC. | Detection F1 | Correction ACC. | Correction PREC. | Correction REC. | Correction F1 |
|-------------|-------------------|----------------|-----------------|----------------|---------------|-----------------|------------------|-----------------|---------------|
| SIGHAN15    | TtT w/o Lcrf       | 35.8           | 34.1            | 32.8           | 33.4          | 31.3            | 27.1             | 23.6            | 25.3          |
|             | TtT w/o Ldp        | 35.5           | 32.0            | 28.0           | 29.9          | 31.2            | 24.9             | 19.3            | 21.6          |
|             | TtT                | 42.6           | 39.4            | 31.5           | 35.0          | 36.7            | 28.9             | 23.6            | 26.0          |
| HybirdSet   | TtT w/o Lcrf       | 75.4           | 84.1            | 61.5           | 71.1          | 71.6            | 82.2             | 53.9            | 65.1          |
|             | TtT w/o Ldp        | 81.2           | 83.4            | 77.1           | 80.1          | 80.0            | 83.0             | 74.7            | 78.6          |
|             | TtT                | 82.7           | 85.6            | 77.9           | 81.5          | 81.1            | 85.0             | 74.7            | 79.5          |

### 表6：焦点损失超参数γ的调优

| TrainSet    | γ    | Detection ACC. | Detection PREC. | Detection REC. | Detection F1 | Correction ACC. | Correction PREC. | Correction REC. | Correction F1 |
|-------------|------|----------------|-----------------|----------------|---------------|-----------------|------------------|-----------------|---------------|
| SIGHAN15    | 0.0  | 42.6           | 39.4            | 31.5           | 35.0          | 36.7            | 28.9             | 23.6            | 26.0          |
|             | 0.1  | 48.8           | 47.0            | 35.5           | 40.3          | 43.8            | 38.7             | 25.1            | 30.4          |
|             | 0.5  | 51.3           | 50.6            | 38.0           | 43.4          | 45.8            | 41.9             | 26.7            | 32.6          |
|             | 1.0  | 51.8           | 51.3            | 37.7           | 43.5          | 46.3            | 42.5             | 26.5            | 32.6          |
|             | 2.0  | 50.0           | 48.6            | 36.3           | 41.5          | 44.4            | 39.5             | 25.0            | 30.6          |
|             | 5.0  | 48.9           | 47.1            | 37.2           | 47.6          | 42.8            | 37.6             | 25.1            | 30.6          |
| HybirdSet   | 0.0  | 82.7           | 85.6            | 77.9           | 81.5          | 81.1            | 85.0             | 74.7            | 79.5          |
|             | 0.1  | 74.6           | 73.5            | 75.4           | 74.4          | 73.2            | 72.7             | 72.6            | 72.7          |
|             | 0.5  | 82.7           | 85.4            | 78.0           | 81.6          | 81.5            | 85.0             | 75.6            | 80.0          |
|             | 1.0  | 81.1           | 83.2            | 77.1           | 80.0          | 80.0            | 82.8             | 74.9            | 78.6          |
|             | 2.0  | 79.2           | 80.4            | 76.2           | 78.2          | 78.2            | 80.0             | 74.1            | 76.9          |
|             | 5.0  | 80.3           | 81.6            | 77.3           | 79.4          | 78.7            | 80.9             | 74.1            | 77.4          |

1. **数据集规模的影响**：
   - 对比SIGHAN15和HybirdSet数据集的结果可以看出，较大的数据集（如HybirdSet）显著提高了模型的表现，尤其在错误检测和修正任务中的F1值上。TtT在HybirdSet上显示出最佳的性能，表明该模型具有强大的泛化能力。

2. **Ldp和Lcrf的作用**：
   - 消融实验（表5）表明，去除Lcrf或Ldp会显著降低性能，尤其是在错误检测和修正任务中的F1值。Ldp和Lcrf的组合显然对模型性能起到了关键作用，表明这两种任务在复杂的错误检测与修正任务中是不可或缺的。

3. **焦点损失调优的影响**：
   - 焦点损失的超参数γ在不同的设置下影响了模型的性能（表6）。选择适当的γ值能够显著提高模型的准确性和F1值，特别是在SIGHAN15数据集上，γ = 0.5时获得了最佳的性能。

4. **性能与计算效率的平衡**：
   - TtT在性能上相较于其他方法（如Transformer-s2s、GPT2-finetune）展示了更高的准确性和F1值，尤其是在HybirdSet数据集上。尽管其计算开销可能较高，但对于提高错误检测与修正的精度，使用TtT仍是较为有效的选择。

# 5 结论

我们提出了一种新的框架，称为**尾对尾非自回归序列预测**（tail-to-tail non-autoregressive sequence prediction，简称TtT），用于CGEC（Chinese Grammatical Error Correction）问题。

我们引入了基于BERT的序列编码器，以进行双向表示学习。

为了同时进行替换、删除、插入和局部释义操作，我们在模型的上尾部分堆叠了一个CRF（条件随机场）层，通过建模相邻词元之间的依赖关系，进行非自回归序列预测。

此外，我们还引入了低秩分解和截断Viterbi算法来加速计算过程。为了缓解类不平衡问题（因为句子中大多数词元并没有发生变化），我们采用了焦点损失惩罚策略。 

在标准数据集上的实验结果表明，TtT在错误检测和修正任务中的句子级准确率、精确度、召回率和F1值方面表现出色。TtT的计算复杂度较低，且能够直接在线部署。

未来，我们计划引入更多的词汇分析知识，如分词和细粒度命名实体识别（Zhang et al., 2020a），以进一步提升模型性能。

# 小结

希望本文对你有所帮助，如果喜欢，欢迎点赞收藏转发一波。

我是老马，期待与你的下次相遇。

# 参考资料

https://blog.csdn.net/qq_36426650/article/details/122796348

* any list
{:toc}
