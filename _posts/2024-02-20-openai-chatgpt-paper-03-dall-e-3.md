---
layout: post
title: openai chatgpt paper-01-openai DALL-E 3 论文 Improving Image Generation with Better Captions  提升图像生成的关键：更好的图像描述
date: 2024-02-20 21:01:55 +0800
categories: [AI]
tags: [ai, paper, sh]
published: true
---


# 摘要

我们展示了通过训练高度描述性的生成图像标题，可以显着改善文本到图像模型的提示跟随能力。

现有的文本到图像模型在跟随详细的图像描述方面存在困难，经常忽略单词或混淆提示的含义。

我们假设这个问题源于训练数据集中存在嘈杂和不准确的图像标题。我们通过训练定制的图像标题生成器来解决这个问题，并使用它重新为训练数据集生成标题。然后我们训练了几个文本到图像模型，并发现在这些合成标题上进行训练可靠地提高了提示跟随能力。

最后，我们利用这些发现构建了 DALL-E 3：一个新的文本到图像生成系统，并对其性能进行了基准测试，评估设计用于衡量提示跟随、连贯性和美感，发现它与竞争对手相比具有明显优势。我们发布了这些评估的样本和代码，以便未来的研究可以继续优化文本到图像系统的这一重要方面。

# 1 引言

近年来生成建模的进展使得文本到图像生成模型实现了显著的性能提升。

特别是，通过采用基于采样的方法，如自回归生成建模[27, 2, 1, 20, 30]或使用扩散过程[25, 6, 11, 12, 19, 22]来解决问题，使我们能够将图像生成问题分解为小的、离散的步骤，这些步骤更容易被神经网络学习。

与此同时，研究人员还找到了利用自注意力层堆叠构建图像生成器的方法[15, 3, 4]。

通过将图像生成与卷积的隐含空间偏差分离开来，使文本到图像模型能够通过变压器的良好研究的缩放属性可靠地改进。

结合足够大的数据集，这些方法使得可以训练出大型的文本到图像模型，这些模型能够生成接近人类可以产生的照片和艺术品质量的图像。

该领域面临的一个重要挑战是图像生成系统的可控性，这些系统经常忽略给定标题中的单词、单词顺序或含义。我们用术语“提示跟随”来指代这些挑战。

在几项研究中已经指出了这个问题：Rassin等人（2022年）指出DALL-E 2没有强制要求每个单词只有一个含义。Saharia等人（2022年）提出通过对预训练语言模型进行条件化来改进它，并引入了一个名为Drawbench的评估，揭示了常见的提示跟随问题。与此同时，Yu等人（2022b年）引入了他们自己的基准测试Parti Prompts，并表明扩展自回归图像生成器是改进提示跟随的另一种替代方法。

在这项工作中，我们提出了一种解决提示跟随的新方法：标题改进。我们假设现有文本到图像模型的一个根本问题是它们所训练的数据集中文本和图像配对的质量较差，这个问题在其他作品中也被指出，比如Jia等人（2021年）。我们提出通过为数据集中的图像生成改进的标题来解决这个问题。我们首先学习一个强大的图像标题生成器，它能够产生图像的详细准确的描述。然后，我们将这个标题生成器应用于我们的数据集，以产生更详细的标题。最后，我们在改进后的数据集上训练文本到图像模型。

在合成数据上进行训练并不是一个新概念。例如，Yu等人（2022b年）提到他们在训练扩展自回归图像生成器时应用了这种技术。我们的贡献在于构建了一种新颖的、描述性的图像标题生成系统，并测量了在训练生成模型时使用合成标题的影响。我们还建立了一套可重现的基准性能档案，用于衡量提示跟随的一系列评估。

本文重点评估了DALL-E 3在训练高度描述性的生成标题时改进的提示跟随能力。它不涵盖DALL-E 3模型的训练或实现细节。我们在第2节中提供了一个训练图像标题生成器的高级概述，第3节评估了在原始标题与生成标题上训练的文本到图像模型，第4节评估了DALL-E 3，第5节讨论了限制和风险。

# 2 数据集重新生成标题

我们的文本到图像模型是在一个由大量配对 (t, i) 组成的数据集上进行训练的，其中 i 是一幅图像，t 是描述该图像的文本。

在大规模数据集中，t 通常是由人类作者衍生出来的，他们专注于简单描述图像的主题，省略了背景细节或图像中描绘的常识关系。通常从 t 中省略的重要细节可能包括：

1. 厨房中存在的水槽或人行道上的停车标志等对象以及这些对象的描述。
2. 场景中对象的位置以及这些对象的数量。
3. 场景中对象的颜色和大小等常识细节。
4. 图像中显示的文字。

更糟糕的是，在互联网上找到的标题往往是错误的；它们描述了图像的相关细节。例如，常见的情况是在用于生成图像标题的 alt-text 中发现广告或迷因。

我们推测所有这些缺陷都可以通过合成生成的标题来解决。在接下来的部分中，我们将讨论我们开发的测试这一理论的程序。

## 2.1 构建图像标题生成器

图像标题生成器与传统的语言模型非常相似，它预测文本。因此，我们首先提供对语言模型的简要描述。首先，使用分词器将文本字符串分解为离散的标记。一旦以这种方式分解，我们语料库中的文本部分可以表示为一个序列，t = [t1, t2, . . . , tn]。然后，我们可以通过最大化以下似然函数来构建一个语言模型：

L(t) = Σlog P(tj |tj−k, . . . , tj−1; Θ) (1)

其中 Θ 是要优化的标题生成器的参数。要将这个语言模型转换为一个标题生成器，你只需要对图像进行条件化。这里的挑战在于图像由成千上万个像素值组成。在我们当前的神经网络中，对所有这些信息进行条件化是非常低效的，因此我们需要一个压缩的表示空间。方便的是，CLIP[17]提供了这样的表示。因此，给定一个预训练的 CLIP 图像嵌入函数 F(i)，我们将我们的语言模型目标扩展如下：

L(t, i) = Σlog P(tj |tj−k, . . . , tj−1; zj ; F(i); Θ) (2)

我们遵循 Yu 等人 (2022a) 的方法，并使用上述公式在我们的 (t, i) 文本和图像对数据集上联合预训练我们的标题生成器与 CLIP 和语言建模目标。

得到的模型确实是一个良好的标题生成器，但表现出与我们在第2节描述的相同的问题，比如不愿描述细节。

### 2.1.1 微调图像标题生成器

为了改进我们图像生成数据集中的标题，我们希望偏向我们的标题生成器产生对学习文本到图像模型有用的图像描述。

在我们的第一次尝试中，我们构建了一个小数据集，其中的标题只描述图像的主要主题。然后，我们继续在这个数据集上训练我们的标题生成器。由此过程引起的 θ 的更新会导致一个偏向于描述图像主要主题的模型。

我们将这种微调生成的标题称为“简短的合成标题”。

我们再次重复这个过程，第二次创建一个数据集，其中包含描述我们微调数据集中每个图像内容的长、高度描述性的标题。这些标题不仅描述图像的主要主题，还描述了其周围环境、背景、图像中的文字、风格、着色等。然后，我们再次在这个数据集上微调我们的基础标题生成器。我们将由这个标题生成器生成的标题称为“描述性合成标题”。

图3显示了地面真实、简短合成和描述性合成标题的示例。

构建完成后，我们将应用我们的图像标题生成器微调到我们文本到图像数据集中的每个图像，从而得到一组合成标题，我们将用于后续实验。

# 3 评估重新生成的数据集

有了我们重新生成的数据集，我们开始评估在合成文本上训练模型的影响。

我们特别想回答两个问题：

1. 使用每种类型的合成标题的性能影响。

2. 合成标题与地面真实标题的最佳混合比例。

## 3.1 混合合成和地面真实标题（Blending synthetic and ground-truth captions）

像我们的文本到图像扩散模型这样的似然模型有一种臭名昭著的倾向，即过度拟合数据集中的分布规律。

例如，一个训练过的文本到图像模型，如果训练的文本总是以空格字符开头，那么如果尝试使用不以空格开头的提示进行推理，它就无法正常工作。

当涉及到在合成标题上训练时，我们需要考虑这个问题。我们的标题生成器模型可能有许多难以检测到的模态行为，但如果训练了这些标题，它们就会成为我们的文本到图像模型的偏差。可能发生这种情况的示例包括字母大小写，标题中是否出现标点符号（例如，它是否总是以句号结尾？），标题的长度，或者风格倾向，比如所有标题都以单词 "a" 或 "an" 开头。

克服这个问题的最佳方法是将我们的输入规范化为更接近人类可能使用的样式和格式的文本分布。

当使用地面真实标题时，你可以免费获得这个，因为这些标题实际上是从人类书写的文本分布中抽取出来的。为了在使用合成标题时在我们的模型训练中引入一些这种规范化，我们选择将合成标题与地面真实标题混合。

混合发生在数据采样时，我们随机选择地面真实标题或合成标题，并以固定的百分比机会。我们将在下一节中分析不同混合比例的性能影响。

## 3.2 评估方法

为了评估，我们在相同的图像数据集上训练了相同的 T5-条件化图像扩散模型。关于训练的模型的详细信息在附录 A 中描述。所有模型都训练了 500,000 个训练步骤，批量大小为 2048，对应总共 10 亿张训练图像。

训练完成后，我们使用评估数据集中的标题生成了每个模型的 50,000 张图像。然后，我们使用 Hessel 等人（2022年）中概述的 CLIP-S 评估度量对这些生成的图像进行评估。我们选择 CLIP 分数作为度量标准，因为它与文本-图像相似性有很强的相关性，这正是我们所追求的。作为一个快速回顾，这个度量标准的计算如下：

首先，我们使用公共的 CLIP ViT-B/32 图像编码器生成图像嵌入 zi，然后我们使用文本编码器为图像标题创建文本嵌入 zt。最后，我们计算 CLIP 分数作为余弦相似度 C：

C(zi, zt) = zi · zt / (∥zi∥∥zt∥) (3)

然后，该度量标准对所有 50,000 个文本/图像对计算得到的距离进行平均，并通过一个因子缩放为 100。我们在训练期间跨多个模型检查点执行此评估，始终使用模型学习权重的指数加权平均值进行评估。

在计算 CLIP 分数时，选择在上述计算中使用哪种标题是重要的。对于我们的测试，我们要么使用地面真实标题，要么使用描述性合成标题。在每个评估中都会注明使用了哪种类型的标题。

## 3.3 标题类型的结果

我们首先分析了在不同类型的标题上训练的模型之间的性能差异。

对于这个评估，我们训练了三个模型：

1. 只在地面真实标题上训练的文本到图像模型。
2. 在 95% 的简短合成标题上训练的文本到图像模型。
3. 在 95% 的描述性合成标题上训练的文本到图像模型。

我们进行了两次这样的评估：一次使用地面真实标题计算的 zt，一次使用描述性合成标题计算的 zt。在这次评估中，我们不使用简短合成标题，因为它们在这个评估中与地面真实标题非常相似。

结果显示，在使用地面真实标题进行评估时，两个在合成标题上训练的模型的性能略优于基线模型，而在使用描述性合成标题进行评估时，它们的性能明显更好。这表明在训练文本到图像模型时使用合成标题没有任何不利之处。

有趣的是，合成标题上的评估曲线的方差要低得多。这加强了我们的理论，即重新生成标题可以看作是一种平均操作。在合成标题上评估的图像生成模型也在所有训练的模型中实现了更高的净 CLIP 分数，这支持了合成标题与其对应图像之间具有更好绑定的观点。

## 3.4 标题混合比例

为了评估标题混合比例，我们使用我们的描述性合成标题训练了四个图像生成模型，采用了不同的混合比例。

我们选择了 65%、80%、90% 和 95% 的合成标题混合。在实验进行到中途时，评估显示，65% 的混合在所有评估中远远落后于其他混合，因此我们将其排除。

## 3.5 高度描述性标题的实际用途

上述实验表明，通过训练大部分合成标题，我们可以最大限度地提高模型的性能。

然而，这样做会导致模型自然地适应由我们的标题生成器生成的长、高度描述性标题的分布。

众所周知，生成模型在从其训练分布中抽样时会产生较差的结果。因此，为了充分发挥我们模型的潜力，我们需要仅使用高度描述性标题从中抽样。幸运的是，最近大型语言模型的突破使得这个问题可以解决。像 GPT-4 这样的模型已经在需要想象力的任务中表现出色，比如讲故事和写诗。可以推断，它们也可能擅长提出图像描述中的合理细节。

实际上，给定一个类似于附录 C 中找到的提示，我们发现 GPT-4 可以轻松地将任何标题“上采样”为一个高度描述性的标题。为了演示这种方法可能有用的方式，我们对 drawbench 数据集中的标题执行此过程，并在表7中可视化结果。

正如图7所示，利用大型语言模型对标题进行“上采样”不仅可以添加缺失的细节，还可以消除相对较小的图像生成模型难以学习的复杂关系的歧义。最终结果是，模型通常会正确地呈现它本来会出错的图像。

# 4 DALL-E 3

为了大规模测试我们的合成标题，我们训练了 DALL-E 3，一个新的最先进的文本到图像生成器。

为了训练这个模型，我们使用了 95% 的合成标题和 5% 的地面真实标题的混合。

该模型本身是上述消融实验中使用的模型的放大版本，并进行了几项其他改进。

| Metric            | DALL-E 3 | DALL-E 2 | Stable Diffusion XL2 |
|-------------------|----------|----------|-----------------------|
| MSCOCO Captions   | CLIP Score ↑  | 32.0     | 31.4     | 30.5                  |
| Drawbench short (GPT-V) 3  | ↑  | 70.4%    | 49.0%    | 46.9%                 |
| Drawbench long (GPT-V)   | ↑  | 81.0%    | 52.4%    | 51.1%                 |
| T2I-C B-VQA Colors   | ↑  | 81.1%    | 59.2%    | 61.9%                 |
| T2I-C B-VQA Shape   | ↑  | 67.5%    | 54.7%    | 61.9%                 |
| T2I-C B-VQA Texture  | ↑  | 80.7%    | 63.7%    | 55.2%                 |


表1 - 各种与提示跟随相关的文本到图像模型的比较评估

1 DALL-E 2 生产版本的图像于 2023 年 9 月 20 日发布。

2 稳定扩散 XL v1.0 使用了 refiner 模块。

3 这里的分数是根据 GPT-V 判断为具有“正确”标题的图像百分比。




# 参考资料

https://cdn.openai.com/papers/dall-e-3.pdf

* any list
{:toc}
