---
layout: post
title: openai chatgpt paper-01-openai DALL-E 3 论文 Improving Image Generation with Better Captions  提升图像生成的关键：更好的图像描述
date: 2024-02-20 21:01:55 +0800
categories: [AI]
tags: [ai, paper, sh]
published: true
---

# 摘要

我们展示了通过训练高度描述性的生成图像标题，可以显着改善文本到图像模型的提示跟随能力。

现有的文本到图像模型在跟随详细的图像描述方面存在困难，经常忽略单词或混淆提示的含义。

我们假设这个问题源于训练数据集中存在嘈杂和不准确的图像标题。我们通过训练定制的图像标题生成器来解决这个问题，并使用它重新为训练数据集生成标题。然后我们训练了几个文本到图像模型，并发现在这些合成标题上进行训练可靠地提高了提示跟随能力。

最后，我们利用这些发现构建了 DALL-E 3：一个新的文本到图像生成系统，并对其性能进行了基准测试，评估设计用于衡量提示跟随、连贯性和美感，发现它与竞争对手相比具有明显优势。我们发布了这些评估的样本和代码，以便未来的研究可以继续优化文本到图像系统的这一重要方面。

![view](https://images.openai.com/blob/54facbbb-c94c-4884-8c94-5b984b19749c/dalle-image-map.png?trim=0,0,0,0&width=2000)

# 1 引言

近年来生成建模的进展使得文本到图像生成模型实现了显著的性能提升。

特别是，通过采用基于采样的方法，如自回归生成建模[27, 2, 1, 20, 30]或使用扩散过程[25, 6, 11, 12, 19, 22]来解决问题，使我们能够将图像生成问题分解为小的、离散的步骤，这些步骤更容易被神经网络学习。

与此同时，研究人员还找到了利用自注意力层堆叠构建图像生成器的方法[15, 3, 4]。

通过将图像生成与卷积的隐含空间偏差分离开来，使文本到图像模型能够通过变压器的良好研究的缩放属性可靠地改进。

结合足够大的数据集，这些方法使得可以训练出大型的文本到图像模型，这些模型能够生成接近人类可以产生的照片和艺术品质量的图像。

该领域面临的一个重要挑战是图像生成系统的可控性，这些系统经常忽略给定标题中的单词、单词顺序或含义。我们用术语“提示跟随”来指代这些挑战。

在几项研究中已经指出了这个问题：Rassin等人（2022年）指出DALL-E 2没有强制要求每个单词只有一个含义。Saharia等人（2022年）提出通过对预训练语言模型进行条件化来改进它，并引入了一个名为Drawbench的评估，揭示了常见的提示跟随问题。与此同时，Yu等人（2022b年）引入了他们自己的基准测试Parti Prompts，并表明扩展自回归图像生成器是改进提示跟随的另一种替代方法。

在这项工作中，我们提出了一种解决提示跟随的新方法：标题改进。我们假设现有文本到图像模型的一个根本问题是它们所训练的数据集中文本和图像配对的质量较差，这个问题在其他作品中也被指出，比如Jia等人（2021年）。我们提出通过为数据集中的图像生成改进的标题来解决这个问题。我们首先学习一个强大的图像标题生成器，它能够产生图像的详细准确的描述。然后，我们将这个标题生成器应用于我们的数据集，以产生更详细的标题。最后，我们在改进后的数据集上训练文本到图像模型。

在合成数据上进行训练并不是一个新概念。例如，Yu等人（2022b年）提到他们在训练扩展自回归图像生成器时应用了这种技术。我们的贡献在于构建了一种新颖的、描述性的图像标题生成系统，并测量了在训练生成模型时使用合成标题的影响。我们还建立了一套可重现的基准性能档案，用于衡量提示跟随的一系列评估。

本文重点评估了DALL-E 3在训练高度描述性的生成标题时改进的提示跟随能力。它不涵盖DALL-E 3模型的训练或实现细节。我们在第2节中提供了一个训练图像标题生成器的高级概述，第3节评估了在原始标题与生成标题上训练的文本到图像模型，第4节评估了DALL-E 3，第5节讨论了限制和风险。

# 2 数据集重新生成标题

我们的文本到图像模型是在一个由大量配对 (t, i) 组成的数据集上进行训练的，其中 i 是一幅图像，t 是描述该图像的文本。

在大规模数据集中，t 通常是由人类作者衍生出来的，他们专注于简单描述图像的主题，省略了背景细节或图像中描绘的常识关系。通常从 t 中省略的重要细节可能包括：

1. 厨房中存在的水槽或人行道上的停车标志等对象以及这些对象的描述。
2. 场景中对象的位置以及这些对象的数量。
3. 场景中对象的颜色和大小等常识细节。
4. 图像中显示的文字。

更糟糕的是，在互联网上找到的标题往往是错误的；它们描述了图像的相关细节。例如，常见的情况是在用于生成图像标题的 alt-text 中发现广告或迷因。

我们推测所有这些缺陷都可以通过合成生成的标题来解决。在接下来的部分中，我们将讨论我们开发的测试这一理论的程序。

## 2.1 构建图像标题生成器

图像标题生成器与传统的语言模型非常相似，它预测文本。因此，我们首先提供对语言模型的简要描述。首先，使用分词器将文本字符串分解为离散的标记。一旦以这种方式分解，我们语料库中的文本部分可以表示为一个序列，t = [t1, t2, . . . , tn]。然后，我们可以通过最大化以下似然函数来构建一个语言模型：

L(t) = Σlog P(tj |tj−k, . . . , tj−1; Θ) (1)

其中 Θ 是要优化的标题生成器的参数。要将这个语言模型转换为一个标题生成器，你只需要对图像进行条件化。这里的挑战在于图像由成千上万个像素值组成。在我们当前的神经网络中，对所有这些信息进行条件化是非常低效的，因此我们需要一个压缩的表示空间。方便的是，CLIP[17]提供了这样的表示。因此，给定一个预训练的 CLIP 图像嵌入函数 F(i)，我们将我们的语言模型目标扩展如下：

L(t, i) = Σlog P(tj |tj−k, . . . , tj−1; zj ; F(i); Θ) (2)

我们遵循 Yu 等人 (2022a) 的方法，并使用上述公式在我们的 (t, i) 文本和图像对数据集上联合预训练我们的标题生成器与 CLIP 和语言建模目标。

得到的模型确实是一个良好的标题生成器，但表现出与我们在第2节描述的相同的问题，比如不愿描述细节。

### 2.1.1 微调图像标题生成器

为了改进我们图像生成数据集中的标题，我们希望偏向我们的标题生成器产生对学习文本到图像模型有用的图像描述。

在我们的第一次尝试中，我们构建了一个小数据集，其中的标题只描述图像的主要主题。然后，我们继续在这个数据集上训练我们的标题生成器。由此过程引起的 θ 的更新会导致一个偏向于描述图像主要主题的模型。

我们将这种微调生成的标题称为“简短的合成标题”。

我们再次重复这个过程，第二次创建一个数据集，其中包含描述我们微调数据集中每个图像内容的长、高度描述性的标题。这些标题不仅描述图像的主要主题，还描述了其周围环境、背景、图像中的文字、风格、着色等。然后，我们再次在这个数据集上微调我们的基础标题生成器。我们将由这个标题生成器生成的标题称为“描述性合成标题”。

图3显示了地面真实、简短合成和描述性合成标题的示例。

构建完成后，我们将应用我们的图像标题生成器微调到我们文本到图像数据集中的每个图像，从而得到一组合成标题，我们将用于后续实验。

# 3 评估重新生成的数据集

有了我们重新生成的数据集，我们开始评估在合成文本上训练模型的影响。

我们特别想回答两个问题：

1. 使用每种类型的合成标题的性能影响。

2. 合成标题与地面真实标题的最佳混合比例。

## 3.1 混合合成和地面真实标题（Blending synthetic and ground-truth captions）

像我们的文本到图像扩散模型这样的似然模型有一种臭名昭著的倾向，即过度拟合数据集中的分布规律。

例如，一个训练过的文本到图像模型，如果训练的文本总是以空格字符开头，那么如果尝试使用不以空格开头的提示进行推理，它就无法正常工作。

当涉及到在合成标题上训练时，我们需要考虑这个问题。我们的标题生成器模型可能有许多难以检测到的模态行为，但如果训练了这些标题，它们就会成为我们的文本到图像模型的偏差。可能发生这种情况的示例包括字母大小写，标题中是否出现标点符号（例如，它是否总是以句号结尾？），标题的长度，或者风格倾向，比如所有标题都以单词 "a" 或 "an" 开头。

克服这个问题的最佳方法是将我们的输入规范化为更接近人类可能使用的样式和格式的文本分布。

当使用地面真实标题时，你可以免费获得这个，因为这些标题实际上是从人类书写的文本分布中抽取出来的。为了在使用合成标题时在我们的模型训练中引入一些这种规范化，我们选择将合成标题与地面真实标题混合。

混合发生在数据采样时，我们随机选择地面真实标题或合成标题，并以固定的百分比机会。我们将在下一节中分析不同混合比例的性能影响。

## 3.2 评估方法

为了评估，我们在相同的图像数据集上训练了相同的 T5-条件化图像扩散模型。关于训练的模型的详细信息在附录 A 中描述。所有模型都训练了 500,000 个训练步骤，批量大小为 2048，对应总共 10 亿张训练图像。

训练完成后，我们使用评估数据集中的标题生成了每个模型的 50,000 张图像。然后，我们使用 Hessel 等人（2022年）中概述的 CLIP-S 评估度量对这些生成的图像进行评估。我们选择 CLIP 分数作为度量标准，因为它与文本-图像相似性有很强的相关性，这正是我们所追求的。作为一个快速回顾，这个度量标准的计算如下：

首先，我们使用公共的 CLIP ViT-B/32 图像编码器生成图像嵌入 zi，然后我们使用文本编码器为图像标题创建文本嵌入 zt。最后，我们计算 CLIP 分数作为余弦相似度 C：

C(zi, zt) = zi · zt / (∥zi∥∥zt∥) (3)

然后，该度量标准对所有 50,000 个文本/图像对计算得到的距离进行平均，并通过一个因子缩放为 100。我们在训练期间跨多个模型检查点执行此评估，始终使用模型学习权重的指数加权平均值进行评估。

在计算 CLIP 分数时，选择在上述计算中使用哪种标题是重要的。对于我们的测试，我们要么使用地面真实标题，要么使用描述性合成标题。在每个评估中都会注明使用了哪种类型的标题。

## 3.3 标题类型的结果

我们首先分析了在不同类型的标题上训练的模型之间的性能差异。

对于这个评估，我们训练了三个模型：

1. 只在地面真实标题上训练的文本到图像模型。
2. 在 95% 的简短合成标题上训练的文本到图像模型。
3. 在 95% 的描述性合成标题上训练的文本到图像模型。

我们进行了两次这样的评估：一次使用地面真实标题计算的 zt，一次使用描述性合成标题计算的 zt。在这次评估中，我们不使用简短合成标题，因为它们在这个评估中与地面真实标题非常相似。

结果显示，在使用地面真实标题进行评估时，两个在合成标题上训练的模型的性能略优于基线模型，而在使用描述性合成标题进行评估时，它们的性能明显更好。这表明在训练文本到图像模型时使用合成标题没有任何不利之处。

有趣的是，合成标题上的评估曲线的方差要低得多。这加强了我们的理论，即重新生成标题可以看作是一种平均操作。在合成标题上评估的图像生成模型也在所有训练的模型中实现了更高的净 CLIP 分数，这支持了合成标题与其对应图像之间具有更好绑定的观点。

## 3.4 标题混合比例

为了评估标题混合比例，我们使用我们的描述性合成标题训练了四个图像生成模型，采用了不同的混合比例。

我们选择了 65%、80%、90% 和 95% 的合成标题混合。在实验进行到中途时，评估显示，65% 的混合在所有评估中远远落后于其他混合，因此我们将其排除。

## 3.5 高度描述性标题的实际用途

上述实验表明，通过训练大部分合成标题，我们可以最大限度地提高模型的性能。

然而，这样做会导致模型自然地适应由我们的标题生成器生成的长、高度描述性标题的分布。

众所周知，生成模型在从其训练分布中抽样时会产生较差的结果。因此，为了充分发挥我们模型的潜力，我们需要仅使用高度描述性标题从中抽样。幸运的是，最近大型语言模型的突破使得这个问题可以解决。像 GPT-4 这样的模型已经在需要想象力的任务中表现出色，比如讲故事和写诗。可以推断，它们也可能擅长提出图像描述中的合理细节。

实际上，给定一个类似于附录 C 中找到的提示，我们发现 GPT-4 可以轻松地将任何标题“上采样”为一个高度描述性的标题。为了演示这种方法可能有用的方式，我们对 drawbench 数据集中的标题执行此过程，并在表7中可视化结果。

正如图7所示，利用大型语言模型对标题进行“上采样”不仅可以添加缺失的细节，还可以消除相对较小的图像生成模型难以学习的复杂关系的歧义。最终结果是，模型通常会正确地呈现它本来会出错的图像。

# 4 DALL-E 3

为了大规模测试我们的合成标题，我们训练了 DALL-E 3，一个新的最先进的文本到图像生成器。

为了训练这个模型，我们使用了 95% 的合成标题和 5% 的地面真实标题的混合。

该模型本身是上述消融实验中使用的模型的放大版本，并进行了几项其他改进。

| Metric            | DALL-E 3 | DALL-E 2 | Stable Diffusion XL2 |
|-------------------|----------|----------|-----------------------|
| MSCOCO Captions   | CLIP Score ↑  | 32.0     | 31.4     | 30.5                  |
| Drawbench short (GPT-V) 3  | ↑  | 70.4%    | 49.0%    | 46.9%                 |
| Drawbench long (GPT-V)   | ↑  | 81.0%    | 52.4%    | 51.1%                 |
| T2I-C B-VQA Colors   | ↑  | 81.1%    | 59.2%    | 61.9%                 |
| T2I-C B-VQA Shape   | ↑  | 67.5%    | 54.7%    | 61.9%                 |
| T2I-C B-VQA Texture  | ↑  | 80.7%    | 63.7%    | 55.2%                 |


表1 - 各种与提示跟随相关的文本到图像模型的比较评估

1 DALL-E 2 生产版本的图像于 2023 年 9 月 20 日发布。

2 稳定扩散 XL v1.0 使用了 refiner 模块。

3 这里的分数是根据 GPT-V 判断为具有“正确”标题的图像百分比。

## 4.1 自动评估

我们将 DALL-E 3 与 DALL-E 2 和 Stable Diffusion XL 1.0（带有 refiner 模块）进行比较。我们希望评估 DALL-E 3 在与提示跟随相关的任务上的

表现。我们描述下面的各个任务。

### 4.1.1 CLIP 分数

我们首先使用公共的 ViT-B/32 模型计算 CLIP 分数，如第 3.2 节所述。对于这个比较，我们使用了来自 MSCOCO 2014 评估数据集的 4,096 个标题来生成我们的图像。在这个评估中，我们使用简短的地面真实标题对模型进行推断。我们的模型在这个评估中的表现优于 DALL-E 2 和 Stable Diffusion XL。

### 4.1.2 Drawbench

接下来，我们对 drawbench 数据集的标题进行评估。对于这个测试，我们使用了基于 GPT-4 的经过指导的、视觉感知的大型语言模型 GPT-V 来评估我们的模型与其他模型的性能。对于 drawbench 中的每个提示，我们使用每个模型生成四幅图像。然后，我们使用图像和文本提示（见附录 D 中的提示）提示我们的视觉感知的大型语言模型。这导致一个结论（“正确”/“不正确”）和一个关于该结论的解释。

由于我们先前观察到我们的模型在给定语言模型的外推标题时表现更好，我们使用 GPT-4 来使用第 3.5 节中描述的过程对 drawbench 标题进行“上采样”。我们使用这些“上采样”标题从所有模型中采样图像时，再次执行以上自动化评估。在要求视觉感知的大型语言模型判断输出时，我们使用原始的地面真实 drawbench 提示。

在所有 drawbench 评估中，我们的模型都击败了 DALL-E 2 和 Stable Diffusion XL。当我们使用“上采样”标题时，差距显著扩大。

### 4.1.3 T2I-CompBench

最后，我们在黄等人（2023年）开发的 T2I-CompBench 评估套件的子集上进行评估。这个基准测试衡量了模型在组合提示上的表现。我们报告了颜色绑定、形状绑定和纹理绑定的分数。我们使用 Disentangled BLIP-VQA 模型来评估这些结果。在所有评估的基准测试中，DALL-E 3 都是最先进的。

## 4.2 人类评估

我们向人类评估员提交了来自 DALL-E 3 和可比较模型的样本。

对于这个评估，我们向评估员呈现了两个从相同标题生成的图像并排放置。然后，我们向评估员提出三个问题之一：

1. 提示跟随：评估员被呈现给文本到图像模型的完整上采样标题，并被要求“选择哪个图像更好地对应于标题”。
2. 风格： “想象您正在使用一种计算机工具，该工具根据某些文本生成图像。如果您使用此工具，您会更喜欢看到哪个图像。”
3. 一致性：“选择哪个图像包含更多一致的对象。‘一致’的对象是指可能存在的对象。仔细观察人体部位、面部和姿势、物体的摆放以及场景中的文本，以作出您的判断。提示：计算每个图像的不一致实例，并选择问题较少的图像。”

对于提示跟随和风格，我们为这次评估组装了一个小型数据集，其中包含 170 个标题，针对的是生产文本到图像系统的典型用法。这些标题涵盖了广泛的实际用例，如生成人物、产品和地点，概念混合，文本渲染和艺术品。我们将这个评估集称为“DALL-E 3 Eval”。这些标题将随着我们的评估样本发布（见第 4.3 节）。对于一致性，我们观察到评估员会对描述虚构场景的图像进行惩罚。因此，我们从 MSCOCO 中随机抽取了 250 个标题，以确保评估提示描述的场景可能存在。请注意，对于风格和一致性评估，我们不向评估员显示用于生成图像的标题，以确保他们将注意力集中在风格或一致性上，而不是提示跟随。对于每对图像和问题，我们从评估员那里收集 3 个答案，每个模型和问题总共有 2040 个评分。人类评估界面显示在第 E 节。

我们将 DALL-E 3 与带有 refiner 模块的 Stable Diffusion XL 和 Midjourney v5.2 进行比较。在表2中，我们报告使用与 Nichol 等人（2022年）概述的相同计算方法的 ELO 分数。

正如结果显示的那样，DALL-E 3 生成的图像在所有三个方面，特别是在提示跟随方面，大多数时间都被人类评估员更喜欢，超过了所有竞争对手。

| Dataset                | DALL-E 3 | Midjourney 5.2 | Stable Diffusion XL | DALL-E 2 |
|-----------------------|----------|----------------|---------------------|----------|
| DALL-E 3 Eval (prompt following) | 153.3    | -104.8         | -189.5              | -        |
| DALL-E 3 Eval (style) | 74.0     | 30.9           | -95.7               | -        |
| MSCOCO (coherence)    | 71.0     | 48.9           | -84.2               | -        |
| Drawbench              | 61.7     | -              | -34.0               | -79.3    |

### 4.2.1 Drawbench 人类评估

在前面的部分中，我们使用了 GPT-V 对 drawbench 进行了评估。我们注意到，在某些类型的测试中，GPT-V 在判断提示跟随方面并没有表现出优于随机的性能。特别是，在涉及计算图像中对象数量的任务中。为了更好地覆盖 drawbench 的性能，我们使用了前面部分描述的程序，向人类评估员提交了图像和标题进行评估。与我们的 GPT-V drawbench 评估一样，我们仅比较了 DALL-E 3、带有 refiner 模块的 Stable Diffusion XL 和 DALL-E 2。

## 4.3 可重现性

我们已将所有以上比较中所有模型生成的所有样本和提示上传到了 GitHub。

# 5 限制与风险

## 5.1 空间意识

尽管 DALL-E 3 在提示跟随方面取得了重大进展，但仍然在对象放置和空间意识方面存在困难。

例如，使用诸如“在左侧”，“在下方”，“在背后”等词语是相当不可靠的。这是因为我们的合成标题生成器也具有这个弱点：它在陈述对象位置方面不可靠，在我们的下游模型中也反映出来。

## 5.2 文本渲染

在构建我们的标题生成器时，我们特别注意确保它能够在生成的标题中包含图像中的突出词语。

因此，DALL-E 3 可以在提示时生成文本。在测试过程中，我们注意到这种能力是不可靠的，因为单词可能存在缺失或额外的字符。我们怀疑这可能与我们使用的 T5 文本编码器有关：当模型遇到提示中的文本时，它实际上看到代表整个单词的标记，并且必须将其映射到图像中的字母。在未来的工作中，我们希望探索在字符级别的语言模型上进行条件设置，以帮助改善这种行为。

## 5.3 具体性

我们观察到我们的合成标题容易产生关于图像的重要细节的幻觉。

例如，给定一幅植物花卉的图画，标题生成器经常会幻觉出一个植物属和种，并将其放入标题中，即使这些细节在图像中以文本形式提供。当描述鸟类的图片时，我们观察到类似的行为：种类被幻觉出来或根本没有提到。

这对我们的文本到图像模型产生了下游影响：DALL-E 3 在为上述特定术语生成图像方面不可靠。我们相信对标题生成器的进一步改进应该能够进一步改善我们的文本到图像模型。

## 5.4 安全性和偏见缓解

我们对部署 DALL-E 3 所引起的安全问题进行了深入分析，包括模型偏见可能带来的风险。这些评估的结果可以在 DALL-E 3 系统卡中找到。


# 参考资料

https://cdn.openai.com/papers/dall-e-3.pdf

* any list
{:toc}
