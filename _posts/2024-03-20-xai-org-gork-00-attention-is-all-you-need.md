---
layout: post
title: 马斯克开源的 grok-1 底层 Transformer 模型论文 《Attention is All You Need》
date: 2024-03-20 21:01:55 +0800
categories: [AI]
tags: [ai, sh]
published: true
---

# 拓展阅读

[马斯克开源的 grok-1 底层 Transformer 模型论文 《Attention is All You Need》](https://mp.weixin.qq.com/s/bZP2R97GUD1NxV22Tn7eOQ)

[马斯克开源的 grok-1 大模型底层 Transformer 模型到底是个啥？](https://mp.weixin.qq.com/s/jvpovKSitioC7IQ8IWTumg)

[马斯克开源的 grok-1 大模型硬核源码第 1 弹](https://mp.weixin.qq.com/s/nMeisZVQmhVYCRi7YHTKIA)

[马斯克开源的 grok-1 大模型硬核源码第 2 弹](https://mp.weixin.qq.com/s/gdrP9HXRkRf9zrMuzrCB7g)

[马斯克开源的 grok-1 大模型硬核源码第 3 弹](https://mp.weixin.qq.com/s/mpoEnVvrtVBSk4PfUIKmMg)

[马斯克开源的 grok-1 大模型硬核源码第 4 弹](https://mp.weixin.qq.com/s/fNLbaROZXFEfbREuBV1Kpg)

# 前言

大家好，我是老马。

网上的大部分关于 gork-1 的内容都是浅尝辄止，本文老马和大家一起简单看一下马斯克这两天开源的 grok-1 的底层 Transformer 的提出论文。

本文翻译自 Vaswani 等人在论文《Attention is All You Need》。

# 摘要

目前主导的序列转导模型基于复杂的循环或卷积神经网络，包括编码器和解码器。

表现最佳的模型还通过注意力机制将编码器和解码器连接起来。

我们提出了一种新的简单网络架构，Transformer，完全基于注意力机制，不需要循环和卷积。在两个机器翻译任务上的实验证明，这些模型在质量上更加优越，同时更易并行化，并且训练时间显著缩短。

我们的模型在WMT 2014英德翻译任务上达到了28.4 BLEU的得分，相比现有的最佳结果，包括集成模型，提高了2个BLEU以上。

在WMT 2014英法翻译任务中，我们的模型在仅使用八个GPU进行了3.5天的训练后，实现了新的单模型最先进BLEU得分，达到了41.8，训练成本仅为文献中最佳模型的一小部分。

我们展示了Transformer在其他任务上的良好泛化能力，成功地将其应用于英语句法成分解析，无论是在大规模还是有限的训练数据上。


# 1 引言

循环神经网络，尤其是长短期记忆（LSTM）[13]和门控循环神经网络[7]，已经被确定为序列建模和转导问题的最先进方法，例如语言建模和机器翻译[35, 2, 5]。

自那时以来，已经进行了大量努力，不断推动循环语言模型和编码器-解码器架构的边界[38, 24, 15]。

循环模型通常将计算沿着输入和输出序列的符号位置进行因式分解。将位置与计算时间步骤对齐，它们生成一个隐藏状态序列 ht，作为先前隐藏状态 ht−1 和位置 t 的输入的函数。这种固有的顺序性质使得在训练示例内部无法进行并行化，这在序列长度较长时变得至关重要，因为内存限制限制了跨示例的批处理。最近的工作通过因子化技巧[21]和条件计算[32]实现了计算效率的显著提高，同时在后者的情况下也提高了模型性能。然而，顺序计算的基本限制仍然存在。

注意力机制已成为各种任务中引人注目的序列建模和转导模型的一个组成部分，它允许对依赖关系进行建模，而不考虑它们在输入或输出序列中的距离[2, 19]。然而，在除了少数情况[27]之外，这种注意力机制通常与循环网络一起使用。

在这项工作中，我们提出了Transformer，这是一种模型架构，摒弃了循环，而完全依赖于注意力机制来绘制输入和输出之间的全局依赖关系。Transformer允许更多的并行化，并且在仅在八个P100 GPU上进行了12小时的训练后，可以达到翻译质量的新水平。

# 2 背景

减少顺序计算的目标也构成了Extended Neural GPU[16]、ByteNet[18]和ConvS2S[9]的基础，它们都使用卷积神经网络作为基本构建块，在所有输入和输出位置上并行计算隐藏表示。在这些模型中，关联来自两个任意输入或输出位置的信号所需的操作数量随着位置之间的距离增加而增加，对于ConvS2S是线性增长，对于ByteNet是对数增长。这使得学习远距离位置之间的依赖关系更加困难。在Transformer中，这被减少到了一定数量的操作，尽管由于平均注意力加权位置而导致有效分辨率降低，这种效果我们通过第3.2节中描述的多头注意力来抵消。

自注意力，有时称为内部注意力，是一种注意力机制，用于计算序列的表示以关联单个序列的不同位置。自注意力已成功用于各种任务，包括阅读理解、抽象摘要、文本蕴含和学习任务独立的句子表示[4, 27, 28, 22]。

端到端记忆网络基于递归注意力机制，而不是序列对齐递归，并且已被证明在简单语言问题回答和语言建模任务上表现良好[34]。

据我们所知，然而，Transformer是第一个完全依赖于自注意力来计算其输入和输出表示的转导模型，而不使用序列对齐的RNN或卷积。

在接下来的章节中，我们将描述Transformer，提出自注意力的动机，并讨论其优点，如[17, 18]和[9]等模型。

# 3 模型架构 Model Architectur

大多数竞争性的神经序列转导模型都具有编码器-解码器结构[5, 2, 35]。

在这里，编码器将一系列符号表示（x1，...，xn）映射到一系列连续表示z =（z1，...，zn）。

给定z，解码器然后生成一个符号序列（y1，...，ym），每次生成一个元素。

在每个步骤中，模型是自回归的[10]，在生成下一个元素时消耗先前生成的符号作为附加输入。

![架构模型](https://tensorflow.google.cn/images/tutorials/transformer/transformer.png?hl=zh-cn)

Transformer遵循这个整体架构，使用堆叠的自注意力和逐点、全连接的层来构建编码器和解码器，分别显示在图1的左半部分和右半部分。

## 3.1 编码器和解码器堆栈 Encoder and Decoder Stack

编码器：编码器由N = 6个相同的层堆叠而成。每个层包含两个子层。第一个是多头自注意力机制，第二个是简单的位置逐点全连接前馈网络。我们在每个子层周围采用了一个残差连接[11]，然后进行层归一化[1]。也就是说，每个子层的输出是LayerNorm(x + Sublayer(x))，其中Sublayer(x)是子层本身实现的功能。为了便于这些残差连接，模型中的所有子层以及嵌入层都产生维度为dmodel = 512的输出。

解码器：解码器也由N = 6个相同的层堆叠而成。除了每个编码器层中的两个子层外，解码器还插入了第三个子层，该子层对编码器堆栈的输出执行多头注意力。与编码器类似，我们在每个子层周围采用了残差连接，然后进行层归一化。我们还修改了解码器堆栈中的自注意力子层，以防止位置关注后续位置。这种掩码，加上输出嵌入向后偏移一个位置，确保位置i的预测只能依赖于位置小于i的已知输出。
 
## 3.2 注意力 Attention

注意力函数可以描述为将一个查询和一组键值对映射到一个输出，其中查询、键、值和输出都是向量。

输出计算为值的加权和，其中分配给每个值的权重由查询与相应键的兼容性函数计算。

### 3.2.1 缩放点积注意力 Scaled Dot-Product Attention

我们将我们特定的注意力称为“缩放点积注意力”（图2）。

- F2

![缩放](https://tensorflow.google.cn/images/tutorials/transformer/scaled_attention.png?hl=zh-cn)

输入由维度为dk的查询和键，以及维度为dv的值组成。我们计算查询与所有键的点积，将每个点积除以√dk，并应用softmax函数以获得值的权重。

在实践中，我们同时在一组查询上计算注意力函数，将其打包成矩阵Q。

键和值也打包成矩阵K和V。

我们计算输出矩阵如下：

$$ \Large{Attention(Q, K, V) = softmax_k(\frac{QK^T}{\sqrt{d_k} }) V} $$

```
Attention(Q, K, V ) = softmax(QK^T*sqrt(d_k))V 
```

点积注意力被缩小了深度的平方根倍。这样做是因为对于较大的深度值，点积的大小会增大，从而推动 softmax 函数往仅有很小的梯度的方向靠拢，导致了一种很硬的（hard）softmax。

例如，假设 Q 和 K 的均值为0，方差为1。它们的矩阵乘积将有均值为0，方差为 dk。因此，dk 的平方根被用于缩放（而非其他数值），因为，Q 和 K 的矩阵乘积的均值本应该为 0，方差本应该为1，这样会获得一个更平缓的 softmax。

遮挡（mask）与 -1e9（接近于负无穷）相乘。这样做是因为遮挡与缩放的 Q 和 K 的矩阵乘积相加，并在 softmax 之前立即应用。目标是将这些单元归零，因为 softmax 的较大负数输入在输出中接近于零。

### 3.2.2 多头注意力（Multi-head attention）

![multi](https://tensorflow.google.cn/images/tutorials/transformer/multi_head_attention.png?hl=zh-cn)

多头注意力由四部分组成：

- 线性层并分拆成多头。

- 按比缩放的点积注意力。

- 多头及联。

- 最后一层线性层。

每个多头注意力块有三个输入：Q（请求）、K（主键）、V（数值）。这些输入经过线性（Dense）层，并分拆成多头。

将上面定义的 scaled_dot_product_attention 函数应用于每个头（进行了广播（broadcasted）以提高效率）。注意力这步必须使用一个恰当的 mask。然后将每个头的注意力输出连接起来（用tf.transpose 和 tf.reshape），并放入最后的 Dense 层。

Q、K、和 V 被拆分到了多个头，而非单个的注意力头，因为多头允许模型共同注意来自不同表示空间的不同位置的信息。在分拆后，每个头部的维度减少，因此总的计算成本与有着全部维度的单个注意力头相同。


原始论文内容过于抽象：

```
我们发现，与使用维度为dmodel的键、值和查询执行单个注意力函数相比，将查询、键和值进行h次线性投影，并使用不同的学习线性投影将它们投影到维度为dk、dk和dv的空间中，会更有益。然后，在这些投影版本的查询、键和值上并行执行注意力函数，得到dv维的输出值。然后将这些输出值串联起来，再次进行投影，得到最终的值，如图2所示。

多头注意力允许模型同时关注不同位置的不同表示子空间的信息。而使用单个注意力头时，平均会阻碍这种关注。
MultiHead(Q, K, V) = Concat(head1, ..., headh)WO
其中headi = Attention(QWQi, KWKi, VWVi)
投影是参数矩阵WQi ∈ Rdmodel×dk, WKi ∈ Rdmodel×dk, WVi ∈ Rdmodel×dv
和WO ∈ Rhdv×dmodel。

在这项工作中，我们采用了h = 8个并行注意力层或头。对于每个注意力头，我们使用dk = dv = dmodel/h = 64。由于每个头的维度降低，总计算成本与具有完整维度的单头注意力相似。
```

### 3.2.3 注意力在我们模型中的应用

Transformer在三个不同的方面使用多头注意力：
• 在“编码器-解码器注意力”层中，查询来自前一个解码器层，而记忆键和值来自编码器的输出。这允许解码器中的每个位置都能关注输入序列中的所有位置。这模仿了序列到序列模型中典型的编码器-解码器注意力机制，如[38, 2, 9]。

• 编码器包含自注意力层。在自注意力层中，所有的键、值和查询都来自同一个地方，即编码器中前一层的输出。编码器中的每个位置都可以关注编码器前一层的所有位置。

• 同样，解码器中的自注意力层允许解码器中的每个位置都可以关注到包括该位置在内的解码器中的所有位置。我们需要防止解码器中的左向信息流，以保持自回归属性。我们通过在缩放点积注意力中屏蔽（设置为−∞）softmax输入中所有对应于非法连接的值来实现这一点。请参见图2。

## 3.3 位置逐点前馈网络  Position-wise Feed-Forward Networks

除了注意力子层外，我们编码器和解码器中的每个层还包含一个全连接的前馈网络，该网络将每个位置独立且相同地应用于每个位置。这由两个线性变换和一个ReLU激活组成。

```
FFN(x) = max(0, xW1 + b1)W2 + b2
```

虽然线性变换在不同位置上是相同的，但它们在不同层之间使用不同的参数。另一种描述方法是两个卷积核大小为1。

输入和输出的维度是dmodel = 512，内部层的维度是dff = 2048。

## 3.4 嵌入和Softmax(Embeddings and Softmax)

与其他序列转导模型类似，我们使用学习到的嵌入将输入标记和输出标记转换为维度为dmodel的向量。

我们还使用通常的学习线性变换和softmax函数将解码器输出转换为预测的下一个标记的概率。

在我们的模型中，我们在两个嵌入层和预softmax线性变换之间共享相同的权重矩阵，类似于[30]。在嵌入层中，我们将这些权重乘以√dmodel。

表1：不同层类型的最大路径长度、每层复杂度和最小顺序操作数。n是序列长度，d是表示维度，k是卷积的卷积核大小，r是受限自注意力中的邻域大小。

| Layer Type             | Complexity per Layer | Sequential Operations | Maximum Path Length |
|------------------------|----------------------|-----------------------|---------------------|
| Self-Attention         | O(n^2 * d)           | O(1)                  | O(1)                |
| Recurrent              | O(n * d^2)           | O(n)                  | O(n)                |
| Convolutional          | O(k * n * d^2)       | O(1)                  | O(log_k(n))         |
| Self-Attention (restricted) | O(r * n * d)    | O(1)                  | O(n/r)              |


## 3.5 位置编码 Positional Encoding

由于我们的模型不包含循环和卷积，为了让模型利用序列的顺序，我们必须注入有关序列中标记的相对或绝对位置的一些信息。

为此，我们在编码器和解码器堆栈的底部添加了“位置编码”到输入嵌入中。

位置编码与嵌入具有相同的维度dmodel，因此两者可以相加。有许多选择的位置编码，可以是学习的或固定的。

在这项工作中，我们使用不同频率的正弦和余弦函数：

PE(pos,2i) = sin(pos/10000^(2i/dmodel))

PE(pos,2i+1) = cos(pos/10000^(2i/dmodel))

其中pos是位置，i是维度。也就是说，位置编码的每个维度对应于一个正弦波。

波长形成从2π到10000·2π的几何级数。

我们选择这个函数，因为我们假设它将允许模型轻松地通过相对位置进行注意力，因为对于任何固定的偏移k，PE(pos+k)可以表示为PE(pos)的线性函数。

我们还尝试使用学习的位置嵌入，发现两个版本产生几乎相同的结果（见表3行（E））。我们选择了正弦版本，因为它可能允许模型推广到训练过程中遇到的长度更长的序列。


# 4 为什么选择自注意力

在本节中，我们将自注意力层的各个方面与常用于将一个变长序列的符号表示（x1，...，xn）映射到另一个相同长度序列（z1，...，zn）的循环和卷积层进行比较，其中xi，zi ∈ Rd，例如典型序列转导编码器或解码器中的隐藏层。在使用自注意力的动机中，我们考虑了三个要求。

一是每层的总计算复杂度。另一个是可以并行计算的数量，即所需的最小顺序操作数。

第三个是网络中长程依赖之间的路径长度。学习长程依赖是许多序列转导任务的关键挑战。影响学习这种依赖性能力的一个关键因素是前向和后向信号在网络中必须穿过的路径长度。输入序列和输出序列中任意组合位置之间的这些路径越短，学习长程依赖就越容易。因此，我们还比较了由不同层类型组成的网络中任意两个输入和输出位置之间的最大路径长度。

如表1所示，自注意力层连接所有位置，需要执行的操作数量是常数，而循环层则需要O(n)个顺序操作。就计算复杂度而言，当序列长度n小于表示维度d时，自注意力层比循环层更快，而这在机器翻译的最新模型中常常是句子表示的情况，例如word-piece和byte-pair表示。为了改善涉及非常长序列的任务的计算性能，自注意力可以被限制为仅考虑围绕各自输出位置的输入序列中大小为r的邻域。这将最大路径长度增加到O(n/r)。我们计划在将来的工作中进一步研究这种方法。

带有宽度小于n的卷积核k的单个卷积层不会连接所有输入和输出位置的所有对。要做到这一点，对于连续卷积核，需要O(n/k)个卷积层的堆栈，或者对于扩张卷积，需要O(logk(n))，增加了网络中任意两个位置之间最长路径的长度。卷积层通常比循环层更昂贵，比例因子为k。然而，可分离卷积显著降低了复杂性，变为O(k · n · d + n · d^2)。然而，即使k = n，可分离卷积的复杂性也等于我们模型中自注意力层和逐点前馈层的组合。

作为副作用，自注意力可以产生更可解释的模型。我们检查了我们模型的注意力分布，并在附录中呈现和讨论了示例。

不仅个别的注意力头清楚地学习执行不同的任务，许多看起来还表现出与句子的句法和语义结构相关的行为。

# 5 训练

本节描述了我们模型的训练方案。

## 5.1 训练数据和批处理

我们在标准的WMT 2014英德数据集上进行了训练，该数据集包含约450万个句对。句子使用字节对编码进行编码[3]，具有约37000个共享的源目标词汇标记。对于英法语，我们使用了规模更大的WMT 2014英法数据集，包含3600万个句子，并将标记分割成一个32000个单词片段词汇[38]。句对被近似序列长度批量处理。每个训练批次包含一组句对，其中大约包含25000个源标记和25000个目标标记。

## 5.2 硬件和训练计划

我们在一台配备8个NVIDIA P100 GPU的机器上进行了模型训练。对于使用整篇论文中描述的超参数的基础模型，每个训练步骤大约需要0.4秒。我们对基础模型进行了总共10万步或12小时的训练。对于我们的大型模型（表3中底部行所述），步骤时间为1.0秒。大型模型进行了30万步（3.5天）的训练。

## 5.3 优化器

我们使用Adam优化器[20]，其中β1 = 0.9，β2 = 0.98和ϵ = 10^-9。我们根据以下公式调整学习率，：

```
lrate = d^(-0.5)_model · min(step^(-0.5), step · warmup_steps^(-1.5)) (3)
```

这意味着前warmup_steps个训练步骤中，学习率线性增加，之后按步骤数的倒数平方根成比例地减小。我们使用warmup_steps = 4000。

## 5.4 正则化

在训练过程中，我们采用了三种类型的正则化：

- 表2：相比先前最先进的模型，在训练成本的一小部分上，Transformer 在英译德和英译法的 newstest2014 测试中取得了更好的 BLEU 分数。

```
| Model                          | BLEU   | Training Cost (FLOPs)   |
|--------------------------------|--------|--------------------------|
| ByteNet [18]                   | 23.75  |                          |
| Deep-Att + PosUnk [39]        | 39.2   | 1.0 × 10^20              |
| GNMT + RL [38]                 | 24.6   | 39.92                    |
|                                |        | 2.3 × 10^19              |
| ConvS2S [9]                    | 25.16  | 40.46                    |
|                                |        | 9.6 × 10^18              |
| MoE [32]                       | 26.03  | 40.56                    |
|                                |        | 2.0 × 10^19              |
| Deep-Att + PosUnk Ensemble [39]| 40.4   | 8.0 × 10^20              |
| GNMT + RL Ensemble [38]       | 26.30  | 41.16                    |
|                                |        | 1.8 × 10^20              |
| ConvS2S Ensemble [9]          | 26.36  | 41.29                    |
|                                |        | 7.7 × 10^19              |
| Transformer (base model)      | 27.3   | 38.1                     |
| Transformer (big)              | 28.4   | 41.8                     |
|                                |        | 2.3 × 10^19              |
```

残差丢弃(Residual Dropout)：我们对每个子层的输出应用了丢弃（dropout）[33]，在将其添加到子层输入并进行归一化之前。此外，我们还对编码器和解码器堆栈中的嵌入和位置编码的总和应用了丢弃。对于基础模型，我们使用了 Pdrop = 0.1 的速率。

标签平滑(Label Smoothing)：在训练过程中，我们采用了值为 ϵls = 0.1 的标签平滑[36]。这会降低困惑度，因为模型学会更加不确定，但会提高准确性和 BLEU 分数。

# 6 结果

## 6.1 机器翻译(Machine Translation)

在 WMT 2014 年英译德任务中，大型 Transformer 模型（表2 中的 Transformer（big））的表现优于先前报告的最佳模型（包括集成模型），BLEU 值超过2.0，建立了一个新的最先进 BLEU 分数为 28.4。该模型的配置列在表3 的底部行中。在 8 个 P100 GPU 上训练耗时 3.5 天。即使是我们的基础模型，也超过了所有先前发布的模型和集成模型，在训练成本的一小部分上。

在 WMT 2014 年英译法任务中，我们的大型模型实现了 41.0 的 BLEU 分数，优于先前发布的所有单一模型，在先前最先进模型的训练成本不到1/4。用于英译法的 Transformer（big）模型使用了丢弃率 Pdrop = 0.1，而不是0.3。

对于基础模型，我们使用了平均最后 5 个检查点获得的单一模型，这些检查点是以 10 分钟间隔写入的。对于大型模型，我们平均了最后 20 个检查点。我们使用了束搜索，束大小为 4，并且长度惩罚 α = 0.6。这些超参数是在开发集上进行实验后选择的。在推理期间，我们将最大输出长度设置为输入长度 + 50，但尽可能提前终止。

表2 总结了我们的结果，并将我们的翻译质量和训练成本与文献中的其他模型架构进行了比较。我们估计训练模型所使用的浮点运算数量，通过将训练时间、使用的 GPU 数量和每个 GPU 的单精度浮点容量的可持续估算相乘。

## 6.2 模型变体(Model Variations)

为了评估 Transformer 不同组件的重要性，我们以不同方式改变了我们的基础模型，并测量了在英译德翻译上性能的变化。

我们使用了 2.8、3.7、6.0 和 9.5 TFLOPS 的值分别用于 K80、K40、M40 和 P100。

表3：Transformer 架构的变体。未列出的值与基础模型相同。所有指标均在英译德翻译开发集 newstest2013 上，所列困惑度是按照我们的字节对编码计算的每个词片段的，并且不应与每个单词的困惑度进行比较。

```
| 实验 | N | dmodel | dff | h | dk | dv | Pdrop | ϵls | 训练步数 | 训练PPL | BLEU | 参数数量 |
|-----|---|--------|-----|---|----|----|-------|------|----------|---------|------|----------|
| (A) | 6 | 512    | 2048| 8 | 64 | 64 | 0.1   | 0.1  | 100K     | 4.92    | 25.8 | 65 × 10^6 |
|     | 1 | 512    | 512 |   |    |    |       |      |          | 5.29    | 24.9 |          |
|     | 4 | 128    | 128 |   |    |    |       |      |          | 5.00    | 25.5 |          |
|     | 16| 32     | 32  |   |    |    |       |      |          | 4.91    | 25.8 |          |
|     | 32| 16     | 16  |   |    |    |       |      |          | 5.01    | 25.4 |          |
| (B) | 16|        |     |   |    |    |       |      |          | 5.16    | 25.1 | 58       |
|     | 32|        |     |   |    |    |       |      |          | 5.01    | 25.4 | 60       |
| (C) | 2 |        |     |   |    |    |       |      |          | 6.11    | 23.7 | 36       |
|     | 4 |        |     |   |    |    |       |      |          | 5.19    | 25.3 | 50       |
|     | 8 |        |     |   |    |    |       |      |          | 4.88    | 25.5 | 80       |
|     | 256| 32     | 32  |   |    |    |       |      |          | 5.75    | 24.5 | 28       |
|     | 1024| 128   | 128 |   |    |    |       |      |          | 4.66    | 26.0 | 168      |
|     | 1024|       |     |   |    |    |       |      |          | 5.12    | 25.4 | 53       |
|     | 4096|       |     |   |    |    |       |      |          | 4.75    | 26.2 | 90       |
| (D) | 0.0       |     |   |    |    |    |       |      |          | 5.77    | 24.6 |          |
|     | 0.2       |     |   |    |    |    |       |      |          | 4.95    | 25.5 |          |
|     | 0.0       |     |   |    |    |    |       |      |          | 4.67    | 25.3 |          |
|     | 0.2       |     |   |    |    |    |       |      |          | 5.47    | 25.7 |          |
| (E) |            |     |   |    |    |    |       |      |          | 4.92    | 25.7 |          |
|     | positional embedding instead of sinusoids |        |    |    |    |    |    |    |          |          |       |
| big | 6 | 1024   | 4096| 16|    |    | 0.3   |      | 300K     | 4.33    | 26.4 | 213 × 10^6|
``` 

在开发集 newstest2013 上使用了束搜索，如前文所述，但没有进行检查点平均化。我们将这些结果呈现在表3中。

在表3的行(A)中，我们改变了注意力头的数量以及注意力键和值的维度，保持了计算量恒定，如第3.2.2节所述。虽然单头注意力比最佳设置差0.9 BLEU，但质量随着头数过多而下降。
在表3的行(B)中，我们观察到减少注意力键大小 dk 会降低模型质量。这表明确定兼容性并不容易，而且比点积更复杂的兼容性函数可能会更有益。

我们在行(C)和(D)中进一步观察到，如预期的那样，更大的模型效果更好，并且丢弃（dropout）对于避免过拟合非常有帮助。

在行(E)中，我们用学习的位置嵌入替换了我们的正弦位置编码[9]，并观察到与基础模型几乎相同的结果。

## 6.3 英语成分句法分析(English Constituency Parsing)

为了评估 Transformer 是否能推广到其他任务，我们进行了英语成分句法分析的实验。这项任务面临着特定的挑战：输出受到强烈的结构约束，并且明显比输入更长。此外，RNN 序列到序列模型在小数据情况下无法达到最先进的结果。

我们在宾夕法尼亚树库（Penn Treebank）的《华尔街日报》（Wall Street Journal，简称 WSJ）部分上训练了一个4层的 Transformer，其 dmodel = 1024，大约有 40K 训练句子。我们还在半监督设置下训练了它，使用了更大的高置信度和 BerkleyParser 语料库，其中约有 1700 万句子。对于仅使用 WSJ 的设置，我们使用了 16K 令牌的词汇表，对于半监督设置，我们使用了 32K 令牌的词汇表。

我们只进行了少量的实验来选择丢弃率，包括注意力和残差（第5.4节），学习率和束大小，在第22节的开发集上，所有其他参数与英译德基础翻译模型保持不变。

在推理期间，我们
表4：Transformer 在英语成分句法分析中推广效果良好（结果在 WSJ 的第23节）


| 模型                              | 训练数据集 | WSJ 23 F1 |
|----------------------------------|-------------|-------------|
| Vinyals & Kaiser el al. (2014) [37] | 仅 WSJ | 88.3 |
| Petrov et al. (2006) [29]          | 仅 WSJ | 90.4 |
| Zhu et al. (2013) [40]             | 仅 WSJ | 90.4 |
| Dyer et al. (2016) [8]             | 仅 WSJ | 91.7 |
| Transformer (4 layers)             | 仅 WSJ | 91.3 |
| Zhu et al. (2013) [40]             | 半监督 | 91.3 |
| Huang & Harper (2009) [14]         | 半监督 | 91.3 |
| McClosky et al. (2006) [26]        | 半监督 | 92.1 |
| Vinyals & Kaiser el al. (2014) [37] | 半监督 | 92.1 |
| Transformer (4 layers)             | 半监督 | 92.7 |
| Luong et al. (2015) [23]           | 多任务 | 93.0 |
| Dyer et al. (2016) [8]             | 生成式 | 93.3 |


我们增加了最大输出长度到输入长度 + 300。对于仅使用 WSJ 和半监督设置，我们都使用了束大小为 21 和 α = 0.3。

我们在表4中的结果显示，尽管缺乏任务特定的调整，我们的模型表现出奇好的性能，比以前报告的所有模型都要好，除了递归神经网络语法 [8]。

与 RNN 序列到序列模型相比 [37]，即使仅在包含 40K 句子的 WSJ 训练集上训练，Transformer 也优于 BerkeleyParser [29]。

# 7 结论

在这项工作中，我们介绍了 Transformer，这是第一个完全基于注意力的序列转导模型，用多头自注意力取代了编码器-解码器架构中最常用的递归层。

对于翻译任务，与基于递归或卷积层的架构相比，Transformer 的训练速度可以显著更快。在 WMT 2014 年英译德和 WMT 2014 年英译法翻译任务中，我们取得了新的最先进。在前一项任务中，我们的最佳模型甚至优于以前报告的所有集成模型。

我们对基于注意力的模型的未来感到兴奋，并计划将它们应用于其他任务。我们计划将 Transformer 扩展到涉及除文本之外的输入和输出模态的问题，并研究本地、受限制的注意力机制，以有效处理大量的输入和输出，如图像、音频和视频。使生成 less sequential 是我们的另一个研究目标。

我们用于训练和评估我们模型的代码可在 https://github.com/tensorflow/tensor2tensor 找到。

致谢 我们感谢 Nal Kalchbrenner 和 Stephan Gouws 提供的有益评论、修正和灵感。

引用省略。

# 小结

感觉 AI 底层，有时候数学是基石。

很多模型都是不断优化的，NLP 也是一个不断发展的领域，比较强大的是，这个模型从目前 GPT 的表现来看，远远不止只是语言领域这么简单。

**山长水远，行则将至。**

我是老马，期待与你的下次重逢。

# 参考资料

https://arxiv.org/abs/1706.03762

https://arxiv.org/pdf/1706.03762.pdf


* any list
{:toc}
