---
layout: post
title:  人工智能基础课-01数学基础九层之台，起于累土：线性代数
date:   2015-01-01 23:20:27 +0800
categories: [人工智能基础课]
tags: [人工智能基础课, other]
published: true
---



01 数学基础 九层之台，起于累土：线性代数
“人工智能基础课”将从数学基础开始。必备的数学知识是理解人工智能不可或缺的要素，今天的种种人工智能技术归根到底都建立在数学模型之上，而这些数学模型又都离不开线性代数（linear algebra）的理论框架。

事实上，线性代数不仅仅是人工智能的基础，更是现代数学和以现代数学作为主要分析方法的众多学科的基础。从量子力学到图像处理都离不开向量和矩阵的使用。而在向量和矩阵背后，线性代数的核心意义在于提供了⼀种看待世界的抽象视角：**万事万物都可以被抽象成某些特征的组合，并在由预置规则定义的框架之下以静态和动态的方式加以观察**。

线性代数中最基本的概念是集合（set）。在数学上，集合的定义是由某些特定对象汇总而成的集体。集合中的元素通常会具有某些共性，因而可以用这些共性来表示。对于集合 { 苹果，橘子，梨 } 来说， 所有元素的共性是它们都是水果；对于集合 {牛，马，羊} 来说，所有元素的共性是它们都是动物。当然 { 苹果，牛 } 也可以构成一个集合，但这两个元素并没有明显的共性，这样的集合在解决实际问题中的作用也就相当有限。

“苹果”或是“牛”这样的具体概念显然超出了数学的处理范围，因而集合的元素需要进行进一步的抽象——用数字或符号来表示。如此一来，集合的元素既可以是单个的数字或符号，也可以是多个数字或符号以某种方式排列形成的组合。

在线性代数中，由单独的数a构成的元素被称为标量（scalar）：一个标量a可以是整数、实数或复数。如果多个标量\({ a_1, a_2, \\cdots, a_n}\) 按一定顺序组成一个序列，这样的元素就被称为向量（vector）。显然，向量可以看作标量的扩展。原始的一个数被替代为一组数，从而带来了维度的增加，给定表示索引的下标才能唯一地确定向量中的元素。

每个向量都由若干标量构成，如果将向量的所有标量都替换成相同规格的向量，得到的就是如下的矩阵（matrix）:

```
{% raw %}
\[\\left( {\\begin{array}{cc}- {a_{11}&{a_{12}&{a_{13}\\cr- {a_{21}&{a_{22}&{a_{23}\\cr- {a_{31}&{a_{32}&{a_{33}- \\end{array} \\right)\]
{% endraw %}
```

相对于向量，矩阵同样代表了维度的增加，矩阵中的每个元素需要使用两个索引（而非一个）确定。同理，如果将矩阵中的每个标量元素再替换为向量的话，得到的就是张量（tensor）。直观地理解，张量就是高阶的矩阵。

如果把三阶魔方的每一个小方块看作一个数，它就是个3×3×3的张量，3×3的矩阵则恰是这个魔方的一个面，也就是张量的一个切片。相比于向量和矩阵，张量是更加复杂，直观性也更差的概念。

向量和矩阵不只是理论上的分析工具，也是计算机工作的基础条件。人类能够感知连续变化的大千世界，可计算机只能处理离散取值的二进制信息，因而来自模拟世界的信号必须在定义域和值域上同时进行数字化，才能被计算机存储和处理。从这个角度看，**线性代数是用虚拟数字世界表示真实物理世界的工具**。

在计算机存储中，标量占据的是零维数组；向量占据的是一维数组，例如语音信号；矩阵占据的是二维数组，例如灰度图像；张量占据的是三维乃至更高维度的数组，例如RGB图像和视频。

描述作为数学对象的向量需要有特定的数学语言，范数和内积就是代表。范数（norm）是对单个向量大小的度量，描述的是向量自身的性质，其作用是将向量映射为一个非负的数值。通用的\(L ^ p\)范数定义如下：

```
{% raw %}

\[{\\left| {\\bf{x} \\right|_p} = {\\left( {\\sum\\limits_i {\\left| {x_i} \\right|}^p} } \\right)^{\\frac{1}{p}\]

{% endraw %}
```

对⼀个给定向量，\(L ^ 1\)范数计算的是向量所有元素绝对值的和，\(L ^ 2\)范数计算的是通常意义上的向量长度，\(L ^ {\\infty}\)范数计算的则是向量中最大元素的取值。

范数计算的是单个向量的尺度，内积（inner product）计算的则是两个向量之间的关系。两个相同维数向量内积的表达式为

```
{% raw %}

\[\\left\\langle {\\bf{x,y} \\right\\rangle = \\sum\\limits_i {x_i} \\cdot {y_i} \]

{% endraw %}
```

即对应元素乘积的求和。内积能够表示两个向量之间的相对位置，即向量之间的夹角。一种特殊的情况是内积为0，即\(\\left\\langle \\bf{x,y} \\right\\rangle = 0\)。在二维空间上，这意味着两个向量的夹角为90度，即相互垂直。而在高维空间上，这种关系被称为正交（orthogonality）。如果两个向量正交，说明他们线性无关，相互独立，互不影响。

在实际问题中，向量的意义不仅是某些数字的组合，更可能是某些对象或某些行为的特征。范数和内积能够处理这些表示特征的数学模型，进而提取出原始对象或原始行为中的隐含关系。

如果有一个集合，它的元素都是具有相同维数的向量（可以是有限个或无限个）， 并且定义了加法和数乘等结构化的运算，这样的集合就被称为线性空间（linear space），定义了内积运算的线性空间则被称为内积空间（inner product space）。**在线性空间中，任意一个向量代表的都是n维空间中的一个点；反过来， 空间中的任意点也都可以唯一地用一个向量表示**。两者相互等效。

在线性空间上点和向量的相互映射中，一个关键问题是参考系的选取。在现实生活中，只要给定经度、纬度和海拔高度，就可以唯一地确定地球上的任何一个位置，因而经度值、纬度值、高度值构成的三维向量(x, y, h)就对应了三维物理空间中的⼀个点。

可是在直觉无法感受的高维空间中，坐标系的定义可就没有这么直观了。要知道，人工神经网络要处理的通常是数以万计的特征，对应着维度同样数以万计的复杂空间，这时就需要正交基的概念了。

在内积空间中，一组两两正交的向量构成这个空间的正交基（orthogonal basis），假若正交基中基向量的\(L ^ 2\)范数都是单位长度1，这组正交基就是标准正交基（orthonormal basis）。正交基的作用就是给内积空间定义出经纬度。⼀旦描述内积空间的正交基确定了，向量和点之间的对应关系也就随之确定。

值得注意的是，描述内积空间的正交基并不唯一。对二维空间来说，平面直角坐标系和极坐标系就对应了两组不同的正交基，也代表了两种实用的描述方式。

线性空间的一个重要特征是能够承载变化。当作为参考系的标准正交基确定后，空间中的点就可以用向量表示。当这个点从一个位置移动到另一个位置时，描述它的向量也会发生改变。**点的变化对应着向量的线性变换（linear transformation），而描述对象变化抑或向量变换的数学语言，正是矩阵**。

在线性空间中，变化的实现有两种方式：一是点本身的变化，二是参考系的变化。在第一种方式中，使某个点发生变化的方法是用代表变化的矩阵乘以代表对象的向量。可是反过来，如果保持点不变，而是换一种观察的角度，得到的也将是不同的结果，正所谓“横看成岭侧成峰，远近高低各不同”。

在这种情况下，矩阵的作用就是对正交基进行变换。因此，对于矩阵和向量的相乘，就存在不同的解读方式：

```
{% raw %}

\[{\\bf{Ax = y}\]

{% endraw %}
```

这个表达式既可以理解为向量x经过矩阵A所描述的变换，变成了向量y；也可以理解为一个对象在坐标系A的度量下得到的结果为向量x，在标准坐标系 I（单位矩阵：主对角线元素为1，其余元素为0）的度量下得到的结果为向量y。

这表示矩阵不仅能够描述变化，也可以描述参考系本身。引用网络上一个精当的类比：表达式Ax就相当于对向量x做了一个环境声明，用于度量它的参考系是A。如果想用其他的参考系做度量的话，就要重新声明。**而对坐标系施加变换的方法，就是让表示原始坐标系的矩阵与表示变换的矩阵相乘**。

描述矩阵的⼀对重要参数是特征值（eigenvalue）和特征向量（eigenvector）。对于给定的矩阵A，假设其特征值为λ，特征向量为x，则它们之间的关系如下：

```
{% raw %}

\[{\\bf{Ax} = \\lambda {\\bf{x}\]

{% endraw %}
```

正如前文所述，矩阵代表了向量的变换，其效果通常是对原始向量同时施加方向变化和尺度变化。可对于有些特殊的向量，矩阵的作用只有尺度变化而没有方向变化，也就是只有伸缩的效果而没有旋转的效果。对于给定的矩阵来说，这类特殊的向量就是矩阵的特征向量，特征向量的尺度变化系数就是特征值。

**矩阵特征值和特征向量的动态意义在于表示了变化的速度和方向**。如果把矩阵所代表的变化看作奔跑的人，那么矩阵的特征值就代表了他奔跑的速度，特征向量代表了他奔跑的方向。但矩阵可不是普通人，它是三头六臂的哪吒，他的不同分身以不同速度（特征值）在不同方向（特征向量）上奔跑，所有分身的运动叠加在⼀起才是矩阵的效果。

求解给定矩阵的特征值和特征向量的过程叫做特征值分解，但能够进行特征值分解的矩阵必须是n维方阵。将特征值分解算法推广到所有矩阵之上，就是更加通用的奇异值分解。

今天我和你分享了人工智能必备的线性代数基础，着重于抽象概念的解释而非具体的数学公式，其要点如下：

* 线性代数的本质在于将具体事物抽象为数学对象，并描述其静态和动态的特性；
* 向量的实质是n维线性空间中的静止点；
* 线性变换描述了向量或者作为参考系的坐标系的变化，可以用矩阵表示；
* 矩阵的特征值和特征向量描述了变化的速度与方向。

线性代数之于人工智能如同加法之于高等数学。这样一个基础的工具集，你能想到它在人工智能中的哪些具体应用呢？

欢迎发表你的观点。

![](https://learn.lianglianglee.com/%e4%b8%93%e6%a0%8f/%e4%ba%ba%e5%b7%a5%e6%99%ba%e8%83%bd%e5%9f%ba%e7%a1%80%e8%af%be/assets/e4111df16317c6c9a400ed9494c2f8a2.jpg)




# 参考资料

https://learn.lianglianglee.com/%e4%b8%93%e6%a0%8f/%e4%ba%ba%e5%b7%a5%e6%99%ba%e8%83%bd%e5%9f%ba%e7%a1%80%e8%af%be/01%20%e6%95%b0%e5%ad%a6%e5%9f%ba%e7%a1%80%20%e4%b9%9d%e5%b1%82%e4%b9%8b%e5%8f%b0%ef%bc%8c%e8%b5%b7%e4%ba%8e%e7%b4%af%e5%9c%9f%ef%bc%9a%e7%ba%bf%e6%80%a7%e4%bb%a3%e6%95%b0.md

* any list
{:toc}
